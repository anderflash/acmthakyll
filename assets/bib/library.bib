Automatically generated by Mendeley Desktop 1.12-dev2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Banerjee2010a,
abstract = {Multi-Agent Plan Recognition (MAPR) seeks to identify the dynamic team structures and team behaviors from the obser- vations of the activity-sequences of a set of intelligent agents, based on a library of known team-activities (plan library). It has important applications in analyzing data from automated monitoring, surveillance, and intelligence analysis in general. Recently, we have introduced a model for MAPR with a flat library structure, to study the complexity of basic MAPR, and also possibly its extensions in the future. Interestingly, this model makes fewer assumptions than existing models, and hence is more general. Therefore, as no existing algorithm would apply to this model, we have developed an hypothe- sis generation algorithm for this model, and adapted Knuth’s Algorithm X for branch and bound search in the resulting hypothesis space. In this paper, we establish the time com- plexity of hypothesis generation in this model, propose and evaluate 3 different bounding criteria, and also empirically study the dependence of runtimes (hypothesis generation, and search times separately) on the model parameters.},
author = {Banerjee, Bikramjit and Kraemer, Landon},
booktitle = {Proceedings of AAAI Conference on Artificial Intelligence},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of AAAI Conference on Artificial Intelligence/2010/Banerjee, Kraemer/Banerjee, Kraemer - 2010 - Search Performance of Multi-Agent Plan Recognition in a General Model.pdf:pdf},
pages = {2--9},
title = {{Search Performance of Multi-Agent Plan Recognition in a General Model}},
year = {2010}
}
@article{Goshtasby1993,
author = {Goshtasby, A},
doi = {10.1007/BF01539537},
file = {::},
journal = {International Journal of Computer Vision},
title = {{Design and recovery of 2-D and 3-D shapes using rational Gaussian curves and surfaces}},
url = {http://link.springer.com/article/10.1007/BF01539537},
year = {1993}
}
@misc{Heda2004,
author = {Heda, N},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2004/Heda/Heda - 2004 - Projector-camera Based Solutions for Simulation System.pdf:pdf},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
pages = {24},
title = {{Projector-camera Based Solutions for Simulation System}},
url = {http://www.cse.iitb.ac.in/~sharat/talks/nil.pdf},
year = {2004}
}
@phdthesis{Karrsgard2003,
abstract = {Vision is the most complex human sense and a very important instrument for communication. A lot of knowledge about a person can be obtained by studying her eyes and it is possible to trace her true intent by observing her eye movements. In the same way as speech or handwriting analysis require accurate interpretation of the speech or the pen movements, visual impression analysis require accurate interpretation of the eye movements. In this thesis the gaze tracking system Smart Eye Pro, developed by Smart Eye AB, is evaluated in terms of its use in distinguishing small eye movements. The large amount of data obtained from the gaze tracker is refined and the thesis aims at evaluating the gain of interpreting the eye movements using hidden Markov models. An eye typing application is developed to facilitate the evaluation. In this application, the user forms a word by fixating characters on a screen in front of her. Eye typing is an interesting application since the quality of the interpretation is easy to determine and the application may be an important aid for people with a handicap that impair their ability to write.},
author = {K\"{a}rrsg\aa rd, I and Lindholm, A},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/Unknown/K\"{a}rrsg\aa rd, Lindholm/K\"{a}rrsg\aa rd, Lindholm - Unknown - Eye movement tracking using hidden Markov models.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {40},
title = {{Eye movement tracking using hidden Markov models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.125.5895\&rep=rep1\&type=pdf}
}
@inproceedings{McMurrough2012,
abstract = {In this paper, we present a low-cost solution for real-time tracking of a human user's head position with respect to a video display source for eye gaze estimation in an assistive setting. The solution utilizes a wearable headset equipped with sensors found in commercially available off-the-shelf video gaming devices in order to minimize hardware complexity and expense. A pair of Nintendo Wiimote imaging sensors are used to create a stereo camera for 6DOF position tracking of the headset, while a modified Playstation Eye monocular camera is used to track the pupil position. The resulting tracking hardware is able to measure the 3D position of four infrared LEDs mounted at known locations on the video display using triangulation of the stereo camera data. Integration of the head tracking estimate with a computer vision based pupil tracking solution in order to compute the user's point of gaze is also described.},
address = {New York, New York, USA},
author = {McMurrough, Christopher and Rich, Jonathan and Metsis, Vangelis and Nguyen, An and Makedon, Fillia},
booktitle = {Proceedings of the 5th International Conference on PErvasive Technologies Related to Assistive Environments - PETRA '12},
doi = {10.1145/2413097.2413125},
file = {::},
isbn = {9781450313001},
keywords = {epipolar geometry,eye gaze,eye tracking,gaze 3D,head tracking,point of regard,stereo camera},
mendeley-tags = {gaze 3D},
month = jun,
pages = {1},
publisher = {ACM Press},
title = {{Low-cost head position tracking for gaze point estimation}},
url = {http://dl.acm.org/citation.cfm?id=2413097.2413125},
year = {2012}
}
@inproceedings{Madsen2011,
abstract = {The paper presents a technique for estimating the radiance of the sky and sun for outdoor, daylight illumi- nation conditions. Shadows cast by dynamic objects are detected using color imagery, combined with depth information from a commercial stereo camera setup. Color information extracted from the detected shadows is used to estimate the radiance of the sun. The technique does not require special purpose objects in the scene, nor does it require High Dynamic Range imagery. Results are demonstrated by rendering augmented objects into real images with shading and shadows which are consistent with the real scene},
address = {Vilamoura, Portugal},
author = {Madsen, Claus B and Lal, Brajesh B},
booktitle = {International Conference on Computer Graphics Theory and Applications - GRAPP},
keywords = {Augmented Reality,HDR.,Illumination,Shadows,Stereo},
pages = {129--139},
publisher = {Springer},
title = {{Outdoor illumination estimation in image sequences for augmented reality}},
year = {2011}
}
@article{Gale1984,
author = {Gale, AG and Johnson, F},
journal = {European Conference on Eye Movements ( \ldots},
keywords = {dispersion,eye movement,gaze analysis},
mendeley-tags = {dispersion,eye movement,gaze analysis},
title = {{Operational problems in analysing eye movements}},
url = {http://books.google.com/books?hl=pt-BR\&lr=\&id=ryMkSMpWy8wC\&oi=fnd\&pg=PA21\&dq=Operational+problems+in+analysing+eye+movements\&ots=IS8mm1S4Zg\&sig=FGpV9lfzZgm6PFC5qQ4wTZ9tRN4 http://books.google.com.br/books?hl=pt-BR\&lr=\&id=ryMkSMpWy8wC\&oi=fnd\&pg=PA21\&dq=Operational+problems+in+analysing+eye+movements\&ots=IS8lm4N12j\&sig=-mKAT8JQ0k1ltO5Jr0M-Nb1344U},
year = {1984}
}
@article{Nieuwenhuizen2009,
author = {Nieuwenhuizen, Karin and Liu, Lei and Liere, Robert Van and Martens, Jean-Bernard},
doi = {10.1109/MCG.2009.121},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Computer Graphics And Applications/2009/Nieuwenhuizen et al/Nieuwenhuizen et al. - 2009 - Insights from Dividing 3D Goal-Directed Movements into Meaningful Phases.pdf:pdf},
issn = {0272-1716},
journal = {IEEE Computer Graphics And Applications},
month = nov,
number = {6},
pages = {44--53},
title = {{Insights from Dividing 3D Goal-Directed Movements into Meaningful Phases}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5307642},
volume = {29},
year = {2009}
}
@article{Agusanto2003,
annote = {- Revisado},
author = {Agusanto, Kusuma and Li, Li and Chuangui, Zhu and Sing, Ng Wan},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2003/Agusanto et al/Agusanto et al. - 2003 - Photorealistic rendering for augmented reality using environment illumination.pdf:pdf},
isbn = {0-7695-2006-5},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = oct,
pages = {208},
publisher = {IEEE Computer Society},
title = {{Photorealistic rendering for augmented reality using environment illumination}},
url = {http://dl.acm.org/citation.cfm?id=946248.946791},
year = {2003}
}
@inproceedings{Chenney2004,
abstract = {We present flow tiles, a novel technique for representing and designing velocity fields. Unlike existing procedural flow generators, tiling offers a natural user interface for field design. Tilings can be constructed to meet a wide variety of external and internal boundary conditions, making them suitable for inclusion in larger environments. Tiles offer memory savings through the re-use of prototypical elements. Each flow tile contains a small field and many tiles can be combined to produce large flows. The corners and edges of tiles are constructed to ensure continuity across boundaries between tiles. In addition, all our tiles and the resulting titing are divergence-free and hence suitable for representing a range of effects. We discuss issues that arise in designing flow tiles, algorithms for creating tilings, and three applications: a crowd on city streets, a river flowing between banks, and swirling fog. The first two applications use stationary fields, while the latter demonstrates a dynamic field.},
address = {New York, New York, USA},
author = {Chenney, Stephen},
booktitle = {Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation - SCA '04},
doi = {10.1145/1028523.1028553},
isbn = {3905673142},
issn = {17275288},
keywords = {fluid simulation,tiles,tiling,velocity field},
pages = {233},
publisher = {ACM Press},
title = {{Flow tiles}},
url = {http://portal.acm.org/citation.cfm?doid=1028523.1028553},
year = {2004}
}
@inproceedings{Wu2010a,
abstract = {A novel method for crowd flow modeling and anomaly detection is proposed for both coherent and incoherent scenes. The novelty is revealed in three aspects. First, it is a unique utilization of particle trajectories for model- ing crowded scenes, in which we propose new and efficient representative trajectories for modeling arbitrarily compli- cated crowd flows. Second, chaotic dynamics are intro- duced into the crowd context to characterize complicated crowd motions by regulating a set of chaotic invariant fea- tures, which are reliably computed and used for detecting anomalies. Third, a probabilistic framework for anomaly detection and localization is formulated. The overall work-flow begins with particle advection based on optical flow. Then particle trajectories are clus- tered to obtain representative trajectories for a crowd flow. Next, the chaotic dynamics of all representative trajectories are extracted and quantified using chaotic invariants known as maximal Lyapunov exponent and correlation dimension. Probabilistic model is learned from these chaotic feature set, and finally, a maximum likelihood estimation criterion is adopted to identify a query video of a scene as normal or abnormal. Furthermore, an effective anomaly localiza- tion algorithm is designed to locate the position and size of an anomaly. Experiments are conducted on known crowd data set, and results show that our method achieves higher accuracy in anomaly detection and can effectively localize anomalies},
address = {San Francisco, CA},
author = {Wu, Shandong and Moore, Brian E. and Shah, Mubarak},
booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5539882},
isbn = {978-1-4244-6984-0},
month = jun,
pages = {2054--2060},
publisher = {IEEE},
title = {{Chaotic invariants of Lagrangian particle trajectories for anomaly detection in crowded scenes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5539882},
year = {2010}
}
@article{Karpov,
author = {Karpov, A and Komogortsev, Oleg V},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of Behavioral Research Methods/2012/Karpov, Komogortsev/Karpov, Komogortsev - 2012 - Automated Classification and Scoring of Smooth Pursuit Eye Movements in Presence of Fixations and Saccades.pdf:pdf},
journal = {Journal of Behavioral Research Methods},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {1--13},
title = {{Automated Classification and Scoring of Smooth Pursuit Eye Movements in Presence of Fixations and Saccades}},
url = {http://cs.txstate.edu/~ok11/papers\_published/TR2011\_11\_23\_SP\_Ka\_Ko.pdf},
year = {2012}
}
@article{Wang2009b,
author = {Wang, Chaoli and Yu, Hongfeng and Ma, Kwan-Liu},
doi = {10.1109/MCG.2009.104},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Computer Graphics and Applications/2009/Wang, Yu, Ma/Wang, Yu, Ma - 2009 - Application-Driven Compression for Visualizing Large-Scale Time-Varying Volume Data.pdf:pdf},
issn = {0272-1716},
journal = {IEEE Computer Graphics And Applications},
keywords = {bit-wise texture,deferred filtering,importance-based compression,large data visualization,packing,time-varying data visualization},
pages = {1--19},
title = {{Application-Driven Compression for Visualizing Large-Scale Time-Varying Volume Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5232779},
volume = {D},
year = {2009}
}
@inproceedings{Lamboray2004,
abstract = {Free-viewpoint video is a promising technology for next-generation virtual and augmented reality applications. Our goal is to enhance collaborative VR applications with 3D video-conferencing features. In this paper, we propose a 3D video streaming technique which can be deployed in telepresence environments. The streaming characteristics of real-time 3D video sequences are investigated under various system and networking conditions. We introduce several encoding techniques and analyze their behavior with respect to resolution, bandwidth and inter-frame jitter. Our 3D video pipeline uses point samples as basic primitives and is fully integrated with a communication framework handling acknowledgment information for reliable network transmissions and application control data. The 3D video reconstruction process dynamically adapts to processing and networking bottlenecks. Our results show that a reliable transmission of our pixel-based differential prediction encoding leads to the best performance in terms of bandwidth, but is also quite sensitive to packet losses. A redundantly encoded stream achieves better results in presence of burst losses and seamlessly adapts to varying network throughput.},
author = {Lamboray, Edouard and Gross, Markus and Wurmlin, S.},
booktitle = {IEEE Virtual Reality 2004},
doi = {10.1109/VR.2004.1310060},
isbn = {0-7803-8415-6},
pages = {91--99, 218},
publisher = {IEEE},
title = {{Real-time streaming of point-based 3D video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1310060},
year = {2004}
}
@article{Okatani2005,
abstract = {This paper presents a method for calibrating a projector-camera system that consists of multiple projectors (or multiple poses of a single projector), a camera, and a planar screen. We consider the problem of estimating the homography between the screen and the image plane of the camera or the screen-camera homography, in the case where there is no prior knowledge regarding the screen surface that enables the direct computation of the homography. It is assumed that the pose of each projector is unknown while its internal geometry is known. Subsequently, it is shown that the screen-camera homography can be determined from only the images projected by the projectors and then obtained by the camera, up to a transformation with four degrees of freedom. This transformation corresponds to arbitrariness in choosing a two-dimensional coordinate system on the screen surface and when this coordinate system is chosen in some manner, the screen-camera homography as well as the unknown poses of the projectors can be uniquely determined. A noniterative algorithm is presented, which computes the homography from three or more images. Several experimental results on synthetic as well as real images are shown to demonstrate the effectiveness of the method.},
author = {Okatani, Takayuki and Deguchi, Koichiro},
doi = {10.1109/TPAMI.2005.235},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE transactions on pattern analysis and machine intelligence/2005/Okatani, Deguchi/Okatani, Deguchi - 2005 - Autocalibration of a projector-camera system.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Calibration,Computer-Assisted,Computer-Assisted: methods,Computer-Assisted: standards,Data Display,Image Enhancement,Image Enhancement: methods,Image Enhancement: standards,Image Interpretation,Imaging,Photography,Photography: methods,Three-Dimensional,Three-Dimensional: methods,User-Computer Interface,anamorphism},
mendeley-tags = {anamorphism},
month = dec,
number = {12},
pages = {1845--55},
pmid = {16355654},
title = {{Autocalibration of a projector-camera system.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1524979 http://www.ncbi.nlm.nih.gov/pubmed/16355654},
volume = {27},
year = {2005}
}
@inproceedings{Li2012,
abstract = {Projective distortion caused by tilted camera brings great inconvenience to subsequent image processing and pattern recognition. In this paper, a method based on projective geometry is introduced to correct the projective distortion of infrared images. The dual conic in distortion image, which contains all the information of projective rectification, is identified firstly, and then the projective transformation matrix is derived by decomposing the dual conic. Finally, optimization is carried out to get the optimal result. Experimental results show that the method used in this paper can remove the projective distortion of infrared image effectively.},
author = {Li, Xiaolu and He, Tao and Xu, Lijun and Chen, Lulu and Guo, Zhanshe},
booktitle = {2012 IEEE International Conference on Imaging Systems and Techniques Proceedings},
doi = {10.1109/IST.2012.6295549},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 IEEE International Conference on Imaging Systems and Techniques Proceedings/2012/Li et al/Li et al. - 2012 - Projective rectification of infrared image based on projective geometry.pdf:pdf},
isbn = {978-1-4577-1775-8},
month = jul,
pages = {356--360},
publisher = {IEEE},
title = {{Projective rectification of infrared image based on projective geometry}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6295549\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=.QT.projective+geometry.QT.},
year = {2012}
}
@article{Heun2013,
address = {New York, New York, USA},
author = {Heun, Valentin and Group, Fluid Interfaces and Kasahara, Shunichi and Corporation, Sony and Maes, Pattie},
doi = {10.1145/2468356.2468528},
isbn = {9781450319522},
journal = {CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13},
pages = {2939--2942},
publisher = {ACM Press},
title = {{Smarter Objects: Using AR Technology to Program Physical Objects and their Interactions}},
url = {http://dl.acm.org/citation.cfm?doid=2468356.2468528},
year = {2013}
}
@inproceedings{Drusch2011,
abstract = {Eye-tracking studies have provided us with interesting findings regarding the way users explore Web pages. Usually, visual fixations are represented using gaze plots and heatmaps. Probably one of the most cited studies on Web page exploration is the one by Nielsen [15] who demonstrated that users often read Web pages in an F-shaped pattern. However, this conclusion is based on aggregated data that do not represent any real user. In order to characterize and to compare scanpaths so as to uncover possible scan-patterns, a clustering method based on the Hausdorff distance has been applied to the data from 113 users. The results have shown that scan-patterns could be identified. Groups of users have been identified and their behaviors have been described with diverse eye-tracking metrics. The contributions of this study are outlined as well as its drawbacks. Research perspectives are proposed.},
address = {New York, New York, USA},
author = {Drusch, Gautier and Bastien, J. M. Christian and Dinet, J\'{e}r\^{o}me},
booktitle = {23rd French Speaking Conference on Human-Computer Interaction - IHM '11},
doi = {10.1145/2044354.2044356},
file = {:home/acmt/Dropbox/Documentos/Mendeley/23rd French Speaking Conference on Human-Computer Interaction - IHM '11/2011/Drusch, Bastien, Dinet/Drusch, Bastien, Dinet - 2011 - From gaze plots to eye fixation patterns using a clustering method based on Hausdorff distances.pdf:pdf},
isbn = {9781450308229},
keywords = {eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
pages = {1},
publisher = {ACM Press},
title = {{From gaze plots to eye fixation patterns using a clustering method based on Hausdorff distances}},
url = {http://dl.acm.org/citation.cfm?id=2044356 http://dl.acm.org/citation.cfm?doid=2044354.2044356},
year = {2011}
}
@incollection{Oyekoya2006a,
abstract = {The best interfaces are the most natural ones. They are unobtrusive and provide relevant information quickly and in ways that do not interfere with the task itself.},
annote = {- cited by: 1
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Oyekoya, OK and Stentiford, FWM},
booktitle = {Intelligent Spaces},
doi = {10.1007/978-1-84628-429-8\_17},
edition = {17},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Intelligent Spaces/2006/Oyekoya, Stentiford/Oyekoya, Stentiford - 2006 - Eye Tracking as a New Interface for Image Retrieval.pdf:pdf},
isbn = {978-1-84628-429-8},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {273--286},
publisher = {Springer London},
title = {{Eye Tracking as a New Interface for Image Retrieval}},
url = {http://link.springer.com/content/pdf/10.1007/978-1-84628-429-8\_17.pdf},
year = {2006}
}
@article{Li2011,
abstract = {Keystone correction is an essential operation for projector-based applications, especially in mobile scenarios. In this paper, we propose a handheld movable projection method that can freely project keystone-free content on a general flat surface without adding any markings or boundary on it. Such a projection system can give the user greater freedom of display control (such as viewing angle, distance, etc.), without suffering from keystone distortion. To achieve this, we attach a camera to the projector to form a camera-projector pair. A green frame with the same resolution as the projector screen is projected onto the screen. Particle filter is employed to track the green frame and the correction of the display content is then achieved by rectifying the projection region of interest into a rectangular area. We built a prototype system to validate the effectiveness of the method. Experimental results show that our method can continuously project distortion free content in real time with good performance.},
annote = {        From Duplicate 1 (                   An Effective Method for Movable Projector Keystone Correction                 - Li, Zhaorong; Wong, Kin-Hong; Gong, Yibo; Chang, Ming-Yuen )
                
- Coletar keywords
        
      },
author = {Li, Zhaorong and Wong, Kin-Hong and Gong, Yibo and Chang, Ming-Yuen},
doi = {10.1109/TMM.2010.2092421},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {Accuracy,Calibration,Cameras,Keystone correction,Mobile communication,Pixel,Three dimensional displays,Tracking,anamorphism,camera-projector pair,display control,display instrumentation,flat surface,handheld movable projection method,image resolution,keystone,keystone-free content,mobile projection,movable projector keystone correction,optical projectors,particle filter,particle filtering (numerical methods),projector screen},
mendeley-tags = {anamorphism,keystone},
month = feb,
number = {1},
pages = {155--160},
shorttitle = {Multimedia, IEEE Transactions on},
title = {{An Effective Method for Movable Projector Keystone Correction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5635340 http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5635340},
volume = {13},
year = {2011}
}
@article{Breen1996,
abstract = {We present several techniques for producing two visual and modeling effects in augmented reality. The first effect involves interactively calculating the occlusions between real and virtual objects. The second effect utilizes a collision detection algorithm to automatically move dynamic virtual objects until they come in contact with static real objects in augmented reality. All of the techniques utilize calibrated data derived from images of a real-world environment.},
author = {Breen, David E. and Whitaker, Ross T. and Rose, Eric and Tuceryan, Mihran},
doi = {10.1111/1467-8659.1530011},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer Graphics Forum/1996/Breen et al/Breen et al. - 1996 - Interactive Occlusion and Automatic Object Placement for Augmented Reality.pdf:pdf},
issn = {0167-7055},
journal = {Computer Graphics Forum},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
month = aug,
number = {3},
pages = {11--22},
title = {{Interactive Occlusion and Automatic Object Placement for Augmented Reality}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-8659.1530011/abstract http://doi.wiley.com/10.1111/1467-8659.1530011},
volume = {15},
year = {1996}
}
@article{Silva2012,
author = {Silva, Danilo Assis Nobre Dos S. and Araujo, Tiago Maritan Ugulino De and Dantas, Leonardo and Nobrega, Yurika Sato and Lima, Hozana Raquel Gomes De and Filho, Guido Lemos De Souza},
doi = {10.1109/SVR.2012.25},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Silva et al/Silva et al. - 2012 - FleXLIBRAS Description and Animation of Signs in Brazilian Sign Language.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {deaf,formal language,generation,humanoid avatars,libras,sign language},
month = may,
pages = {227--236},
publisher = {Ieee},
title = {{FleXLIBRAS: Description and Animation of Signs in Brazilian Sign Language}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297534},
year = {2012}
}
@article{Hoiem2007,
abstract = {Humans have an amazing ability to instantly grasp the overall 3D structure of a scene—ground orientation, relative positions of major landmarks, etc.—even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this “surface layout” of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis. In this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout.},
author = {Hoiem, Derek and Efros, Alexei A. and Hebert, Martial},
doi = {10.1007/s11263-006-0031-y},
file = {:home/acmt/Dropbox/Documentos/Mendeley/International Journal of Computer Vision/2007/Hoiem, Efros, Hebert/Hoiem, Efros, Hebert - 2007 - Recovering Surface Layout from an Image.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = feb,
number = {1},
pages = {151--172},
title = {{Recovering Surface Layout from an Image}},
url = {http://link.springer.com/article/10.1007/s11263-006-0031-y http://link.springer.com/10.1007/s11263-006-0031-y},
volume = {75},
year = {2007}
}
@book{Wainer2009,
address = {New York, New York, USA},
author = {Wainer, Gabriel A.},
edition = {2},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2009/Wainer/Wainer - 2009 - Discrete-Event Modeling and Simulation A Practitioner's Approach (Computational Analysis, Synthesis, and Design of Dynam.pdf:pdf},
isbn = {9781420053364},
pages = {486},
publisher = {CRC Press},
title = {{Discrete-Event Modeling and Simulation: A Practitioner's Approach (Computational Analysis, Synthesis, and Design of Dynamic Systems)}},
url = {http://www.amazon.com/Discrete-Event-Modeling-Simulation-Practitioners- Computational/dp/1420053361},
year = {2009}
}
@inproceedings{Bell2001,
abstract = {We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible.We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to avoid occlusion. Layout decisions from previous frames are taken into account to reduce visual discontinuities. We present augmented reality and virtual reality examples to which we have applied our approach, including a dynamically labeled and annotated environment.},
address = {New York, New York, USA},
author = {Bell, Blaine and Feiner, Steven and H\"{o}llerer, Tobias},
booktitle = {Proceedings of the 14th annual ACM symposium on User interface software and technology - UIST '01},
doi = {10.1145/502348.502363},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 14th annual ACM symposium on User interface software and technology - UIST '01/2001/Bell, Feiner, H\"{o}llerer/Bell, Feiner, H\"{o}llerer - 2001 - View management for virtual and augmented reality.pdf:pdf},
isbn = {158113438X},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {101},
publisher = {ACM Press},
title = {{View management for virtual and augmented reality}},
url = {http://dl.acm.org/citation.cfm?id=502363 http://portal.acm.org/citation.cfm?doid=502348.502363},
year = {2001}
}
@article{Behzadan2010,
author = {Behzadan, Amir H. and Kamat, Vineet R.},
doi = {10.1111/j.1467-8667.2009.00601.x},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer-Aided Civil and Infrastructure Engineering/2010/Behzadan, Kamat/Behzadan, Kamat - 2010 - Scalable Algorithm for Resolving Incorrect Occlusion in Dynamic Augmented Reality Engineering Environments.pdf:pdf},
issn = {10939687},
journal = {Computer-Aided Civil and Infrastructure Engineering},
month = jan,
number = {1},
pages = {3--19},
title = {{Scalable Algorithm for Resolving Incorrect Occlusion in Dynamic Augmented Reality Engineering Environments}},
url = {http://doi.wiley.com/10.1111/j.1467-8667.2009.00601.x},
volume = {25},
year = {2010}
}
@article{Srivastava2005,
address = {New York, New York, USA},
author = {Srivastava, Deepti and Narasimhan, Priya},
doi = {10.1145/1083217.1083226},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2005 workshop on Architecting dependable systems - WADS '05/2005/Srivastava, Narasimhan/Srivastava, Narasimhan - 2005 - Architectural support for mode-driven fault tolerance in distributed applications.pdf:pdf},
isbn = {1595931244},
journal = {Proceedings of the 2005 workshop on Architecting dependable systems - WADS '05},
keywords = {corba,cots systems,distributed systems,erance,fault tol-,modes,nsf ca-,partially supported by the,replication,software architecture,this work has been},
pages = {1--7},
publisher = {ACM Press},
title = {{Architectural support for mode-driven fault tolerance in distributed applications}},
url = {http://portal.acm.org/citation.cfm?doid=1083217.1083226},
year = {2005}
}
@article{Hara2005,
abstract = {Several techniques have been developed for recovering reflectance properties of real surfaces under unknown illumination. However, in most cases, those techniques assume that the light sources are located at inifinity, which cannot be applied safely to, for example, reflectance modeling of indoor environments. In this paper, we propose two types of methods to estimate the surface reflectance property of an object, as well as the position of a light source from a single view without the distant illumination assumption, thus relaxing the conditions in the previous methods. Given a real image and a 3D geometric model of an object with specular reflection as inputs, the first method estimates the light source position by fitting to the Lambertian diffuse component, while separating the specular and diffuse components by using an iterative relaxation scheme. Our second method extends that first method by using as input a specular component image, which is acquired by analyzing multiple polarization images taken from a single view, thus removing its constraints on the diffuse reflectance property. This method simultaneously recovers the reflectance properties and the light source positions by optimizing the linearity of a log-transformed Torrance-Sparrow model. By estimating the object's reflectance property and the light source position, we can freely generate synthetic images of the target object under arbitrary lighting conditions with not only source direction modification but also source-surface distance modification. Experimental results show the accuracy of our estimation framework.},
author = {Hara, Kenji and Nishino, Ko and Ikeuchi, Katsushi},
doi = {10.1109/TPAMI.2005.82},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Light,Photometry,Photometry: methods,Radiation Dosage,Radiometry,Radiometry: methods},
month = apr,
number = {4},
pages = {493--505},
pmid = {15794156},
title = {{Light source position and reflectance estimation from a single view without the distant illumination assumption.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15794156},
volume = {27},
year = {2005}
}
@misc{XJTek2010a,
author = {XJTek},
title = {{AnyLogic}},
url = {http://www.xjtek.com},
year = {2010}
}
@inproceedings{Bimber2006,
abstract = {This tutorial discusses the Spatial Augmented Reality (SAR) concept, its advantages and limitations. It will present examples of state-of-the-art display configurations, appropriate real-time rendering techniques, details about hardware and software implementations, and current areas of application. Specifically, it will describe techniques for optical combination using single/multiple spatially aligned mirror-beam splitters, image sources, transparent screens and optical holograms. Furthermore, it presents techniques for projectorbased augmentation of geometrically complex and textured display surfaces, and (along with optical combination) methods for achieving consistent illumination and occlusion effects. Emerging technologies that have the potential of enhancing future augmented reality displays will be surveyed.},
address = {New York, New York, USA},
author = {Bimber, Oliver and Raskar, Ramesh},
booktitle = {ACM SIGGRAPH 2006 Courses on - SIGGRAPH '06},
doi = {10.1145/1185657.1185796},
isbn = {1595933646},
pages = {1},
publisher = {ACM Press},
title = {{Modern approaches to augmented reality}},
url = {http://portal.acm.org/citation.cfm?doid=1185657.1185796},
year = {2006}
}
@article{Land2000,
abstract = {In cricket, a batsman watches a fast bowler's ball come toward him at a high and unpredictable speed, bouncing off ground of uncertain hardness. Although he views the trajectory for little more than half a second, he can accurately judge where and when the ball will reach him. Batsmen's eye movements monitor the moment when the ball is released, make a predictive saccade to the place where they expect it to hit the ground, wait for it to bounce, and follow its trajectory for 100-200 ms after the bounce. We show how information provided by these fixations may allow precise prediction of the ball's timing and placement. Comparing players with different skill levels, we found that a short latency for the first saccade distinguished good from poor batsmen, and that a cricket player's eye movement strategy contributes to his skill in the game.},
author = {Land, M F and McLeod, P},
doi = {10.1038/81887},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Nature neuroscience/2000/Land, McLeod/Land, McLeod - 2000 - From eye movements to actions how batsmen hit the ball.pdf:pdf},
issn = {1097-6256},
journal = {Nature neuroscience},
keywords = {Adult,Baseball,Baseball: physiology,Head Movements,Head Movements: physiology,Humans,Male,Motion Perception,Motion Perception: physiology,Motor Skills,Motor Skills: physiology,Psychomotor Performance,Psychomotor Performance: physiology,Pursuit,Reaction Time,Reaction Time: physiology,Saccades,Saccades: physiology,Smooth,Smooth: physiology,Sports,Sports: physiology},
month = dec,
number = {12},
pages = {1340--5},
pmid = {11100157},
title = {{From eye movements to actions: how batsmen hit the ball.}},
url = {http://www.nature.com/neuro/journal/v3/n12/abs/nn1200\_1340.html http://www.ncbi.nlm.nih.gov/pubmed/11100157},
volume = {3},
year = {2000}
}
@article{Macvean2013,
address = {New York, New York, USA},
author = {Macvean, Andrew and Robertson, Judy},
doi = {10.1145/2470654.2466163},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1251},
publisher = {ACM Press},
title = {{Understanding exergame users' physical activity, motivation and behavior over time}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466163},
year = {2013}
}
@inproceedings{Pinho2006,
abstract = {This chapter presents a study on the relative aspects to the interaction in atmospheres virtual immersed. General considerations are presented on atmospheres virtual immersed with focus in the basic forms of interaction, metaphors and interaction parameters, for soon afterwards to present the selection techniques and manipulation of objects, as well as tha navigation in atmospheres three-dimensional immersed.},
address = {Bel\'{e}m-PA},
author = {Pinho, M\'{a}rcio Serolli and Rebelo, Irla Bocianoski},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {149--172},
publisher = {SBC},
title = {{Intera\c{c}\~{a}o em Ambientes Virtuais Imersivos}},
year = {2006}
}
@article{Bay2008,
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF’s application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF’s usefulness in a broad range of topics in computer vision.},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {camera calibration,feature description,interest points,local features,object recognition},
month = jun,
number = {3},
pages = {346--359},
title = {{Speeded-Up Robust Features (SURF)}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314207001555},
volume = {110},
year = {2008}
}
@inproceedings{Duchowski2010,
abstract = {The scanpath comparison framework based on string editing is revisited. The previous method of clustering based on k-means "preevaluation" is replaced by the mean shift algorithm followed by elliptical modeling via Principal Components Analysis. Ellipse intersection determines cluster overlap, with fast nearest-neighbor search provided by the kd-tree. Subsequent construction of Y - matrices and parsing diagrams is fully automated, obviating prior interactive steps. Empirical validation is performed via analysis of eye movements collected during a variant of the Trail Making Test, where participants were asked to visually connect alphanumeric targets (letters and numbers). The observed repetitive position similarity index matches previously published results, providing ongoing support for the scanpath theory (at least in this situation). Task dependence of eye movements may be indicated by the global position index, which differs considerably from past results based on free viewing.},
address = {New York, New York, USA},
author = {Duchowski, Andrew T. and Driver, Jason and Jolaoso, Sheriff and Tan, William and Ramey, Beverly N. and Robbins, Ami},
booktitle = {Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10},
doi = {10.1145/1743666.1743719},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10/2010/Duchowski et al/Duchowski et al. - 2010 - Scanpath comparison revisited.pdf:pdf},
isbn = {9781605589947},
keywords = {eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
pages = {219},
publisher = {ACM Press},
title = {{Scanpath comparison revisited}},
url = {http://dl.acm.org/citation.cfm?id=1743719 http://portal.acm.org/citation.cfm?doid=1743666.1743719},
year = {2010}
}
@misc{CNISENAI2012,
author = {{CNI SENAI}},
title = {{Olimp\'{\i}ada do Conhecimento 2012 - Etapa Nacional}},
url = {http://www.senai.br/olimpiada},
urldate = {10/02/2012},
year = {2012}
}
@article{Knudsen2007,
abstract = {A mechanistic understanding of attention is necessary for the elucidation of the neurobiological basis of conscious experience. This chapter presents a framework for thinking about attention that facilitates the analysis of this cognitive process in terms of underlying neural mechanisms. Four processes are fundamental to attention: working memory, top-down sensitivity control, competitive selection, and automatic bottom-up filtering for salient stimuli. Each process makes a distinct and essential contribution to attention. Voluntary control of attention involves the first three processes (working memory, top-down sensitivity control, and competitive selection) operating in a recurrent loop. Recent results from neurobiological research on attention are discussed within this framework.},
author = {Knudsen, Eric I},
doi = {10.1146/annurev.neuro.30.051606.094256},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Annual review of neuroscience/2007/Knudsen/Knudsen - 2007 - Fundamental components of attention.pdf:pdf},
issn = {0147-006X},
journal = {Annual review of neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Attention,Attention: physiology,Brain,Brain: anatomy \& histology,Brain: physiology,Cognition,Cognition: physiology,Fixation,Humans,Memory,Neural Inhibition,Neural Inhibition: physiology,Neural Pathways,Neural Pathways: physiology,Ocular,Ocular: physiology,Parietal Lobe,Parietal Lobe: physiology,Prefrontal Cortex,Prefrontal Cortex: physiology,Short-Term,Short-Term: physiology},
month = jan,
pages = {57--78},
pmid = {17417935},
title = {{Fundamental components of attention.}},
url = {http://www.annualreviews.org/doi/pdf/10.1146/annurev.neuro.30.051606.094256 http://www.ncbi.nlm.nih.gov/pubmed/17417935},
volume = {30},
year = {2007}
}
@inproceedings{Liu2012a,
abstract = {Keystone distortion is a long-standing problem in stereoscopic cinematography. Keystone distortion occurs when a stereoscopic camera toes in to achieve a desirable disparity distribution. One particular problem from keystone distortion is vertical disparity, which often compromises stereoscopic 3D viewing experience. Keystone distortion can be corrected by applying a proper homography; however, this damages the desirable disparity distribution. This paper presents an approach to keystone correction for stereoscopic cinematography that both corrects keystone distortion and preserves the original disparity distribution. Our method formulates keystone correction as a spatially-varying warping problem. Our method eliminates the vertical disparities and preserves the original horizontal disparities by encoding them as data terms in the warping problem. The energy terms are designed to be quadratic and thus the keystone correction problem can be quickly solved using a sparse linear solver. Our experiment shows that our method can effectively solve the keystone problem while preserving desirable horizontal disparities.},
author = {Liu, Feng and Niu, Yuzhen and Jin, Hailin},
booktitle = {2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2012.6238901},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops/2012/Liu, Niu, Jin/Liu, Niu, Jin - 2012 - Keystone correction for stereoscopic cinematography.pdf:pdf},
isbn = {978-1-4673-1612-5},
issn = {2160-7508},
keywords = {Adaptive optics,Cameras,Cinematography,Optical distortion,Optical imaging,Stereo image processing,Videos,anamorphism,disparity distribution,distortion,homography,horizontal disparity preservation,keystone,keystone correction problem,keystone distortion,sparse linear solver,sparse matrices,spatially-carving warping problem,stereoscopic 3D viewing experience,stereoscopic camera,stereoscopic cinematography,vertical disparity elimination},
mendeley-tags = {anamorphism,keystone},
month = jun,
pages = {1--7},
publisher = {IEEE},
shorttitle = {Computer Vision and Pattern Recognition Workshops},
title = {{Keystone correction for stereoscopic cinematography}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6238901},
year = {2012}
}
@article{Oren2007,
author = {\"{O}ren, TI},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2007 Summer Computer Simulation \ldots/2007/\"{O}ren/\"{O}ren - 2007 - The importance of a comprehensive and integrative view of modeling and simulation.pdf:pdf},
journal = {Proceedings of the 2007 Summer Computer Simulation \ldots},
keywords = {augmented reality,simulation},
mendeley-tags = {augmented reality,simulation},
title = {{The importance of a comprehensive and integrative view of modeling and simulation}},
url = {http://dl.acm.org/citation.cfm?id=1358066},
year = {2007}
}
@book{Pullum2001,
author = {Pullum, Laura L.},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2001/Pullum/Pullum - 2001 - Software Fault Tolerance Techniques and Implementation (Artech House Computing Library).pdf:pdf},
isbn = {1580531377},
pages = {360},
publisher = {Artech House},
title = {{Software Fault Tolerance Techniques and Implementation (Artech House Computing Library)}},
url = {http://www.amazon.com/Software-Tolerance-Techniques-Implementation- Computing/dp/1580531377},
year = {2001}
}
@article{Alt2013,
address = {New York, New York, USA},
author = {Alt, Florian and Shirazi, Alireza Sahami and Kubitza, Thomas and Schmidt, Albrecht},
doi = {10.1145/2470654.2466226},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1709},
publisher = {ACM Press},
title = {{Interaction techniques for creating and exchanging content with public displays}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466226},
year = {2013}
}
@article{Boulanger2013,
address = {New York, New York, USA},
author = {Boulanger, Cati and Boulanger, Adam and de Greef, Lilian and Kearney, Andy and Sobel, Kiley and Transue, Russell and Sweedyk, Z and Dietz, Paul H. and Bathiche, Steven},
doi = {10.1145/2470654.2466160},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1243},
publisher = {ACM Press},
title = {{Stroke rehabilitation with a sensing surface}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466160},
year = {2013}
}
@article{Kuttal2013,
address = {New York, New York, USA},
author = {Kuttal, Sandeep Kaur and Sarma, Anita and Rothermel, Gregg},
doi = {10.1145/2470654.2466213},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1609},
publisher = {ACM Press},
title = {{Debugging support for end user mashup programming}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466213},
year = {2013}
}
@phdthesis{Cao2009,
abstract = {The recent trend towards miniaturization of projection technology indicates that handheld devices will soon have the ability to project information onto any surface, thus enabling interaction and applications that are not possible with current handheld devices. This opens up an emerging research area on interaction using handheld projectors. With the ability to project information, a handheld device can surmount the limitations of its small internal screen by creating a larger information display on an external surface. By doing so, the display and interaction space can be expanded to cover almost an entire physical environment. Large amounts of data can be displayed, a rich interaction vocabulary can be supported, and multiple co-located people can share the viewing experience at the same time. In this thesis, I investigate research issues involved in the design, implementation, and user performance and behaviors regarding the usage of interactive handheld projectors. I create a handheld projector interaction prototype platform, and explore interaction concepts and techniques to support both single and multi-user interaction using one or several handheld projectors. I also empirically investigate the user behaviors related to handheld projector usage, in terms of both quantitative interaction performance with pointing tasks, and qualitative social behaviors that emerge from a game application. This work is a multi-faceted investigation on handheld projector interaction, and will provide the groundwork for future research and development of interactive handheld projectors.},
author = {Cao, X},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2009/Cao/Cao - 2009 - Handheld projector interaction.pdf:pdf},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
pages = {180},
school = {University of Toronto},
title = {{Handheld projector interaction}},
url = {http://www.cs.toronto.edu/~caox/XiangCao\_PhD\_thesis.pdf},
year = {2009}
}
@inproceedings{Garstka2011,
abstract = {Simulation and modelling are key techniques used by computational scientists in many disciplines. Teaching students how to best use these methods is assisted by highly interactive, visual and motivational technology. Recent commodity pricing of depth ﬁeld cameras such as the Kinect makes an integrated approach to real time interactive teaching and learning of simulation and modelling feasible and exciting to students. We describe some of the techniques, software prototypes we have developed for teaching interactive simulation methods using devices such as the MS Kinect. We discuss examples including a discrete lattice model like the Ising model of a magnet, and continuous equation models such as ﬂuid ﬂow and speculate on the implications of ubiquitous depth-of-ﬁeld devices for highly interactive simulations for learning.},
address = {Colorado},
author = {Garstka, J and Peters, G},
booktitle = {8th IEEE Int. Workshop on Projector-Camera Sytems},
file = {:home/acmt/Dropbox/Documentos/Mendeley/8th IEEE Int. Workshop on Projector-Camera Sytems/2011/Garstka, Peters/Garstka, Peters - 2011 - View-dependent 3d projection using depth-image-based head tracking.pdf:pdf},
keywords = {Kinect,computational science education.,depth-of-ﬁeld,interative simulation,teaching technologies,visualisation technology},
pages = {52--58},
publisher = {IEEE Computer Society Press},
title = {{View-dependent 3d projection using depth-image-based head tracking}},
url = {http://lds62-112-144-233.my-simplyroot.de/wp-content/uploads/downloads/2011/12/0008.pdf},
year = {2011}
}
@article{Orji2013,
address = {New York, New York, USA},
author = {Orji, Rita and Mandryk, Regan L. and Vassileva, Julita and Gerling, Kathrin M.},
doi = {10.1145/2470654.2481341},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2467},
publisher = {ACM Press},
title = {{Tailoring persuasive health games to gamer type}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481341},
year = {2013}
}
@article{Liu2012b,
abstract = {In augmented reality, one of key tasks to achieve a convincing visual appearance consistency between virtual objects and video scenes is to have a coherent illumination along the whole sequence. As outdoor illumination is largely dependent on the weather, the lighting condition may change from frame to frame. In this paper, we propose a full image-based approach for online tracking of outdoor illumination variations from videos captured with moving cameras. Our key idea is to estimate the relative intensities of sunlight and skylight via a sparse set of planar feature-points extracted from each frame. To address the inevitable feature misalignments, a set of constraints are introduced to select the most reliable ones. Exploiting the spatial and temporal coherence of illumination, the relative intensities of sunlight and skylight are finally estimated by using an optimization process. We validate our technique on a set of real-life videos and show that the results with our estimations are visually coherent along the video sequences.},
author = {Liu, Yanli and Granier, Xavier},
doi = {10.1109/TVCG.2012.53},
file = {::},
issn = {1941-0506},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Algorithms,Computer Graphics,Humans,Lighting,Motion,User-Computer Interface,Video Recording,augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = apr,
number = {4},
pages = {573--80},
pmid = {22402684},
title = {{Online tracking of outdoor lighting variations for augmented reality with moving cameras.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6165138 http://www.ncbi.nlm.nih.gov/pubmed/22402684},
volume = {18},
year = {2012}
}
@inproceedings{Lee2007,
abstract = {Crowd simulation techniques have frequently been used to animate a large group of virtual humans in computer graphics applications. We present a data-driven method of simulating a crowd of virtual humans that exhibit behaviors imitating real human crowds. To do so, we record the motion of a human crowd from an aerial view using a camcorder, extract the two-dimensional moving trajectories of each individual in the crowd, and then learn an agent model from observed trajectories. The agent model decides each agent's actions based on features of the environment and the motion of nearby agents in the crowd. Once the agent model is learned, we can simulate a virtual crowd that behaves similarly to the real crowd in the video. The versatility and flexibility of our approach is demonstrated through examples in which various characteristics of group behaviors are captured and reproduced in simulated crowds.},
address = {San Diego, USA},
author = {Lee, KH and Choi, MG and Hong, Q and Lee, J},
booktitle = {Proceedings of the 2007 ACM \ldots},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2007 ACM \ldots/2007/Lee et al/Lee et al. - 2007 - Group behavior from video a data-driven approach to crowd simulation.pdf:pdf},
keywords = {crowd simulation,data-driven},
mendeley-tags = {crowd simulation,data-driven},
pages = {109--118},
publisher = {ACM},
title = {{Group behavior from video: a data-driven approach to crowd simulation}},
url = {http://dl.acm.org/citation.cfm?id=1272706},
year = {2007}
}
@article{Chin2006,
abstract = {This paper outlines the development and initial testing of a new hybrid computer cursor control system based on Eye Gaze Tracking (EGT) and electromyogram (EMG) processing for hands-free control of the computer cursor. The ultimate goal of the system is to provide an efficient computer interaction mechanism for individuals with severe motor disabilities (or specialized operators whose hands are committed to other tasks, such as surgeons, pilots, etc.) The paper emphasizes the enhancements that have been made on different areas of the architecture, with respect to a previous prototype developed by our group, and demonstrates the performance improvement verified for some of the enhancements.},
annote = {- cited by: 6
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Chin, Craig A and Barreto, Armando},
doi = {10.1109/IEMBS.2006.259595},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Conference proceedings ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engine. Conference/2006/Chin, Barreto/Chin, Barreto - 2006 - Enhanced hybrid electromyogramEye Gaze Tracking cursor control system for hands-free computer interaction.pdf:pdf},
issn = {1557-170X},
journal = {Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Conference},
keywords = {Algorithms,Computer Peripherals,Data Display,Electromyography,Electromyography: methods,Eye Movement Measurements,Eye Movements,Eye Movements: physiology,Facial Muscles,Facial Muscles: physiology,Fixation,Humans,Ocular,Ocular: physiology,User-Computer Interface,gaze analysis},
mendeley-tags = {gaze analysis},
month = jan,
pages = {2296--9},
pmid = {17946102},
title = {{Enhanced hybrid electromyogram/Eye Gaze Tracking cursor control system for hands-free computer interaction.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4462251 http://www.ncbi.nlm.nih.gov/pubmed/17946102},
volume = {1},
year = {2006}
}
@inproceedings{Tori2006,
address = {Bel\'{e}m},
author = {Tori, Romero and Kirner, Claudio},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {2--21},
publisher = {SBC},
title = {{Fundamentos de Realidade Virtual}},
year = {2006}
}
@article{Akpan2013,
address = {New York, New York, USA},
author = {Akpan, Imeh and Marshall, Paul and Bird, Jon and Harrison, Daniel},
doi = {10.1145/2470654.2481306},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2213},
publisher = {ACM Press},
title = {{Exploring the effects of space and place on engagement with an interactive installation}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481306},
year = {2013}
}
@inproceedings{Wu2006,
author = {Wu, Lifang and Meng, Xianglong and Liu, Xun and Chen, Shiju},
booktitle = {18th International Conference on Pattern Recognition (ICPR'06)},
doi = {10.1109/ICPR.2006.122},
isbn = {0-7695-2521-0},
issn = {1051-4651},
keywords = {Deformable models,Detection algorithms,Games,Information analysis,Information retrieval,Layout,Motion detection,Object segmentation,Tracking,Videos,adaptive object model,basketball,basketball detection,basketball segmentation,basketball videos,image motion analysis,image segmentation,motion tracking,object based video analysis,object detection,object segmentation,sport,target tracking,tracking,video retrieval,video signal processing},
language = {English},
mendeley-tags = {basketball,tracking},
pages = {319--322},
publisher = {IEEE},
title = {{A New Method of Object Segmentation in the Basketball Videos}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1698897},
volume = {1},
year = {2006}
}
@inproceedings{Grosch2007,
abstract = {Inserting virtual objects in real camera images with correct lighting is an active area of research. Current methods use a high dynamic range camera with a fish-eye lens to capture the incoming illumination. The main problem with this approach is the limitation to distant illumination. Therefore, the focus of our work is a real-time description of both near - and far-field illumination for interactive movement of virtual objects in the camera image of a real room. The daylight, which is coming in through the windows, produces a spatially varying distribution of indirect light in the room; therefore a near-field description of incoming light is necessary. Our approach is to measure the daylight from outside and to simulate the resulting indirect light in the room. To accomplish this, we develop a special dynamic form of the irradiance volume for real-time updates of indirect light in the room and combine this with importance sampling and shadow maps for light from outside. This separation allows object movements with interactive frame rates (10--17 fps). To verify the correctness of our approach, we compare images of synthetic objects with real objects.},
address = {New York, New York, USA},
author = {Grosch, Thorsten and Eble, Tobias and Mueller, Stefan},
booktitle = {Proceedings of the 2007 ACM symposium on Virtual reality software and technology - VRST '07},
doi = {10.1145/1315184.1315207},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Image (Rochester, N.Y.)/2007/Grosch, Eble, Mueller/Grosch, Eble, Mueller - 2007 - Consistent Interactive Augmentation of Live Camera Images with Correct Near-field Illumination.pdf:pdf},
isbn = {9781595938633},
keywords = {augmented image synthesis,era images,focussed on interactive augmentation,global illumination,indirect light,light is necessary because,of cam-,of the spatially varying,our work is therefore,showing a real room,under time-varying daylight,we},
number = {212},
pages = {125},
publisher = {ACM Press},
title = {{Consistent interactive augmentation of live camera images with correct near-field illumination}},
url = {http://portal.acm.org/citation.cfm?doid=1315184.1315207},
volume = {1},
year = {2007}
}
@article{Komogortsev2007,
author = {Komogortsev, Oleg V},
file = {::},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
title = {{EYE MOVEMENT PREDICTION BY OCULOMOTOR PLANT MODELING WITH KALMAN FILTER 2003-2007}},
url = {http://scholar.google.com.br/scholar?start=70\&q=identification+fixation+eye\&hl=pt-BR\&as\_sdt=2005\&sciodt=0,5\&cites=12738018990440336398\&scipsc=1\#5},
year = {2007}
}
@article{Paidimarri2006,
abstract = {igh-resolution portable projectors have become commodity items now to own – but not to use. It is not always possible to find a display area where the camera can be properly aligned so that an undistorted image be seen. We present a method to project an undistorted image using a digital projector on a piecewise-planar display area. We use uncalibrated structured light ranging to segment the unknown projection area and further compute the homographies that map the projector space to the camera space through each of the planes. The edge detection and point-correspondences are subpixel precise. Finally, we use these computed homographies to pre-warp the display image so that a distortion-free image is visible. Our results show a seamless and correct rectification with accurate segmentation of the planes.},
author = {Paidimarri, K and Chandran, S},
doi = {10.1007/11949619\_26},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer Vision, Graphics and Image Processing/2006/Paidimarri, Chandran/Paidimarri, Chandran - 2006 - Ad-Hoc Multi-planar Projector Displays.pdf:pdf},
journal = {Computer Vision, Graphics and Image Processing},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
pages = {289--298},
title = {{Ad-Hoc Multi-planar Projector Displays}},
url = {http://link.springer.com/chapter/10.1007/11949619\_26},
volume = {4338},
year = {2006}
}
@article{Hacisalihzade1993,
abstract = {An interactive program package for the acquisition, analysis and plotting of human eye movements is introduced. It is shown that the programs described in this paper can be used by scientists in a wide range of disciplines in spite of their different data analysis requirements. An example dealing with smooth pursuit tracking is given.},
author = {Hacisalihzade, Selim S. and Allen, John S. and Stark, Lawrence W.},
doi = {10.1016/0169-2607(93)90056-Q},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer Methods and Programs in Biomedicine/1993/Hacisalihzade, Allen, Stark/Hacisalihzade, Allen, Stark - 1993 - Computer analysis of eye movements.pdf:pdf},
issn = {01692607},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {gaze analysis,nystagmus},
mendeley-tags = {gaze analysis,nystagmus},
month = jul,
number = {3},
pages = {181--187},
title = {{Computer analysis of eye movements}},
url = {http://www.sciencedirect.com/science/article/pii/016926079390056Q http://linkinghub.elsevier.com/retrieve/pii/016926079390056Q},
volume = {40},
year = {1993}
}
@article{Foulsham2008,
abstract = {Saliency map models account for a small but significant amount of the variance in where people fixate, but evaluating these models with natural stimuli has led to mixed results. In the present study, the eye movements of participants were recorded while they viewed color photographs of natural scenes in preparation for a memory test (encoding) and when recognizing them later. These eye movements were then compared to the predictions of a well defined saliency map model (L. Itti \& C. Koch, 2000), in terms of both individual fixation locations and fixation sequences (scanpaths). The saliency model is a significantly better predictor of fixation location than random models that take into account bias toward central fixations, and this is the case at both encoding and recognition. However, similarity between scanpaths made at multiple viewings of the same stimulus suggests that repetitive scanpaths also contribute to where people look. Top-down recapitulation of scanpaths is a key prediction of scanpath theory (D. Noton \& L. Stark, 1971), but it might also be explained by bottom-up guidance. The present data suggest that saliency cannot account for scanpaths and that incorporating these sequences could improve model predictions.},
author = {Foulsham, Tom and Underwood, Geoffrey},
doi = {10.1167/8.2.6},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of vision/2008/Foulsham, Underwood/Foulsham, Underwood - 2008 - What can saliency models predict about eye movements Spatial and sequential aspects of fixations during enc.pdf:pdf},
issn = {1534-7362},
journal = {Journal of vision},
keywords = {Analysis of Variance,Cognition,Cognition: physiology,Eye Movements,Eye Movements: physiology,Eye Tracking,Fixation,Form Perception,Form Perception: physiology,Humans,Ocular,Ocular: physiology,Pattern Recognition,Photic Stimulation,Segmentation,Semantics,Task Performance and Analysis,Visual,Visual: physiology,scanpath,similarity},
mendeley-tags = {Eye Tracking,Segmentation,scanpath,similarity},
month = jan,
number = {2},
pages = {6.1--17},
pmid = {18318632},
title = {{What can saliency models predict about eye movements? Spatial and sequential aspects of fixations during encoding and recognition.}},
url = {http://jov.highwire.org/content/8/2/6.short http://www.ncbi.nlm.nih.gov/pubmed/18318632},
volume = {8},
year = {2008}
}
@incollection{Fitts1975,
address = {Belmont},
booktitle = {Human Performance},
editor = {Fitts, P. and Posner, M.},
pages = {42--82},
publisher = {Greenwood Press},
title = {{Component processes and performance capacities}},
year = {1975}
}
@misc{Martinez2013,
author = {Martinez, Enrique Sanchez},
keywords = {SBGames},
mendeley-tags = {SBGames},
title = {{Games Factory}},
url = {http://www.arrakis.es/~esanchez/},
urldate = {2013/07/23},
year = {2013}
}
@article{Tadokoro2000,
author = {Tadokoro, S and Kitano, H},
journal = {\ldots . ICRA'00. IEEE  \ldots},
keywords = {augmented reality,simulation},
mendeley-tags = {augmented reality,simulation},
title = {{The robocup-rescue project: A robotic approach to the disaster mitigation problem}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=845369},
year = {2000}
}
@article{Kratz2012,
abstract = {Video analysis of crowded scenes is challenging due to the complex motion of individual people in the scene. The collective motion of pedestrians form a crowd flow, but individuals often largely deviate from it as they anticipate and react to each other. Deviations from the crowd decreases the pedestrian’s efficiency: a sociological concept that measures the difference of actual motion from the intended speed and direction. In this paper, we derive a novel method for estimating pedestrian efficiency from videos. We first introduce a novel crowd motion model that encodes the temporal evolution of local motion patterns represented with directional statistics distributions. This model is then used to estimate the intended motion of pedestrians at every space-time location, which enables visual measurement of the pedestrian efficiency. We demonstrate the use of this pedestrian efficiency to detect unusual events and to track individuals in crowded scenes. Experimental results show that the use of pedestrian efficiency leads to state-of-the-art accuracy in these critical applications.},
author = {Kratz, Louis and Nishino, Ko},
doi = {10.1007/978-3-642-33765-9\_40},
journal = {Computer Vision – ECCV},
pages = {558--572},
title = {{Going with the Flow: Pedestrian Efficiency in Crowded Scenes}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33765-9\_40},
volume = {7575},
year = {2012}
}
@misc{Worldskills2012,
author = {International, Worldskills},
title = {{Worldskills}},
url = {www.worldskills.com},
urldate = {14/03/2012}
}
@article{Stampfl2003,
author = {Stampfl, P},
journal = {Proc. EUROGRAPHICS},
keywords = {augmented reality,sound},
mendeley-tags = {augmented reality,sound},
title = {{3deSoundBox–a Scalable, Platform-Independent 3D Sound System for Virtual and Augmented Reality Applications}},
url = {http://scholar.google.com.br/scholar?cites=15533066508098542028\&as\_sdt=2005\&sciodt=0,5\&hl=pt-BR\#8},
year = {2003}
}
@misc{Herrlinger,
author = {Herrlinger, Sebastian},
title = {{OpenRTMFP/Cumulus}},
url = {https://github.com/OpenRTMFP/Cumulus},
urldate = {17/01/2012}
}
@phdthesis{Lee2008a,
abstract = {Today, the primary use of projection technology is for creating large flat displays that provide a shared viewing experience for presentations or entertainment applications. While research projects have explored the powerful ability for projected light to create illusions that can reshape our perception and our interaction with surfaces in the environment, very few of these systems have had success in terms of commercial and consumer adoption. Part of this limited adoption can be attributed to the lack of practicality in the cost-of-operation due to the complexity of installation and reliability of execution. Often these systems require expert knowledge to perform system setup and calibration between the projected image and the physical surfaces to make these illusions effective. In this thesis, I present a technique for inherently adding object location discovery and tracking capabilities to commercial projectors. This is accomplished by introducing light sensors into the projection area and then spatially encoding the image area using a series of structured light patterns. This delivers a unique pattern of light to every pixel in the projector’s screen space directly encoding the location data using the projector itself. By unifying the image projection and location tracking technologies, many of the difficult calibration and alignment issues related to interactive projection and projected spatial augmented reality applications can be eliminated simplifying their implementation and execution. Furthermore, by creating a hybrid visible light and infrared light projector, a single calibration-free device can perform invisible location tracking of input devices while simultaneously presenting visible application content. I present a detailed description of the projector-based location discovery and tracking technique, a description of three prototype implementations, and a demonstration of the effectiveness of this simplification by re-implementing, and in some cases improving upon, several location-sensitive projector applications that have been previously executed using external calibration and tracking technologies.},
author = {Lee, JC},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2008/Lee/Lee - 2008 - Projector-based location discovery and tracking.pdf:pdf},
keywords = {Image projection,anamorphism,augmented reality,high-speed projection,incremental tracking.,infrared projection,keystone,location tracking,motion capture,multi-projector applications,projector alignment,projector calibration},
mendeley-tags = {anamorphism,keystone},
pages = {106},
school = {Carnegie Mellon University},
title = {{Projector-based location discovery and tracking}},
url = {http://ra.adm.cs.cmu.edu/anon/usr/ftp/hcii/CMU-HCII-08-102.pdf},
year = {2008}
}
@inproceedings{Batz2009,
author = {Batz, Georg and Lee, Kwang-Kyu and Wollherr, Dirk and Buss, Martin},
booktitle = {2009 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2009.5152413},
isbn = {978-1-4244-2788-8},
issn = {1050-4729},
keywords = {Control design,Force feedback,Force sensors,Industrial control,Least squares methods,Robot sensing systems,Service robots,Stability,Torque,Trajectory,basketball,tracking},
language = {English},
mendeley-tags = {basketball,tracking},
month = may,
pages = {514--519},
publisher = {IEEE},
title = {{Robot basketball: A comparison of ball dribbling with visual and force/torque feedback}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5152413},
year = {2009}
}
@book{Kalra2006,
address = {Berlin, Heidelberg},
doi = {10.1007/11949619},
editor = {Kalra, Prem K. and Peleg, Shmuel},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2006/Unknown/Unknown - 2006 - Computer Vision, Graphics and Image Processing.pdf:pdf},
isbn = {978-3-540-68301-8},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Computer Vision, Graphics and Image Processing}},
url = {http://www.springerlink.com/index/10.1007/11949619},
volume = {4338},
year = {2006}
}
@inproceedings{Modesto2006,
abstract = {The objective of this chapter is to present to the reader a brief introduction on the 3D representation 3D of human beings in virtual environments, the so called Virtual Avatars or Virtual Humans. Its basic characteristics and the ways of its movements generation are defined. In order to standardize the modeling and portability of the models created, the H-anim specification 1.1 and examples of its use are also presented.},
address = {Bel\'{e}m-PA},
author = {Modesto, F\'{a}bio Alexandre Caravieri and Brega, Jos\'{e} Remo Ferreira and Garcia, Marcelo de Brito and Meiguins, Bianchi Serique and Sementille, Ant\^{o}nio Carlos and Rodello, Ildeberto Aparecido and Junior, Rosevaldo dias de Souza},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {79--97},
publisher = {SBC},
title = {{Humanos Virtuais e Avatares}},
year = {2006}
}
@inproceedings{Kan2012,
abstract = {In this paper we present a novel high-quality rendering system for Augmented Reality (AR). We study ray-tracing based rendering techniques in AR with the goal of achieving real-time performance and improving visual quality as well as visual coherence between real and virtual objects in a final composited image. A number of realistic and physically correct rendering effects are demonstrated, that have not been presented in real-time AR environments before. Examples are high-quality specular effects such as caustics, refraction, reflection, together with a depth of field effect and anti-aliasing. We present a new GPU implementation of photon mapping and its application for the calculation of caustics in environments where real and virtual objects are combined. The composited image is produced on-the-fly without the need of any preprocessing step. A main contribution of our work is the achievement of interactive rendering speed for high-quality ray-tracing algorithms in AR setups. Finally we performed an evaluation to study how users perceive visual quality and visual coherence with different realistic rendering effects. The results of our user study show that in 40.1\% cases users mistakenly judged virtual objects as real ones. Moreover we show that high-quality rendering positively affects the perceived visual coherence.},
address = {Atlanta, GA},
author = {Kan, P. and Kaufmann, H},
booktitle = {2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2012.6402546},
file = {::},
isbn = {978-1-4673-4662-7},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = nov,
pages = {99--108},
publisher = {IEEE},
title = {{High-quality reflections, refractions, and caustics in Augmented Reality and their contribution to visual coherence}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6402546 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6402546},
year = {2012}
}
@book{opencv,
annote = {"This library is useful for practitioners, and is an excellent tool for those entering the field: it is a set of computer vision algorithms that work as advertised."
-William T. Freeman, Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology 
        
Learning OpenCV puts you in the middle of the rapidly expanding field of computer vision. Written by the creators of the free open source OpenCV library, this book introduces you to computer vision and demonstrates how you can quickly build applications that enable computers to "see" and make decisions based on that data. 
        
Computer vision is everywhere-in security systems, manufacturing inspection systems, medical image analysis, Unmanned Aerial Vehicles, and more. It stitches Google maps and Google Earth together, checks the pixels on LCD screens, and makes sure the stitches in your shirt are sewn properly. OpenCV provides an easy-to-use computer vision framework and a comprehensive library with more than 500 functions that can run vision code in real time. 
        
Learning OpenCV will teach any developer or hobbyist to use the framework quickly with the help of hands-on exercises in each chapter. This book includes:
A thorough introduction to OpenCV
Getting input from cameras
Transforming images
Segmenting images and shape matching
Pattern recognition, including face detection
Tracking and motion in 2 and 3 dimensions
3D reconstruction from stereo vision
Machine learning algorithms
Getting machines to see is a challenging but entertaining goal. Whether you want to build simple or sophisticated vision applications, Learning OpenCV is the book you need to get started.
      },
author = {Kaehler, Adrian and Bradski, Gary},
edition = {1},
isbn = {978-0-596-51613-0},
keywords = {SBGames},
mendeley-tags = {SBGames},
pages = {580},
publisher = {O'Reilly Media},
title = {{Learning OpenCV: Computer Vision with the OpenCV Library}},
url = {http://shop.oreilly.com/product/9780596516130.do},
year = {2008}
}
@misc{Technologies2013,
author = {Technologies, Unity},
keywords = {SBGames},
mendeley-tags = {SBGames},
title = {{Unity3D}},
url = {http://www.unity.com},
urldate = {2013/07/26},
year = {2013}
}
@inproceedings{Amrutur,
abstract = {A large fraction of scientific and engineering computations involve sparse matrices. While dense matrix computations can be parallelized relatively easily, sparse matrices with arbitrary or irregular structure pose a real challenge to designers of highly parallel machines. A recent paper by N.K. Karmarkar (1991) proposed a new parallel architecture for sparse matrix computations based on finite projective geometries. Mathematical structure of these geometries plays an important role in defining the interconnections between the processors and memories in this architecture, and also aids in efficiently solving several difficult problems (such as load balancing, data-routing, memory-access conflicts, etc.) that are encountered in the design of parallel systems. The authors discuss some of the key issues in the system design of such a machine, and show how exploiting the structure of the geometry results in an efficient hardware implementation of the machine. They also present circuit designs and simulation results for key elements of the system: a 200 MHz pipelined memory; a pipelined multiplier based on an adder unit with a delay of 2 ns; and a 500 Mbit/s CMOS input/output buffer},
author = {Amrutur, B.S. and Joshi, R. and Karmarkar, N.K.},
booktitle = {[1992] Proceedings of the International Conference on Application Specific Array Processors},
doi = {10.1109/ASAP.1992.218581},
file = {:home/acmt/Dropbox/Documentos/Mendeley/1992 Proceedings of the International Conference on Application Specific Array Processors/Unknown/Amrutur, Joshi, Karmarkar/Amrutur, Joshi, Karmarkar - Unknown - A projective geometry architecture for scientific computation.pdf:pdf},
isbn = {0-8186-2967-3},
pages = {64--80},
publisher = {IEEE Comput. Soc. Press},
title = {{A projective geometry architecture for scientific computation}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=218581\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=.QT.projective+geometry.QT.}
}
@inproceedings{Sung2010,
author = {Sung, Chang Ho and Moon, Il-chul and Kim, Tag Gon},
booktitle = {2010 19th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises},
doi = {10.1109/WETICE.2010.31},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2010 19th IEEE International Workshops on Enabling Technologies Infrastructures for Collaborative Enterprises/2010/Sung, Moon, Kim/Sung, Moon, Kim - 2010 - Collaborative Work in Domain-Specific Discrete Event Simulation Software Development Fleet Anti-air Defense Sim.pdf:pdf},
isbn = {978-1-4244-7216-1},
keywords = {-modeling and simulation,collaborative work,m,opment process,s,s stakeholders,software devel-},
pages = {160--165},
publisher = {IEEE},
title = {{Collaborative Work in Domain-Specific Discrete Event Simulation Software Development: Fleet Anti-air Defense Simulation Software}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5541948},
year = {2010}
}
@inproceedings{Kirner2007,
address = {Petropolis},
author = {Kirner, Claudio and Siscoutto, Robson A.},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2007/Machado/Machado - 2007 - Dispositivos H\'{a}pticos para Interfaces de Realidade Virtual e Aumentada.pdf:pdf},
pages = {2--21},
publisher = {SBC},
title = {{Fundamentos de Realidade Virtual e Aumentada}},
year = {2007}
}
@article{Rusu2010,
author = {Rusu, RB},
journal = {KI-K\"{u}nstliche Intelligenz},
title = {{Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments}},
url = {http://link.springer.com/article/10.1007/s13218-010-0059-6},
year = {2010}
}
@book{Westman2005,
author = {Westman, Hans},
booktitle = {ACM SIGGRAPH Computer Graphics},
doi = {10.1145/1080376.1080380},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM SIGGRAPH Computer Graphics/2005/Westman/Westman - 2005 - Computer graphics.pdf:pdf},
isbn = {1925884813},
issn = {00978930},
month = may,
number = {2},
pages = {4},
title = {{Computer graphics}},
url = {http://portal.acm.org/citation.cfm?doid=1080376.1080380},
volume = {39},
year = {2005}
}
@inproceedings{Haller2003,
abstract = {Shadows add a level of realism to a rendered image. Moreover, they are used as visual clues to determine spacial relationships and real-time shadows will gain importance in current real-time computer graphics applications for this reason. Twenty-five years ago, Crow published the shadow volume approach for determining shadowed regions of a scene. In this paper we present a modified real-time shadow volume algorithm that can be used in an Augmented/Mixed Reality application. Finally, the proposed concepts provide a novel sense of visual output of an AR application.},
address = {New York, New York, USA},
author = {Haller, Michael and Drab, Stephan and Hartmann, Werner},
booktitle = {Proceedings of the ACM symposium on Virtual reality software and technology - VRST '03},
doi = {10.1145/1008653.1008665},
isbn = {1581135696},
keywords = {augmented reality,shadow volumes},
pages = {56},
publisher = {ACM Press},
title = {{A real-time shadow approach for an augmented reality application using shadow volumes}},
url = {http://portal.acm.org/citation.cfm?doid=1008653.1008665},
year = {2003}
}
@article{Jokela2013,
address = {New York, New York, USA},
author = {Jokela, Tero and Lucero, Andr\'{e}s},
doi = {10.1145/2470654.2466459},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3355},
publisher = {ACM Press},
title = {{A comparative evaluation of touch-based methods to bind mobile devices for collaborative interactions}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466459},
year = {2013}
}
@inproceedings{Salvucci2000a,
abstract = {This paper describes EyeTracer, an interactive environment for manipulating, viewing, and analyzing eye-movement protocols. EyeTracer augments the typical functionality of such systems by incorporating model-based tracing algorithms that interpret protocols with respect to the predictions of a cognitive process model. These algorithms provide robust strategy classification and fixation assignment that help to alleviate common difficulties with eye-movement data, such as equipment noise and individual variability. Using the tracing algorithms for analysis and visualization, EyeTracer facilitates both exploratory analysis for initial understanding of behavior and confirmatory analysis for model evaluation and refinement.},
address = {New York, New York, USA},
annote = {- cited by: 27
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Salvucci, Dario D.},
booktitle = {Proceedings of the symposium on Eye tracking research \& applications - ETRA '00},
doi = {10.1145/355017.355026},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the symposium on Eye tracking research \& applications - ETRA '00/2000/Salvucci/Salvucci - 2000 - An interactive model-based environment for eye-movement protocol analysis and visualization.pdf:pdf},
isbn = {1581132808},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {57--63},
publisher = {ACM Press},
title = {{An interactive model-based environment for eye-movement protocol analysis and visualization}},
url = {http://dl.acm.org/citation.cfm?id=355026 http://portal.acm.org/citation.cfm?doid=355017.355026},
year = {2000}
}
@article{LeMeur2013,
abstract = {In this article, we are interested in the computational modeling of visual attention. We report methods commonly used to assess the performance of these kinds of models. We survey the strengths and weaknesses of common assessment methods based on diachronic eye-tracking data. We then illustrate the use of some methods to benchmark computational models of visual attention.},
annote = {crossRef(Jarodzka2010)},
author = {{Le Meur}, Olivier and Baccino, Thierry},
doi = {10.3758/s13428-012-0226-9},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Behavior research methods/2013/Le Meur, Baccino/Le Meur, Baccino - 2013 - Methods for comparing scanpaths and saliency maps strengths and weaknesses.pdf:pdf},
issn = {1554-3528},
journal = {Behavior research methods},
keywords = {Algorithms,Attention,Attention: classification,Biological,Differential Threshold,Eye Movement Measurements,Eye Movements,Humans,Models,Optic Flow,Pattern Recognition,ROC Curve,Visual,Visual Field Tests,Visual Fields,Visual: classification,eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
month = mar,
number = {1},
pages = {251--66},
pmid = {22773434},
title = {{Methods for comparing scanpaths and saliency maps: strengths and weaknesses.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22773434},
volume = {45},
year = {2013}
}
@inproceedings{Zhao2013,
abstract = {The growing use of social media means that an increasing amount of people's lives are visible online. We draw from Goffman's theatrical metaphor and Hogan's exhibition approach to explore how people manage their personal collection of social media data over time. We conducted a qualitative study of 13 participants to reveal their day-to-day decision-making about producing and curating digital traces on Facebook. Their goals and strategies showed that people experience the Facebook platform as consisting of three different functional regions: a performance region for managing recent data and impression management, an exhibition region for longer term presentation of self-image, and a personal region for archiving meaningful facets of life. Further, users' need for presenting and archiving data in these three regions is mediated by temporality. These findings trigger a discussion of how to design social media that support these dynamic and sometimes conflicting needs.},
address = {New York, New York, USA},
author = {Zhao, Xuan and Salehi, Niloufar and Naranjit, Sasha and Alwaalan, Sara and Voida, Stephen and Cosley, Dan},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470656},
isbn = {9781450318990},
pages = {1--10},
publisher = {ACM Press},
title = {{The many faces of facebook: experiencing social media as performance, exhibition, and personal archive}},
url = {http://stephen.voida.com/uploads/Publications/Publications/zhao-chi13.pdf http://dl.acm.org/citation.cfm?doid=2470654.2470656},
year = {2013}
}
@inproceedings{Vouzounaras2010,
abstract = {In this paper, a novel method is proposed able to automatically generate accurate 3D models of both outdoor buildings and indoor scenes with perspective cues from line segments that are automatically extracted from a single image with an uncalibrated camera. The proposed method uses geometric constraints and knowledge of photography and achieves an accurate, real-time and fully automated 3D reconstruction of the scene without any intervention from the user.},
address = {New York, New York, USA},
author = {Vouzounaras, Georgios and {Perez-Moneo Agapito}, Juan Diego and Daras, Petros and Strintzis, Michael G.},
booktitle = {Proceedings of the 2010 ACM workshop on Surreal media and virtual cloning - SMVC '10},
doi = {10.1145/1878083.1878100},
isbn = {9781450301756},
keywords = {constraints,curvilinear modality,hierarchical,photorealistic,projective correction,reconstruction,single picture,single-view 3d reconstruction,vanishing point detection},
mendeley-tags = {reconstruction},
month = jun,
pages = {63},
publisher = {ACM Press},
title = {{3D reconstruction of indoor and outdoor building scenes from a single image}},
url = {http://dl.acm.org/citation.cfm?id=1082121.1082135 http://portal.acm.org/citation.cfm?doid=1878083.1878100 http://dl.acm.org/citation.cfm?id=1878083.1878100},
year = {2010}
}
@article{Lektauers2005,
author = {Lektauers, A},
file = {::},
journal = {Proceedings of the 19th European Conference on  \ldots},
keywords = {augmented reality,simulation},
mendeley-tags = {augmented reality,simulation},
title = {{A mixed reality framework for visualization and execution of DEVS-based simulation models}},
url = {http://www.comp.glam.ac.uk/ASMTA2005/Proc/pdf/vv-05.pdf},
year = {2005}
}
@inproceedings{Chilana2013,
abstract = {We present a multi-site field study to evaluate LemonAid, a crowdsourced contextual help approach that allows users to retrieve relevant questions and answers by making selections within the interface. We deployed LemonAid on 4 different web sites used by thousands of users and collected data over several weeks, gathering over 1,200 usage logs, 168 exit surveys, and 36 one-on-one interviews. Our results indicate that over 70\% of users found LemonAid to be helpful, intuitive, and desirable for reuse. Software teams found LemonAid easy to integrate with their sites and found the analytics data aggregated by LemonAid a novel way of learning about users' popular questions. Our work provides the first holistic picture of the adoption and use of a crowdsourced contextual help system and offers several insights into the social and organizational dimensions of implementing such help systems for real-world applications.},
address = {New York, New York, USA},
author = {Chilana, Parmit K and Ko, Andrew J and Wobbrock, Jacob O and Grossman, Tovi},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470685},
isbn = {9781450318990},
pages = {217--226},
publisher = {ACM Press},
title = {{A multi-site field study of crowdsourced contextual help}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470685},
year = {2013}
}
@inproceedings{Dietz2004,
abstract = {Recent advances in computer video projection open up new possibilities for real-time interactive, persuasive displays. Now a display can continuously adapt to a viewer so as to maximize its effectiveness. However, by the very nature of persuasion, these displays must be both immersive and subtle. We have been working on technologies that support this application including multi-projector and implicit interaction techniques. These technologies have been used to create a series of interactive persuasive displays that are described.},
address = {New York, New York, USA},
author = {Dietz, Paul and Raskar, Ramesh and Booth, Shane and van Baar, Jeroen and Wittenburg, Kent and Knep, Brian},
booktitle = {Proceedings of the working conference on Advanced visual interfaces - AVI '04},
doi = {10.1145/989863.989898},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the working conference on Advanced visual interfaces - AVI '04/2004/Dietz et al/Dietz et al. - 2004 - Multi-projectors and implicit interaction in persuasive public displays.pdf:pdf},
isbn = {1581138679},
keywords = {anamorphism},
mendeley-tags = {anamorphism},
pages = {209},
publisher = {ACM Press},
title = {{Multi-projectors and implicit interaction in persuasive public displays}},
url = {http://dl.acm.org/citation.cfm?id=989898 http://portal.acm.org/citation.cfm?doid=989863.989898},
year = {2004}
}
@misc{Simscript2011,
author = {Simscript},
title = {{Simscript}},
url = {http://www.simscript.com},
year = {2011}
}
@inproceedings{Han2003,
abstract = {It is common experience for human vision to perceive full 3D shape and scene from a single 2D image with the occluded parts "filled-in" by prior visual knowledge. We represent prior knowledge of 3D shapes and scenes by probabilistic models at two levels - both are defined on graphs. The first level model is built on a graph representation for single objects, and it is a mixture model for both man-made block objects such as trees and grasses. It assumes surface and boundary smoothness, 3D angle symmetry etc. The second level model is built on the relation graph of all objects in a scene. It assumes that objects should be supported for maximum stability with global bounding surfaces, such as ground, sky and walls. Given an input image, we extract the geometry and photometric structures through image segmentation and sketching, and represent them in a big graph. Then we partition the graph into subgraphs each being an object, infer the 3D shape and recover occluded surfaces, edges and vertices in each subgraph, and infer the scene structures between the recovered 3D sub-graphs. The inference algorithm samples from the prior model under the constraint that it reproduces the observed image/sketch under projective geometry.},
address = {Nice, France},
author = {Han, F and Zhu, SC},
booktitle = {First IEEE International Workshop on Higher-Level Knowledge in 3D Modeling and Motion Analysis, 2003. HLK 2003.},
doi = {10.1109/HLK.2003.1240854},
isbn = {0-7695-2049-9},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {12--20},
publisher = {IEEE},
title = {{Bayesian reconstruction of 3D shapes and scenes from a single image}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1240854 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240854},
year = {2003}
}
@article{Loscos2000,
abstract = {Computer augmented reality (CAR) is a rapidly emerging field which enables users to mix real and virtual worlds. Our goal is to provide interactive tools to perform common illumination, i.e., light interactions between real and virtual objects, including shadows and relighting (real and virtual light source modification). In particular, we concentrate on virtually modifying real light source intensities and inserting virtual lights and objects into a real scene; such changes can be very useful for virtual lighting design and prototyping. To achieve this, we present a three-step method. We first reconstruct a simplified representation of real scene geometry using semiautomatic vision-based techniques. With the simplified geometry, and by adapting recent hierarchical radiosity algorithms, we construct an approximation of real scene light exchanges. We next perform a preprocessing step, based on the radiosity system, to create unoccluded illumination textures. These replace the original scene textures which contained real light effects such as shadows from real lights. This texture is then modulated by a ratio of the radiosity (which can be changed) over a display factor which corresponds to the radiosity for which occlusion has been ignored. Since our goal is to achieve a convincing relighting effect, rather than an accurate solution, we present a heuristic correction process which results in visually plausible renderings. Finally, we perform an interactive process to compute new illumination with modified real and virtual light intensities},
author = {Loscos, C. and Drettakis, George and Robert, Luc},
doi = {10.1109/2945.895874},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {4},
pages = {289--305},
title = {{Interactive virtual relighting of real scenes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=895874},
volume = {6},
year = {2000}
}
@phdthesis{Bergstrand2008,
abstract = {The movement of a persons eyes is an interesting factor to study in different research areas where attention is important, for example driving. In 2004 the Swedish national road and transport research institute (VTI) introduced Simu- lator III – their third generation of driving simulators. Inside Simulator III a camera based eye tracking system is installed that records the eye movements of the driver. To be useful, the raw data from the eye tracking system needs to be analyzed and concentrated into a number of measures relevant for the research at VTI. This thesis presents methods to analyze the data from the eye tracker and transform it into something more useful. A world coordinate system is setup to connect the eye tracking system with the real world in a consistent way. A set of measures is collected, mainly from ISO and SAE standards, to be used as output from the analysis. Finally an application is developed for performing the analysis. The application reads the data from the eye tracker and the simulator, analyzes the data, and outputs a set of eye movement measures usable for the researchers at VTI.},
author = {Bergstrand, M},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2008/Bergstrand/Bergstrand - 2008 - Automatic analysis of eye tracker data from a driving simulator.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {63},
school = {Link\"{o}pings unversitet},
title = {{Automatic analysis of eye tracker data from a driving simulator}},
url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2:636736},
year = {2008}
}
@article{Sodnik2006,
author = {Sodnik, J and Tomazic, S and Grasset, R},
journal = {Proceedings of the 18th \ldots},
keywords = {augmented reality,sound},
mendeley-tags = {augmented reality,sound},
title = {{Spatial sound localization in an augmented reality environment}},
url = {http://dl.acm.org/citation.cfm?id=1228197},
year = {2006}
}
@article{Yamazoe,
abstract = {This paper proposes a depth measurement error model of consumer depth cameras such as Microsoft KINECT, and its calibration method. These devices are originally designed for video game interface, thus, the obtained depth map are not enough accurate for 3D measurement. To decrease these depth errors, several models have been proposed, however, these models consider only camera-related parameters. Since the depth sensors are based on projector-camera systems, we should consider projector-related parameters. Therefore, we propose the error model of the consumer depth cameras especially the KINECT, considering both intrinsic parameters of the camera and the projector. To calibrate the error model, we also propose the parameter estimation method by only showing a planar board to the depth sensors. Our error model and its calibration are necessary step for using the KINECT as a 3D measuring device. Experimental results show the validity and effectiveness of the error model and its calibration.},
author = {Yamazoe, H. and Habe, H. and Mitsugami, I. and Yagi, Y.},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/Unknown/Yamazoe et al/Yamazoe et al. - Unknown - Easy depth sensor calibration.pdf:pdf},
title = {{Easy depth sensor calibration}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6460172\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=kinect+calibration}
}
@inproceedings{Ryan2008,
abstract = {We present a low-cost wearable eye tracker built from off-the-shelf components. Based on the open source openEyes project (the only other similar effort that we are aware of), our eye tracker operates in the visible spectrum and variable lighting conditions. The novelty of our approach rests in automatically switching between tracking the pupil/iris boundary in bright light to tracking the iris/sclera boundary (limbus) in dim light. Additional improvements include a semi-automatic procedure for calibrating the eye and scene cameras, as well as an automatic procedure for initializing the location of the pupil in the first image frame. The system is accurate to two degrees visual angle in both indoor and outdoor environments.},
address = {New York, New York, USA},
author = {Ryan, Wayne J. and Duchowski, Andrew T. and Birchfield, Stan T.},
booktitle = {Proceedings of the 2008 symposium on Eye tracking research \& applications - ETRA '08},
doi = {10.1145/1344471.1344487},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2008 symposium on Eye tracking research \& applications - ETRA '08/2008/Ryan, Duchowski, Birchfield/Ryan, Duchowski, Birchfield - 2008 - Limbuspupil switching for wearable eye tracking under variable lighting conditions.pdf:pdf},
isbn = {9781595939821},
keywords = {eye tracker},
mendeley-tags = {eye tracker},
pages = {61},
publisher = {ACM Press},
title = {{Limbus/pupil switching for wearable eye tracking under variable lighting conditions}},
url = {http://dl.acm.org/citation.cfm?id=1344487 http://portal.acm.org/citation.cfm?doid=1344471.1344487},
year = {2008}
}
@inproceedings{Cruz-Neira1993,
address = {New York, New York, USA},
author = {Cruz-Neira, Carolina and Sandin, Daniel J. and DeFanti, Thomas A.},
booktitle = {Proceedings of the 20th annual conference on Computer graphics and interactive techniques - SIGGRAPH '93},
doi = {10.1145/166117.166134},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 20th annual conference on Computer graphics and interactive techniques - SIGGRAPH '93/1993/Cruz-Neira, Sandin, DeFanti/Cruz-Neira, Sandin, DeFanti - 1993 - Surround-screen projection-based virtual reality.pdf:pdf},
isbn = {0897916018},
pages = {135--142},
publisher = {ACM Press},
title = {{Surround-screen projection-based virtual reality}},
url = {http://dl.acm.org/citation.cfm?id=166134 http://portal.acm.org/citation.cfm?doid=166117.166134},
year = {1993}
}
@article{Ferreira2012,
author = {Ferreira, Alessandro Luiz Stamatto and Santos, Selan Rodrigues Dos and Miranda, Leonardo Cunha De},
doi = {10.1109/SVR.2012.14},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Ferreira, Santos, Miranda/Ferreira, Santos, Miranda - 2012 - TrueSight A Pedestrian Navigation System Based in Automatic Landmark Detection and Extraction on Andr.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {-landmark,Android,Landmark Recognition,Localization,Mobile Device,Navigation},
month = may,
pages = {91--99},
publisher = {Ieee},
title = {{TrueSight A Pedestrian Navigation System Based in Automatic Landmark Detection and Extraction on Android Smartphone}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297564},
year = {2012}
}
@inproceedings{SilveiraJacques2006,
abstract = {This paper presents a new adaptive background model for grayscale video sequences, that includes shadows and highlight detection. In the training period, statistics are computed for each image pixel to obtain the initial background model and an estimate of the image global noise, even in the presence of several moving objects. Each new frame is then compared to this background model, and spatio-temporal features are used to obtain foreground pixels. Local statistics are then used to detect shadows and highlights, and pixels that are detected as either shadow or highlight for a certain number of frames are adapted to become part of the background. Experimental results indicate that the proposed algorithm can effectively detect shadows and highlights, adapting the background with respect to illumination changes},
address = {Atlanta, GA},
author = {{Silveira Jacques}, Julio and Jung, Claudio and Musse, Soraia},
booktitle = {2006 International Conference on Image Processing},
doi = {10.1109/ICIP.2006.312599},
isbn = {1-4244-0480-0},
issn = {1522-4880},
keywords = {background subtraction,computer vision},
mendeley-tags = {background subtraction,computer vision},
pages = {1817--1820},
publisher = {IEEE},
shorttitle = {Image Processing, 2006 IEEE International Conferen},
title = {{A Background Subtraction Model Adapted to Illumination Changes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4106905},
year = {2006}
}
@article{VanDam2000,
author = {van Dam, a.},
doi = {10.1109/38.814559},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Computer Graphics And Applications/2000/van Dam/van Dam - 2000 - Beyond WIMP.pdf:pdf},
issn = {02721716},
journal = {IEEE Computer Graphics And Applications},
number = {1},
pages = {50--51},
title = {{Beyond WIMP}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=814559},
volume = {20},
year = {2000}
}
@incollection{Regis2012,
abstract = {This paper focuses on ocular measurement to detect the human operator’s particular state of “attentional tunnelling” during a robot supervisory task. After a survey of the existing ocular metrics, an innovative fixation detection algorithm is proposed. Then the metrics derived from the ocular parameters calculated by the algorithm are tested in a human-robot experiment. Among the metrics calculated, 3 of them appear to be able to statisticaly discrimintate the operators who faced attentional tunnelling.},
address = {Toulouse, France.},
annote = {- cited by: 1
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Regis, N and Dehais, F and Tessier, C and Giagnon, J. F.},
booktitle = {Human Factors and Ergonomics Society},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Human Factors and Ergonomics Society/2012/Regis et al/Regis et al. - 2012 - Ocular metrics for detecting attentional tunnelling.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {121--132},
title = {{Ocular metrics for detecting attentional tunnelling}},
url = {http://www.hfes-europe.org/books/proceedings2012/regis.pdf},
year = {2012}
}
@inproceedings{West2006,
abstract = {Fixation sequence analysis can reveal the cognitive strategies that drive eye movements. Unfortunately this type of analysis is not as common as other popular eye movement measures, such as fixation duration and trace length, because the proper tools for fixation sequence analysis are not incorporated into most popular eye movement software. This paper describes eyePatterns, a new tool for discovering similarities in fixation sequences and identifying the experimental variables that may influence their characteristics.},
address = {New York, New York, USA},
author = {West, Julia M. and Haake, Anne R. and Rozanski, Evelyn P. and Karn, Keith S.},
booktitle = {Proceedings of the 2006 symposium on Eye tracking research \& applications - ETRA '06},
doi = {10.1145/1117309.1117360},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2006 symposium on Eye tracking research \& applications - ETRA '06/2006/West et al/West et al. - 2006 - eyePatterns software for identifying patterns and similarities across fixation sequences.pdf:pdf},
isbn = {1595933050},
keywords = {eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
pages = {149},
publisher = {ACM Press},
title = {{eyePatterns: software for identifying patterns and similarities across fixation sequences}},
url = {http://dl.acm.org/citation.cfm?id=1117360 http://portal.acm.org/citation.cfm?doid=1117309.1117360},
year = {2006}
}
@inproceedings{Sun2013,
abstract = {Architectural sketching and massing are used by designers to analyze and explore the design space of buildings. This paper describes a novel multi-touch interface for fast architectural sketching and massing of tall buildings. It incorporates a family of multi-touch gestures, enabling one to quickly sketch the 2D contour of a base floor plan and extrude it to model a building with multi-floor structures. Further, it provides a set of gestures to users: select and edit a range of floors; scale contours of a building; copy, paste, and rotate a building, i.e., create a twisted structure; edit profile curves of a building's profile; and collapse and remove a selected range of floors. The multi-touch system also allows users to apply textures or geometric facades to the building, and to compare different designs side-by-side. To guide the design process, we describe interactions with a domain expert, a practicing architect. The final interface is evaluated by architects and students in an architecture Dept., which demonstrates that the system allows rapid conceptual design and massing of novel multi-story building structures.},
address = {New York, New York, USA},
author = {Sun, Qian and Lin, Juncong and Fu, Chi-Wing and Kaijima, Sawako and He, Ying},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470689},
isbn = {9781450318990},
pages = {247--256},
publisher = {ACM Press},
title = {{A multi-touch interface for fast architectural sketching and massing}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470689},
year = {2013}
}
@book{Casse2006,
abstract = {This lucid and accessible text provides an introductory guide to projective geometry, an area of mathematics concerned with the properties and invariants of geometric figures under projection. Including numerous worked examples and exercises throughout, the book covers axiomatic geometry, field planes and PG(r, F), coordinating a projective plane, non-Desarguesian planes, conics and quadrics in PG(3, F). Assuming familiarity with linear algebra, elementary group theory, partial differentiation and finite fields, as well as some elementary coordinate geometry, this text is ideal for 3rd and 4th year lucid and accessible text provides an introductory guide to projective geometry, an area of mathematics concerned with the properties and invariants of geometric figures under projection. Including numerous worked examples and exercises throughout, the book covers axiomatic geometry, field planes and PG(r, F), coordinatising a projective plane, non-Desarguesian planes, conics and quadrics in PG(3, F). Assuming familiarity with linear algebra, elementary group theory, partial differentiation and finite fields, as well as some elementary coordinate geometry, this text is ideal for 3rd and 4th year mathematics undergraduates.},
author = {Casse, Rey},
booktitle = {Nature},
doi = {10.1038/141535b0},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Nature/2006/Casse/Casse - 2006 - An Introduction to Projective Geometry.pdf:pdf},
isbn = {9781402042485},
issn = {00280836},
number = {3569},
pages = {535--535},
publisher = {Oxford university Press},
title = {{An Introduction to Projective Geometry}},
url = {http://books.google.at/books?hl=de\&id=9khtJHzmtTgC\&dq=casse+projective+geometry\&printsec=frontcover\&source=web\&ots=vOHQvhknVT\&sig=z6EIUjfanqekH082kJ-hmHtoJKc\&sa=X\&oi=book\_result\&resnum=10\&ct=result},
volume = {141},
year = {2006}
}
@misc{Oracle,
author = {Oracle},
title = {{MySQL}},
url = {http://www.mysql.com/},
urldate = {17/01/2012}
}
@inproceedings{Leao2010,
abstract = {Aplica\c{c}\~{o}es de RA (Realidade Aumentada) s\~{a}o as que sobrep\~{o}em objetos virtuais em uma cena real no contexto correto, visando adicionar informa\c{c}\~{a}o para o usu\'{a}rio final. O presente trabalho se prop\~{o}e a desenvolver uma aplica\c{c}\~{a}o capaz de simular modifica\c{c}\~{o}es em objetos reais presentes em aplica\c{c}\~{o}es de RA. \'{E} proposta uma abordagem para simular a modifica\c{c}\~{a}o de objetos reais em aplica\c{c}\~{o}es de RA, atrav\'{e}s da sobreposi\c{c}\~{a}o de uma objeto com uma r\'{e}plica 3D propositalmente modificada do mesmo. O trabalho utiliza t\'{e}cnicas de textura din\^{a}mica e de inpaint para aprimorar a resposta visual da modifica\c{c}\~{a}o realizada. Os resultados obtidos s\~{a}o satisfat\'{o}rios tanto em rela\c{c}\~{a}o ao realismo da altera\c{c}\~{a}o do objeto real como em rela\c{c}\~{a}o \`{a} performance da aplica\c{c}\~{a}o.},
author = {Le\~{a}o, CWM and Lima, JP and Teichrieb, V},
booktitle = {Workshop de Realidade Virtual e Aumentada},
keywords = {augmented reality,geometry,tracking},
mendeley-tags = {augmented reality,geometry,tracking},
title = {{Modifica\c{c}\~{o}es geom\'{e}tricas aplicadas a elementos reais em aplica\c{c}\~{o}es de RA}},
url = {http://www.lbd.dcc.ufmg.br/colecoes/wrva/2010/0041.pdf},
year = {2010}
}
@inproceedings{Teather2013,
abstract = {We present a study of cursors for selecting 2D-projected 3D targets. We compared a stereo- and mono-rendered (one-eyed) cursor using two mouse-based and two remote pointing techniques in a 3D Fitts' law pointing experiment. The first experiment used targets at fixed depths. Results indicate that one-eyed cursors only improve screen-plane pointing techniques, and that constant target depth does not influence pointing throughput. A second experiment included pointing between targets at varying depths and used only "screen-plane" pointing techniques. Our results suggest that in the absence of stereo cue conflicts, screen-space projections of Fitts' law parameters (target size and distance) yield constant throughput despite target depth differences and produce better models of performance.},
address = {New York, New York, USA},
author = {Teather, Robert J. and Stuerzlinger, Wolfgang},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470677},
isbn = {9781450318990},
pages = {159--168},
publisher = {ACM Press},
title = {{Pointing at 3d target projections with one-eyed and stereo cursors}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470677},
year = {2013}
}
@article{Barinova2008,
abstract = {We consider the problem of estimating 3-d structure from a single still image of an outdoor urban scene. Our goal is to efficiently create 3-d models which are visually pleasant. We chose an appropriate 3-d model structure and formulate the task of 3-d reconstruction as model fitting problem. Our 3-d models are composed of a number of vertical walls and a ground plane, where ground-vertical boundary is a continuous polyline. We achieve computational efficiency by special preprocessing together with stepwise search of 3-d model parameters dividing the problem into two smaller sub-problems on chain graphs. The use of Conditional Random Field models for both problems allows to various cues. We infer orientation of vertical walls of 3-d model vanishing points.},
author = {Barinova, O and Konushin, V and Yakubenko, A},
doi = {10.1007/978-3-540-88688-4\_8},
journal = {Computer Vision – ECCV 2008},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {100--113},
title = {{Fast automatic single-view 3-d reconstruction of urban scenes}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-88688-4\_8},
volume = {5303},
year = {2008}
}
@article{Roberto2011,
author = {Roberto, Rafael and Freitas, Daniel and Lima, Jo\~{a}o Paulo and Teichrieb, Veronica and Kelner, Judith},
doi = {10.1109/SVR.2011.19},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 XIII Symposium on Virtual Reality/2011/Roberto et al/Roberto et al. - 2011 - ARBlocks A Concept for a Dynamic Blocks Platform for Educational Activities.pdf:pdf},
isbn = {978-1-4577-0661-5},
journal = {2011 XIII Symposium on Virtual Reality},
keywords = {-augmented reality,design for children,education,face,tangible user inter-},
month = may,
pages = {28--37},
publisher = {Ieee},
title = {{ARBlocks: A Concept for a Dynamic Blocks Platform for Educational Activities}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951832},
year = {2011}
}
@book{Stewart2012,
author = {Stewart, James},
edition = {7},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2012/Stewart/Stewart - 2012 - Calculus.pdf:pdf},
isbn = {0538497815},
keywords = {Calculus,livro},
mendeley-tags = {Calculus,livro},
publisher = {Brooks/Cole},
title = {{Calculus}},
year = {2012}
}
@article{Malsburg2011,
abstract = {Which repair strategy does the language system deploy when it gets garden-pathed, and what can regressive eye movements in reading tell us about reanalysis strategies? Several influential eye-tracking studies on syntactic reanalysis (Frazier and Rayner, 1982, Meseguer et al., 2002 and Mitchell et al., 2008) have addressed this question by examining scanpaths, i.e., sequential patterns of eye fixations. However, in the absence of a suitable method for analyzing scanpaths, these studies relied on simplified dependent measures that are arguably ambiguous and hard to interpret. We address the theoretical question of repair strategy by developing a new method that quantifies scanpath similarity. Our method reveals several distinct fixation strategies associated with reanalysis that went undetected in a previously published data set (Meseguer et al., 2002). One prevalent pattern suggests re-parsing of the sentence, a strategy that has been discussed in the literature (Frazier \& Rayner, 1982); however, readers differed tremendously in how they orchestrated the various fixation strategies. Our results suggest that the human parsing system non-deterministically adopts different strategies when confronted with the disambiguating material in garden-path sentences.},
author = {von der Malsburg, Titus and Vasishth, Shravan},
doi = {10.1016/j.jml.2011.02.004},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of Memory and Language/2011/von der Malsburg, Vasishth/von der Malsburg, Vasishth - 2011 - What is the scanpath signature of syntactic reanalysis.pdf:pdf},
issn = {0749596X},
journal = {Journal of Memory and Language},
keywords = {eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
month = aug,
number = {2},
pages = {109--127},
title = {{What is the scanpath signature of syntactic reanalysis?}},
url = {http://www.sciencedirect.com/science/article/pii/S0749596X11000179 http://linkinghub.elsevier.com/retrieve/pii/S0749596X11000179},
volume = {65},
year = {2011}
}
@article{Tavares2012,
author = {Tavares, Anderson C.M. and Cruz, Maria L.P. De M. and Fernandes, Sergio M.M.},
doi = {10.1109/SVR.2012.32},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Tavares, Cruz, Fernandes/Tavares, Cruz, Fernandes - 2012 - Augmented Reality in Collaborative Virtual Environment for Discrete Event Systems Modeling and Simulat.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {-augmented reality,collaborative virtual envi-,discrete event systems,modeling and simulation,ronment},
month = may,
pages = {155--164},
publisher = {Ieee},
title = {{Augmented Reality in Collaborative Virtual Environment for Discrete Event Systems Modeling and Simulation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297572},
year = {2012}
}
@inproceedings{Niu2013,
abstract = {The increasing popularity of stereoscopic 3D brings the demand for tools for editing and authoring stereoscopic images and videos. This paper shows that even a simple task like cropping is difficult for amateur users with little stereoscopic photography knowledge. Unlike regular monocular (2D) images, cropping a stereoscopic image needs to be carefully executed to avoid stereoscopic violations, which otherwise cause an unpleasant stereoscopic viewing experience. In this paper, we present a system that assists in stereoscopic photo cropping by automatically measuring the stereoscopic photography violations and alerting users with the potential violations. Our study shows that compared to a popular stereoscopic photo editing system, our system makes stereoscopic photo cropping easier even for amateur users with little stereoscopic photography knowledge and provides a good user experience.},
author = {Niu, Yuzhen},
booktitle = {2013 IEEE International Conference on Multimedia and Expo (ICME)},
doi = {10.1109/ICME.2013.6607587},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2013 IEEE International Conference on Multimedia and Expo (ICME)/2013/Niu/Niu - 2013 - Making stereo photo cropping easy.pdf:pdf},
isbn = {978-1-4799-0015-2},
issn = {1945-7871},
keywords = {Agriculture,Histograms,Image color analysis,Image edge detection,Photography,Stereo image processing,Stereoscopic photography,Three-dimensional displays,anamorphism,digital photography,keystone,photo cropping,photo editing,stereo photo cropping,stereoscopic 3D,stereoscopic photo editing system,stereoscopic photography violations},
mendeley-tags = {anamorphism,keystone},
month = jul,
pages = {1--6},
publisher = {IEEE},
shorttitle = {Multimedia and Expo (ICME), 2013 IEEE Internationa},
title = {{Making stereo photo cropping easy}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6607587},
year = {2013}
}
@article{Xu,
author = {Xu, W and Wang, Y and Liu, Y and Weng, D and Tan, M and Salzmann, M},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
title = {{REAL-TIME KEYSTONE CORRECTION FOR HAND-HELD PROJECTORS WITH AN RGBD CAMERA}},
url = {http://scholar.google.com.br/scholar?hl=pt-BR\&q=REAL-TIME+KEYSTONE+CORRECTION+FOR+HAND-HELD+PROJECTORS+WITH+AN+RGBDCAMERA\&btnG=\&lr=\#0}
}
@article{Bose2003,
abstract = {Most outdoor visual surveillance scenes involve objects moving on a ground plane. We present a new, fully auto- mated technique for both affine and metric rectification of this ground plane (up to a scale factor) by simply track- ing moving objects. In particular, we derive the necessary constraints on the image plane to ground plane projective transformation by observing objects which move at constant (world) velocity for some part of their trajectory. No knowl- edge of camera parameters is assumed. We describe a hi- erarchy of possible solutions, depending on the nature of the motion trajectories observed. We also show how to au- tomatically detect degenerate cases where 2D rectification is not possible. Useful applications of the various types of rectification are presented. Our experiments demonstrate all the possible solutions on a variety of scenes, as well as some of the applications made possible by rectification},
author = {Bose, B and Grimson, E},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proc. IEEE Workshop Visual Surveillance and Performance Evaluation of Tracking and Surveillance/2003/Bose, Grimson/Bose, Grimson - 2003 - Ground plane rectification by tracking moving objects.pdf:pdf},
journal = {Proc. IEEE Workshop Visual Surveillance and Performance Evaluation of Tracking and Surveillance},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {94--101},
title = {{Ground plane rectification by tracking moving objects}},
url = {http://people.csail.mit.edu/people/cielbleu/pubs/BoseGrimson03GroundPlaneRectification.pdf},
year = {2003}
}
@inproceedings{Sarim2010,
address = {New York, New York, USA},
author = {Sarim, Muhammad and Hilton, Adrian and Guillemaut, Jean-Yves and Kim, Hansung and Takai, Takeshi},
booktitle = {Proceedings of the 1st international workshop on 3D video processing - 3DVP '10},
doi = {10.1145/1877791.1877795},
isbn = {9781450301596},
keywords = {multi-view,reconstruction,segmentation,silhouette,wide-baseline},
mendeley-tags = {reconstruction},
month = oct,
pages = {13},
publisher = {ACM Press},
title = {{Wide-baseline multi-view video segmentation for 3D reconstruction}},
url = {http://dl.acm.org/citation.cfm?id=1877791.1877795},
year = {2010}
}
@inproceedings{Piekarski2001,
abstract = {This paper presents new techniques for capturing and viewing on site 3D graphical models for large outdoor objects. Using an augmented reality wearable computer, we have developed a software system, known as Tinmith-Metro. Tinmith-Metro allows users to control a 3D constructive solid geometry modeller for building graphical objects of large physical artefacts, for example buildings, in the physical world. The 3D modeller is driven by a new user interface known as Tinmith-Hand, which allows the user to control the modeller using a set of pinch gloves and hand tracking. These techniques allow user to supply their AR renderers with models that would previously have to be captured with manual, time-consuming, and/or expensive methods},
address = {Zurich},
author = {Piekarski, W and Thomas, BH},
booktitle = {Proceedings Fifth International Symposium on Wearable Computers},
doi = {10.1109/ISWC.2001.962093},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings Fifth International Symposium on Wearable Computers/2001/Piekarski, Thomas/Piekarski, Thomas - 2001 - Tinmith-Metro new outdoor techniques for creating city models with an augmented reality wearable computer.pdf:pdf},
isbn = {0-7695-1318-2},
keywords = {augmented reality,outdoors},
mendeley-tags = {augmented reality,outdoors},
pages = {31--38},
publisher = {IEEE Comput. Soc},
title = {{Tinmith-Metro: new outdoor techniques for creating city models with an augmented reality wearable computer}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=962093 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=962093},
year = {2001}
}
@book{Ribeiro2011,
address = {Uberl\^{a}ndia},
author = {Ribeiro, Marcos Wagner S. and Zorzal, Ezequiel Roberto},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2011/Ribeiro, Zorzal/Ribeiro, Zorzal - 2011 - Pre-Symposium SVR 2011.pdf:pdf},
publisher = {SBC},
title = {{Pre-Symposium SVR 2011}},
year = {2011}
}
@inproceedings{Pinhanez2006,
abstract = {This chapter discusses not commonly used technologies of interfaces for Virtual and Augmented Reality and their potential usage. The majority of the chapter is dedicated to exploring different input and output devices that are currently in use or in research phase which can help the creation of new interfaces for Virtual Reality, possibly solving some of the problems usually seen in traditional interface systems.},
address = {Bel\'{e}m-PA},
author = {Pinhanez, Claudio},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {173--197},
publisher = {SBC},
title = {{Interfaces N\~{a}o-Convencionais}},
year = {2006}
}
@article{Lee2013c,
abstract = {Encoders are generally used to track the motion of industrial mechanisms. However, the information obtained by encoders may have errors due to encoder aging or mechanism-design problem. Therefore, information by visual feedback is a better way to track the movement of industrial mechanisms. However, image information costs lots of computing effort so it is not easy to be used in real-time control applications. This manuscript derives a simple but effective visual feedback method to follow the target and the image information is obtained only by a general handy camcorder. Besides, the proposed method can track multi-locations in a meantime. Fast image pattern recognition and localisation of the colour histogram by using a moving tracking block is applied to increase the calculation speed. Finally, the obtained locations information by the proposed visual feedback method is applied in an industrial crane control system to verify the effectiveness.},
author = {Lee, Lun-Hui and Huang, Pei-Hsiang and Pan, Shing-Tai and Lie, Handra Wijaya and Chiang, Tung-Chien and Chang, Cheng-Yuan},
doi = {10.1080/00207721.2013.779762},
file = {:home/acmt/Dropbox/Documentos/Mendeley/International Journal of Systems Science/2013/Lee et al/Lee et al. - 2013 - Applying vision feedback to crane controller design.pdf:pdf},
issn = {0020-7721},
journal = {International Journal of Systems Science},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
month = mar,
pages = {1--9},
title = {{Applying vision feedback to crane controller design}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00207721.2013.779762},
year = {2013}
}
@inproceedings{Chen2008b,
abstract = {This paper describes a novel real-time 3D gaze estimation system. The system consists of two cameras and two IR light sources. There are three novelties in this method. First, in our system, two IR lights are mounted near the centers of the stereo cameras, respectively. Based on this specific configuration, the 3D position of the corneal center can be simply derived by the 3D reconstruction technique. Then, after extracting the 3D position of the "virtual pupil" correctly, the optical axis of the eye can be obtained directly by connecting the "virtual pupil" with the corneal center. Second, we systematically analyze the noise in our 3D gaze estimation algorithm and propose an effective constraint to reduce this noise. Third, to estimate the user-dependent parameters (i.e. the constraint parameters and the eye parameters), a simple calibration method is proposed by gazing at four positions on the screen. Experimental results show that our system can accurately estimate and track eye gaze under natural head movement.},
address = {New York, New York, USA},
author = {Chen, Jixu and Tong, Yan and Gray, Wayne and Ji, Qiang},
booktitle = {Proceedings of the 2008 symposium on Eye tracking research \& applications - ETRA '08},
doi = {10.1145/1344471.1344518},
isbn = {9781595939821},
keywords = {3d gaze},
mendeley-tags = {3d gaze},
pages = {189},
publisher = {ACM Press},
title = {{A robust 3D eye gaze tracking system using noise reduction}},
url = {http://dl.acm.org/citation.cfm?id=1344518 http://portal.acm.org/citation.cfm?doid=1344471.1344518},
year = {2008}
}
@article{Bardzell2013,
address = {New York, New York, USA},
author = {Bardzell, Jeffrey and Bardzell, Shaowen},
doi = {10.1145/2470654.2466451},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3297},
publisher = {ACM Press},
title = {{What is "critical" about critical design?}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466451},
year = {2013}
}
@article{Sato2005,
abstract = {We propose a new method for superimposing synthetic objects with natural shadings and cast shadows onto a real scene whose illumination condition is dynamically changing in this paper. In general, high computational cost for rendering virtual objects with convincing shading and shadows, such as interreflections or shadows under area light sources, prohibits real-time synthesis of such composite images with superimposed virtual objects. To overcome this limitation, we take advantage of the linearity of the relationship between brightness changes observed on an object surface and change of illumination radiance in a scene and introduce an efficient rendering technique based on a linear combination of pre-rendered reference images of the scene. We have successfully tested the proposed method in a natural illumination condition of an indoor environment to demonstrate how fast the method could superimpose a virtual object onto the scene with highly realistic shadings and shadows.},
author = {Sato, Imari and Hayashida, Morihiro and Kai, Fumiyo and Sato, Yoichi and Ikeuchi, Katsushi},
doi = {10.1002/scj.10155},
issn = {0882-1666},
journal = {Systems and Computers in Japan},
keywords = {computer graphics,computer vision,illumination distribution estimation,mixed reality},
month = dec,
number = {14},
pages = {102--111},
title = {{Fast image synthesis of virtual objects in a real scene with natural shadings}},
url = {http://doi.wiley.com/10.1002/scj.10155},
volume = {36},
year = {2005}
}
@article{Pasman2006a,
author = {Woodward, Charles and Pasman, W.},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings/2006/Woodward, Pasman/Woodward, Pasman - 2006 - Implementation of an Augmented Reality System on a PDA.pdf:pdf},
journal = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
pages = {276--277},
publisher = {IEEE Comput. Soc},
title = {{Implementation of an Augmented Reality System on a PDA}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240718},
year = {2006}
}
@article{Daly2007,
author = {Daly, Leonard and Brutzman, Don},
doi = {10.1109/MSP.2007.905889},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Signal Processing Magazine/2007/Daly, Brutzman/Daly, Brutzman - 2007 - X3D Extensible 3D Graphics Standard Standards in a Nutshell.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
month = nov,
number = {6},
pages = {130--135},
title = {{X3D: Extensible 3D Graphics Standard [Standards in a Nutshell]}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4387948},
volume = {24},
year = {2007}
}
@inproceedings{Tan2000,
abstract = {Three efficient techniques are presented for modelling the shape of 3-D objects employing uncalibrated cameras, The first technique employing multiple video cameras is proposed for recovering the 3-D shape of non-rigid objects by factorization. The second technique employing pairs of facing cameras which surround a rigid object reconstructs an entire shape of the object by collecting all the projected feature points around the object into a single measurement matrix, which is factorized to yield an entire 3-D model of the object. The first and the second technique are combined to yield the third technique which not only models 3-D shape of a non-rigid object at each time but also recovers its deformation process during observation all at once. Satisfactory performance of these techniques are shown by the performed experiments. The present study is expected to contribute to various fields which have interests in 3-D shape/deformation recovery, modelling, and analysis, such as 3-D modelling of characters in video games, 3-D modelling of excavated objects in archaeology, human motion analysis in sports, dancing, or rehabilitation, etc},
author = {Tan, J.K. and Ishikawa, S.},
booktitle = {2000 TENCON Proceedings. Intelligent Systems and Technologies for the New Millennium (Cat. No.00CH37119)},
doi = {10.1109/TENCON.2000.893540},
isbn = {0-7803-6355-8},
keywords = {3-D shape,Cameras,Control engineering,Deformable models,Games,Humans,Motion analysis,Shape control,Shape measurement,Streaming media,Three dimensional displays,archaeology,dancing,deformation process,excavated objects,factorization,human motion analysis,image motion analysis,image reconstruction,matrix decomposition,measurement matrix,modelling,multiple video cameras,nonrigid objects,projected feature points,reconstruction,rehabilitation,sport,three-dimensional objects,uncalibrated cameras,video games,video signal processing},
mendeley-tags = {reconstruction},
pages = {59--63},
publisher = {IEEE},
shorttitle = {TENCON 2000. Proceedings},
title = {{On modelling three-dimensional objects by uncalibrated cameras}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=893540},
volume = {1},
year = {2000}
}
@inproceedings{Smith2006,
address = {New York, New York, USA},
author = {Smith, J. David and Graham, T. C. Nicholas},
booktitle = {Proceedings of the 2006 ACM SIGCHI international conference on Advances in computer entertainment technology - ACE '06},
doi = {10.1145/1178823.1178847},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2006 ACM SIGCHI international conference on Advances in computer entertainment technology - ACE '06/2006/Smith, Graham/Smith, Graham - 2006 - Use of eye movements for video game control.pdf:pdf},
isbn = {1595933808},
keywords = {eye tracking,video game},
mendeley-tags = {eye tracking,video game},
pages = {20},
publisher = {ACM Press},
title = {{Use of eye movements for video game control}},
url = {http://dl.acm.org/citation.cfm?id=1178847 http://portal.acm.org/citation.cfm?doid=1178823.1178847},
year = {2006}
}
@book{McPherson2001,
abstract = {This book describes the basis, application, and interpretation of statistics, and presents a wide range of univariate and multivariate statistical methodology. In its first edition it has proved popular across all science and technology based disciplines, including the social sciences, and in areas of commerce. It is used both as a reference on statistical methodology for researchers and technicians, and as a textbook with particular appeal for graduate classes containing students of mixed mathematical and statistical background. The book is developed without the use of calculus, although several self-contained sections containing calculus are included to provide additional insight for readers who have a calculus background. Based on the author's "Statistics in Scientific Investigation," the book has been extended substantially in the area of multivariate applications and through the expansion of logistic regression and log linear methodology. It presumes readers have access to a statistical computing package and includes guidance on the application of statistical computing packages. The new edition retains the unique feature of being written from the users' perspective; it connects statistical models and methods to investigative questions and background information, and connects statistical results with interpretations in plain English. In keeping with this approach, methods are grouped by usage rather than by commonality of statistical methodology. Guidance is provided on the choice of appropriate methods. The use of real life examples has been retained and expanded. Using the power of the Internet, expanded reports on the examples are available at a Springer Web site as Word documents. Additionaly, all data sets are available at the Web site as Excel files, and program files and data sets are provided for SAS users and SPSS users. The programs are annotated so users can adapt for their own data sets. Glen McPherson has had a long career as an acedmic and a consultant in applied statistics. After holding a tenured position for thirty years at The University of Tasmania in Australia, where he developed and directed under-graduate and gradate programs in applied statistics, he resigned to devote more time to book writing and to his consultancy role to government, business, and industry. The practical experience built into the book is drawn from the two thousand consulantcies he has undetaken across virtually all areas in which statistics has an application.},
author = {McPherson, Glen},
isbn = {0387951105},
pages = {640},
publisher = {Springer},
title = {{Applying and Interpreting Statistics: A Comprehensive Guide (Google eBook)}},
url = {http://books.google.com/books?id=FezZhTX9OgQC\&pgis=1},
year = {2001}
}
@article{Livingstone2002,
author = {Livingstone, M and Hubel, DH},
file = {::},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Vision and art: The biology of seeing}},
url = {http://great-humanities-textbooks.info/wp-content/uploads/pdfs/Vision and Art The Biology of Seeing by Margaret S Livingstone - Finally  A Book Offering Ideas For An Age-Old Question.pdf},
year = {2002}
}
@inproceedings{Siscoutto2006,
abstract = {This chapter introduces stereoscopy, showing how stereo images are composed by the human vision as well as artificially, presenting some techniques and devices to generate stereoscopy and some related mathematical fundaments. In addition, some problems related to computer-generated stereoscopic visualization are discussed. At the end, two virtual reality applications involving stereoscopy are presented.},
address = {Bel\'{e}m-PA},
author = {Siscoutto, Robson and Szenberg, Fl\'{a}vio and Tori, Romero and Raposo, Alberto Barbosa and Celes, Waldemar and Gattass, Marcelo},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {221--245},
publisher = {SBC},
title = {{Estereoscopia}},
year = {2006}
}
@article{Norman1999,
author = {Norman, Donald A},
doi = {10.1145/301153.301168},
file = {:home/acmt/Dropbox/Documentos/Mendeley/interactions/1999/Norman/Norman - 1999 - Affordance, conventions, and design.pdf:pdf},
issn = {10725520},
journal = {interactions},
month = may,
number = {3},
pages = {38--43},
title = {{Affordance, conventions, and design}},
url = {http://portal.acm.org/citation.cfm?doid=301153.301168},
volume = {6},
year = {1999}
}
@inproceedings{Koh2009,
abstract = {This paper evaluates the input performance capabilities of Velocity Threshold (I-VT) and Kalman Filter (I-KF) eye movement detection models when employed for eye-gaze-guided interface control. I-VT is a common eye movement identification model employed by the eye tracking community, but it is neither robust nor capable of handling high levels of noise present in the eye position data. Previous research implies that use of a Kalman filter reduces the noise in the eye movement signal and predicts the signal during brief eye movement failures, but the actual performance of I-KF was never evaluated. We evaluated the performance of I-VT and I-KF models using guidelines for ISO 9241 Part 9 standard, which is designed for evaluation of non keyboard/mouse input devices with emphasis on performance, comfort, and effort. Two applications were implemented for the experiment: 1) an accuracy test 2) a photo viewing application specifically designed for eye-gaze-guided control. Twenty-one subjects participated in the evaluation of both models completing a series of tasks. The results indicates that I-KF allowed participants to complete more tasks with shorter completion time while providing higher general comfort, accuracy and operation speeds with easier target selection than the I-VT model. We feel that these results are especially important to the engineers of new assistive technologies and interfaces that employ eye-tracking technology in their design.},
address = {New York, New York, USA},
annote = {- cited by: 16- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000
        
Resumo: },
author = {Koh, Do Hyong and {Munikrishne Gowda}, Sandeep A. and Komogortsev, Oleg V},
booktitle = {Proceedings of the 1st ACM SIGCHI symposium on Engineering interactive computing systems - EICS '09},
doi = {10.1145/1570433.1570470},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 1st ACM SIGCHI symposium on Engineering interactive computing systems - EICS '09/2009/Koh, Munikrishne Gowda, Komogortsev/Koh, Munikrishne Gowda, Komogortsev - 2009 - Input evaluation of an eye-gaze-guided interface.pdf:pdf},
isbn = {9781605586007},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {197},
publisher = {ACM Press},
title = {{Input evaluation of an eye-gaze-guided interface}},
url = {http://dl.acm.org/citation.cfm?id=1570470 http://portal.acm.org/citation.cfm?doid=1570433.1570470},
year = {2009}
}
@inproceedings{Nunes2007,
address = {Petropolis},
author = {Nunes, F\'{a}tima L. S. and Costa, Rosa M. E. M. and Oliveira, Ana Cl\'{a}udia M. T. G. and Delfino, S\'{e}rgio R. and Pavarini, Larissa and Rodello, Ildeberto A. and Brega, Jos\'{e} Remo F. and Sementille, Antonio C.},
booktitle = {Symposium of Virtual Reality},
pages = {223--255},
publisher = {SBC},
title = {{Aplica\c{c}\~{o}es M\'{e}dicas usando Realidade Virtual e Realidade Aumentada}},
year = {2007}
}
@article{Williams1999,
author = {Williams, AM and Grant, A},
journal = {International Journal of Sport Psychology},
pages = {194--220},
title = {{Training perceptual skill in sport.}},
url = {http://psycnet.apa.org/?fa=main.doiLanding\&uid=1999-11726-004},
volume = {30},
year = {1999}
}
@inproceedings{Sun2008,
abstract = {This paper presents a novel algorithm for calibrating multiple casually placed projectors, via a single uncalibrated camera, to produce a "wallpaper" projection, which is equivalent to printing a single image onto a flat sheet of paper and pasting it to the screen. Based on a piecewise planar model, the method described here combines the advantages of global surface fitting and homographies to generate high accuracy geometric correction independent of the camera position and viewing angle. Experimental results validate the approach, achieving seamless displays with various multi-projector arrangements. The proposed calibration mechanism is applicable to both curved and planar screen surfaces and to single as well as multiple projector displays.},
address = {New York, New York, USA},
author = {Sun, Wei and Sobel, Irwin and Culbertson, Bruce and Gelb, Dan and Robinson, Ian},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Projector camera systems - PROCAMS '08},
doi = {10.1145/1394622.1394624},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 5th ACMIEEE International Workshop on Projector camera systems - PROCAMS '08/2008/Sun et al/Sun et al. - 2008 - Calibrating multi-projector cylindrically curved displays for wallpaper projection.pdf:pdf},
isbn = {9781605582726},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
pages = {1},
publisher = {ACM Press},
title = {{Calibrating multi-projector cylindrically curved displays for "wallpaper" projection}},
url = {http://dl.acm.org/citation.cfm?id=1394624 http://portal.acm.org/citation.cfm?doid=1394622.1394624},
year = {2008}
}
@phdthesis{Agustin2010,
abstract = {People with severe motor-skill disabilities are often unable to use standard input devices such as a mouse or a keyboard to control a computer and they are, therefore, in strong need for alternative input devices. Gaze tracking offers them the possibility to use the movements of their eyes to interact with a computer, thereby making them more independent. A big effort has been put toward improving the robustness and accuracy of the technology, and many commercial systems are nowadays available in the market. Despite the great improvements that gaze tracking systems have undergone in the last years, high prices have prevented gaze interaction from becoming mainstream. The use of specialized hardware, such as industrial cameras or infrared light sources, increases the accuracy of the systems, but also the price, which prevents many poten- tial users fromhaving access to the technology. Furthermore, the different components are often required to be placed in specific locations, or are built into the monitor, thus decreasing the flexibility of the setup. Gaze tracking systems built from low-cost and off-the-shelf components have the potential to facilitate access to the technology and bring the prices down. Such systems are oftenmore flexible, as the components can be placed in different locations, but also less robust, due to the lack of control over the hardware setup and the lower quality of the components compared to commercial systems. The work developed for this thesis deals with some of the challenges introduced by the use of low-cost and off-the-shelf components for gaze interaction. The main contributions are: - Development and performance evaluation of the ITU Gaze Tracker, an off-the- shelf gaze tracker that uses an inexpensive webcam or video camera to track the user’s eye. The software is readily available as open source, offering the possibility to try out gaze interaction for a low price and to analyze, improve and extend the software by modifying the source code. - Anovel gaze estimationmethod based on homographicmappings between planes. No knowledge about the hardware configuration is required, allowing for a flex- ible setup where camera and light sources can be placed at any location. -A novel algorithm to detect the type of movement that the eye is performing, i.e. fixation, saccade or smooth pursuit. The algorithm is based on eye velocity and movement pattern, and allows to smooth the signal appropriately for each kind of movement to remove jitter due to noise while maximizing responsiveness.},
author = {Agustin, Javier San},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2010/Agustin/Agustin - 2010 - Off-the-shelf gaze interaction.pdf:pdf},
keywords = {fixation,gaze events,saccade,smooth pursuit},
mendeley-tags = {fixation,gaze events,saccade,smooth pursuit},
pages = {179},
title = {{Off-the-shelf gaze interaction}},
url = {http://forskningsbasen.deff.dk/Share.external?sp=S3eb0b28e-2894-417a-b93b-747cb73c28f7\&sp=Situ},
year = {2010}
}
@inproceedings{Pelechano2005,
abstract = {We describe a new architecture to integrate a psychological model into a crowd simulation system in order to obtain believable emergent behaviors. Our existing crowd simulation system (MACES) performs high level wayfinding to explore unknown environments and obtain a cognitive map for navigation purposes, in addition to dealing with low level motion within each room based on social forces. Communication and roles are added to achieve individualistic behaviors and a realistic way to spread information about the environment. To expand the range of realistic human behaviors, we use a system (PMFserv) that implements human behavior models from a range of ability, stress, emotion, decision theoretic and motivation sources. An architecture is proposed that combines and integrates MACES and PMFserv to add validated agent behaviors to crowd simulations.},
address = {Lausanne},
author = {Pelechano, N and O'Brien, K and Silverman, B and Badler, N},
booktitle = {First International Workshop on Crowd Simulation},
file = {:home/acmt/Dropbox/Documentos/Mendeley/First International Workshop on Crowd Simulation/2005/Pelechano et al/Pelechano et al. - 2005 - Crowd simulation incorporating agent psychological models, roles and communication.pdf:pdf},
keywords = {crowd simulation},
mendeley-tags = {crowd simulation},
title = {{Crowd simulation incorporating agent psychological models, roles and communication}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord\&metadataPrefix=html\&identifier=ADA522128},
year = {2005}
}
@inproceedings{Hashimoto2013,
abstract = {This paper introduces an input and output device that enables illumination, bi-directional data communication, and position sensing on a soft cloth. This "LightCloth" is woven from diffusive optical fibers. Since the fibers are arranged in parallel, the cloth has one-dimensional position information. Sensor-emitter pairs attached to bundles of contiguous fibers enable bundle-specific light input and output. We developed a prototype system that allows full-color illumination and 8-bit data input by infrared signals. We present as an application a chair with a LightCloth cover whose illumination pattern is specified using an infrared light pen. Here we describe the implementation details of the device and discuss possible interactions using the device.},
address = {New York, New York, USA},
author = {Hashimoto, Sunao and Suzuki, Ryohei and Kamiyama, Youichi and Inami, Masahiko and Igarashi, Takeo},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470739},
isbn = {9781450318990},
pages = {603},
publisher = {ACM Press},
title = {{LightCloth: senseable illuminating optical fiber cloth for creating interactive surfaces}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470739},
year = {2013}
}
@article{Qu2013,
address = {New York, New York, USA},
author = {Qu, Yan and Zhang, Jun},
doi = {10.1145/2470654.2470711},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Human Mobility,OPTICS,Regularly Visited Patches},
pages = {395},
publisher = {ACM Press},
title = {{Regularly visited patches in human mobility}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470711},
year = {2013}
}
@incollection{Goldberg2014,
author = {Goldberg, JH and Helfman, JI},
booktitle = {Handbook of Human Centric Visualization},
doi = {10.1007/978-1-4614-7485-2\_13},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Handbook of Human Centric Visualization/2014/Goldberg, Helfman/Goldberg, Helfman - 2014 - Eye Tracking on Visualizations Progressive Extraction of Scanning Strategies.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {337--372},
title = {{Eye Tracking on Visualizations: Progressive Extraction of Scanning Strategies}},
url = {http://link.springer.com/chapter/10.1007/978-1-4614-7485-2\_13},
year = {2014}
}
@article{Curmi2013,
abstract = {A number of studies in the literature have looked into the use of real-time biometric data to improve one's own physiological performance and wellbeing. However, there is limited research that looks into the effects that sharing biometric data with others could have on one's social network. Following a period of research on existing mobile applications and prototype testing, we developed a system, HeartLink, which collects real-time personal biometric data such as heart rate and broadcasts this data online. Insights gained on designing systems to broadcast real-time biometric data are presented. In this paper we also report emerging results from testing HeartLink in a pilot study and a user study that were conducted during sport events. The results showed that sharing heart rate data does influence the relationship of the persons involved and that the degree of influence seems related to the tie strength prior to visualizing the data.},
address = {New York, New York, USA},
author = {Curmi, Franco and Ferrario, Maria Angela and Southern, Jen and Whittle, Jon},
doi = {10.1145/2470654.2466231},
isbn = {9781450319522},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {4503},
publisher = {ACM Press},
title = {{HeartLink: open broadcast of live biometric data to social networks}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466231},
year = {2013}
}
@misc{Wikipedia2013,
author = {Wikipedia},
keywords = {rbs},
mendeley-tags = {rbs},
title = {{Systematic Review}},
url = {http://en.wikipedia.org/wiki/Systematic\_review},
urldate = {22-11-2013},
year = {2013}
}
@inproceedings{Fumarola2010,
abstract = {Animation techniques are used during simulation studies for verifying and validating the model, and for communication purposes to external parties. Including animation in a simulation model, often results in models that contain many references to animation specific code. Moreover, in the specific case of discrete event simulation, challenges arise due to the difference in time-bases with animation techniques that mostly have a discrete notion of time. Typical approaches to developing animation for simulation models provide tightly coupled simulation and animation components that are fine-tuned to provide acceptable results. Apart of the disadvantage of having entangled model and animation code, this also has the disadvantage of reduced flexibility that one would desire in order to be able to use other animation techniques: for instance going from 2D to 3D with modern visualization libraries. In this paper, we will present our approach for loosely coupled discrete event simulation models and animation components.},
annote = {Division between animation (view) and logic, like MVC (model-view-controller)
      },
author = {Fumarola, Michele and Seck, Mamadou and Verbraeck, Alexander},
booktitle = {Proceedings of the 2010 Winter Simulation Conference},
doi = {10.1109/WSC.2010.5678857},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2010 Winter Simulation Conference/2010/Fumarola, Seck, Verbraeck/Fumarola, Seck, Verbraeck - 2010 - An approach for loosely coupled discrete event simulation models and animation components.pdf:pdf},
isbn = {978-1-4244-9866-6},
month = dec,
pages = {2161--2170},
publisher = {IEEE},
title = {{An approach for loosely coupled discrete event simulation models and animation components}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5678857},
year = {2010}
}
@incollection{Hayhoe2007,
address = {Amsterdam},
author = {Hayhoe, Mary and Droll, J. and Mennie, N.},
booktitle = {Eye Movements: A Window on Mind and Brain},
isbn = {9780080474915},
pages = {642--659},
publisher = {Elsevier Science},
title = {{Learning where to look}},
year = {2007}
}
@article{Dorr2010,
abstract = {How similar are the eye movement patterns of different subjects when free viewing dynamic natural scenes? We collected a large database of eye movements from 54 subjects on 18 high-resolution videos of outdoor scenes and measured their variability using the Normalized Scanpath Saliency, which we extended to the temporal domain. Even though up to about 80\% of subjects looked at the same image region in some video parts, variability usually was much greater. Eye movements on natural movies were then compared with eye movements in several control conditions. "Stop-motion" movies had almost identical semantic content as the original videos but lacked continuous motion. Hollywood action movie trailers were used to probe the upper limit of eye movement coherence that can be achieved by deliberate camera work, scene cuts, etc. In a "repetitive" condition, subjects viewed the same movies ten times each over the course of 2 days. Results show several systematic differences between conditions both for general eye movement parameters such as saccade amplitude and fixation duration and for eye movement variability. Most importantly, eye movements on static images are initially driven by stimulus onset effects and later, more so than on continuous videos, by subject-specific idiosyncrasies; eye movements on Hollywood movies are significantly more coherent than those on natural movies. We conclude that the stimuli types often used in laboratory experiments, static images and professionally cut material, are not very representative of natural viewing behavior. All stimuli and gaze data are publicly available at http://www.inb.uni-luebeck.de/tools-demos/gaze.},
annote = {- cited by:71
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Dorr, Michael and Martinetz, Thomas and Gegenfurtner, Karl R and Barth, Erhardt},
doi = {10.1167/10.10.28},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of vision/2010/Dorr et al/Dorr et al. - 2010 - Variability of eye movements when viewing dynamic natural scenes.pdf:pdf},
issn = {1534-7362},
journal = {Journal of vision},
keywords = {Eye Movements,Eye Movements: physiology,Fixation,Humans,Motion Perception,Motion Perception: physiology,Ocular,Ocular: physiology,Pattern Recognition,Photic Stimulation,Visual,Visual: physiology,gaze analysis},
mendeley-tags = {gaze analysis},
month = jan,
number = {10},
pages = {28},
pmid = {20884493},
title = {{Variability of eye movements when viewing dynamic natural scenes.}},
url = {http://ww.journalofvision.org/content/10/10/28.short http://www.ncbi.nlm.nih.gov/pubmed/20884493},
volume = {10},
year = {2010}
}
@article{Rosner2013,
address = {New York, New York, USA},
author = {Rosner, Daniela K. and Ikemiya, Miwa and Kim, Diana and Koch, Kristin},
doi = {10.1145/2470654.2466218},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1649},
publisher = {ACM Press},
title = {{Designing with traces}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466218},
year = {2013}
}
@inproceedings{Noh2009,
abstract = {The seamless integration between computer-generated objects with real scene is desirable in augmented reality applications. The challenging task in augmented reality application is to make virtual objects mixed harmoniously with the real scene. To achieve these photorealistic rendering, it is not only considering visual complexity of virtual objects but the consistency of illumination between virtual objects with real scene must also be determined. With the consistency of illumination, virtual objects can cast shadows that appear as part of the real scene. The consistent shadow of the virtual objects will give a correct explanation of the relative distance between the virtual and the real objects. This paper explore regarding shadow techniques to improve photorealism for rendering virtual objects in augmented reality system. The evaluation among previous techniques presented in this paper is also important as a preliminary step for future findings related to the field of augmented reality.},
address = {Dubai},
author = {Noh, Zakiah and Sunar, Mohd Shahrizal},
booktitle = {2009 Second International Conference on Machine Vision},
doi = {10.1109/ICMV.2009.41},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2009 Second International Conference on Machine Vision/2009/Noh, Sunar/Noh, Sunar - 2009 - A Review of Shadow Techniques in Augmented Reality.pdf:pdf},
isbn = {978-1-4244-5644-4},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
pages = {320--324},
publisher = {IEEE},
title = {{A Review of Shadow Techniques in Augmented Reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5381137 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5381137},
year = {2009}
}
@inproceedings{Brostow2006,
abstract = {While crowds of various subjects may offer applicationspecific cues to detect individuals, we demonstrate that for the general case, motion itself contains more information than previously exploited. This paper describes an unsupervised data driven Bayesian clustering algorithm which has detection of individual entities as its primary goal. We track simple image features and probabilistically group them into clusters representing independently moving entities. The numbers of clusters and the grouping of constituent features are determined without supervised learning or any subject-specific model. The new approach is instead, that space-time proximity and trajectory coherence through image space are used as the only probabilistic criteria for clustering. An important contribution of this work is how these criteria are used to perform a one-shot data association without iterating through combinatorial hypotheses of cluster assignments. Our proposed general detection algorithm can be augmented with subject-specific filtering, but is shown to already be effective at detecting individual entities in crowds of people, insects, and animals. This paper and the associated video examine the implementation and experiments of our motion clustering framework.},
address = {New York, NY},
author = {Brostow, G.J. and Cipolla, R.},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 - CVPR},
doi = {10.1109/CVPR.2006.320},
isbn = {0-7695-2597-0},
keywords = {trajectory extraction},
mendeley-tags = {trajectory extraction},
pages = {594--601},
publisher = {IEEE},
title = {{Unsupervised Bayesian Detection of Independent Motion in Crowds}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1640809},
volume = {1},
year = {2006}
}
@misc{Khronos2012,
author = {Khronos},
title = {{WebGL Specification}},
url = {http://www.khronos.org/registry/webgl/specs/latest/},
urldate = {26/02/2012},
year = {2012}
}
@article{Caldara2011,
abstract = {Eye movement data analyses are commonly based on the probability of occurrence of saccades and fixations (and their characteristics) in given regions of interest (ROIs). In this article, we introduce an alternative method for computing statistical fixation maps of eye movements--iMap--based on an approach inspired by methods used in functional magnetic resonance imaging. Importantly, iMap does not require the a priori segmentation of the experimental images into ROIs. With iMap, fixation data are first smoothed by convolving Gaussian kernels to generate three-dimensional fixation maps. This procedure embodies eyetracker accuracy, but the Gaussian kernel can also be flexibly set to represent acuity or attentional constraints. In addition, the smoothed fixation data generated by iMap conform to the assumptions of the robust statistical random field theory (RFT) approach, which is applied thereafter to assess significant fixation spots and differences across the three-dimensional fixation maps. The RFT corrects for the multiple statistical comparisons generated by the numerous pixels constituting the digital images. To illustrate the processing steps of iMap, we provide sample analyses of real eye movement data from face, visual scene, and memory processing. The iMap MATLAB toolbox is editable and freely available for download online (www.unifr.ch/psycho/ibmlab/).},
annote = {crossRef(Jarodzka 2010)},
author = {Caldara, Roberto and Miellet, S\'{e}bastien},
doi = {10.3758/s13428-011-0092-x},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Behavior research methods/2011/Caldara, Miellet/Caldara, Miellet - 2011 - iMap a novel method for statistical fixation mapping of eye movement data.pdf:pdf},
issn = {1554-3528},
journal = {Behavior research methods},
keywords = {Algorithms,Biometry,Biometry: methods,Data Interpretation, Statistical,Eye Movement Measurements,Eye Movement Measurements: statistics \& numerical ,Eye Movements,Humans,Software,eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
month = sep,
number = {3},
pages = {864--78},
pmid = {21512875},
title = {{iMap: a novel method for statistical fixation mapping of eye movement data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21512875},
volume = {43},
year = {2011}
}
@article{Hoiem2005a,
abstract = {This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children's pop-up book illustration. Our main insight is that instead of attempting to recover precise geometry, we statistically model geometric classes defined by their orientations in the scene. Our algorithm labels regions of the input image into coarse categories: "ground", "sky", and "vertical". These labels are then used to "cut and fold" the image into a pop-up model using a set of simple assumptions. Because of the inherent ambiguity of the problem and the statistical nature of the approach, the algorithm is not expected to work on every image. However. it performs surprisingly well for a wide range of scenes taken from a typical person's photo album.},
author = {Hoiem, Derek and Efros, Alexei A. and Hebert, Martial},
doi = {10.1145/1073204.1073232},
issn = {07300301},
journal = {ACM Transactions on Graphics},
month = jul,
number = {3},
pages = {577},
title = {{Automatic photo pop-up}},
url = {http://dl.acm.org/citation.cfm?id=1073232 http://portal.acm.org/citation.cfm?doid=1073204.1073232},
volume = {24},
year = {2005}
}
@inproceedings{Sukthankar2001,
abstract = {Standard presentation systems consisting of a laptop con- nected to a projector suffer from two problems: ( 1 ) the projected image appears distorted (keystoned) unless the projector is precisely aligned to the projection screen; ( 2 ) the speaker is forced to interact with the computer rather than the audience. This paper shows how the addition of an uncalibrated camera, aimed at the screen, solves both problems. Although the locations, orientations and optical parameters of the camera and projector are unknown, the projector-camera system calibrates itself by exploiting the homography between the projected slide and the camera im- age. Significant improvements are possible over passively calibrating systems since the projector actively manipulates the environment by placing feature points into the scene. For instance, using a low-resolution (160x 120) camera, we can achieve an accuracy of k3 pixels in a 1024x768 presen- tation slide. The camera-projector system infers models for the projector-to-camera and projector-to-screen mappings in order to provide two major benefits. First, images sent to the projector are pre-warped in such a way that the distor- tions induced by the arbitrary projector-screen geometry are precisely negated. This enables projectors to be mounted anywhere in the environment - for instance, at the side of the room, where the speaker is less likely to cast shadows on the screen, and where the projector does not occlude the audience’s view. Second, the system detects the position of the user’s laser pointer dot in the camera image at 20Hz, al- lowing the laser pointer to emulate the pointing actions of a mouse. This enables the user to activate virtual buttons in the presentation (such as “next slide”) and draw on the projected image. The camera-assisted presentation system requires no special hardware (aside from the cheap camera) and runs on a standard laptop as a Java application. It is now used by the authors for all of their conference presen- tations.},
annote = {        From Duplicate 2 (                   Smarter presentations: exploiting homography in camera-projector systems                 - Sukthankar, Rahul; Stockton, R.G.; Mullin, M.D. )
                
        From Duplicate 1 (                           Smarter Presentations : Exploiting Homography in Camera-Projector Systems                         - Sukthankar, Rahul; Stockton, Robert G; Mullin, Matthew D )
                
        
        
        
        
      },
author = {Sukthankar, Rahul and Stockton, R.G. and Mullin, M.D.},
booktitle = {Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001},
doi = {10.1109/ICCV.2001.937525},
isbn = {0-7695-1143-0},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
pages = {247--253},
publisher = {IEEE Comput. Soc},
title = {{Smarter presentations: exploiting homography in camera-projector systems}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=937525 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=937525},
volume = {1},
year = {2001}
}
@article{DiVerdi2006,
author = {{Di Verdi}, S. and Nurmi, D. and Hollerer, T.},
doi = {10.1109/ISMAR.2003.1240729},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings/2006/Di Verdi, Nurmi, Hollerer/Di Verdi, Nurmi, Hollerer - 2006 - ARWin - a desktop augmented reality Window Manager.pdf:pdf},
isbn = {0-7695-2006-5},
journal = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
pages = {298--299},
publisher = {IEEE Comput. Soc},
title = {{ARWin - a desktop augmented reality Window Manager}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240729},
year = {2006}
}
@article{Drewes2010,
author = {Drewes, H},
file = {::},
title = {{Eye gaze tracking for human computer interaction}},
url = {http://edoc.ub.uni-muenchen.de/11591/},
year = {2010}
}
@inproceedings{Hoiem2005,
address = {New York, New York, USA},
author = {Hoiem, Derek and Efros, Alexei A. and Hebert, Martial},
booktitle = {ACM SIGGRAPH 2005 Papers on - SIGGRAPH '05},
doi = {10.1145/1186822.1073232},
issn = {0730-0301},
keywords = {image segmentation,image-based rendering,machine learning,reconstruction,single-view reconstruction},
mendeley-tags = {reconstruction},
month = jul,
number = {3},
pages = {577},
publisher = {ACM Press},
title = {{Automatic photo pop-up}},
url = {http://dl.acm.org/citation.cfm?id=1186822.1073232 http://portal.acm.org/citation.cfm?doid=1186822.1073232},
volume = {24},
year = {2005}
}
@article{Torricelli2009,
author = {Torricelli, Diego and Goffredo, Michela and Conforto, Silvia and Schmid, Maurizio},
doi = {10.1016/j.patrec.2009.05.014},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Pattern Recognition Letters/2009/Torricelli et al/Torricelli et al. - 2009 - An adaptive blink detector to initialize and update a view-basedremote eye gaze tracking system in a natural.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {blink,gaze analysis},
mendeley-tags = {blink,gaze analysis},
month = sep,
number = {12},
pages = {1144--1150},
title = {{An adaptive blink detector to initialize and update a view-basedremote eye gaze tracking system in a natural scenario}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865509001329 http://linkinghub.elsevier.com/retrieve/pii/S0167865509001329},
volume = {30},
year = {2009}
}
@inproceedings{Jarodzka2010,
abstract = {A great need exists in many fields of eye-tracking research for a robust and general method for scanpath comparisons. Current measures either quantize scanpaths in space (string editing measures like the Levenshtein distance) or in time (measures based on attention maps). This paper proposes a new pairwise scanpath similarity measure. Unlike previous measures that either use AOI sequences or forgo temporal order, the new measure defines scanpaths as a series of geometric vectors and compares temporally aligned scanpaths across several dimensions: shape, fixation position, length, direction, and fixation duration. This approach offers more multifaceted insights to how similar two scanpaths are. Eight fictitious scanpath pairs are tested to elucidate the strengths of the new measure, both in itself and compared to two of the currently most popular measures - the Levenshtein distance and attention map correlation.},
address = {New York, New York, USA},
author = {Jarodzka, Halszka and Holmqvist, Kenneth and Nystr\"{o}m, Marcus},
booktitle = {Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10},
doi = {10.1145/1743666.1743718},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10/2010/Jarodzka, Holmqvist, Nystr\"{o}m/Jarodzka, Holmqvist, Nystr\"{o}m - 2010 - A vector-based, multidimensional scanpath similarity measure.pdf:pdf},
isbn = {9781605589947},
keywords = {eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
pages = {211},
publisher = {ACM Press},
title = {{A vector-based, multidimensional scanpath similarity measure}},
url = {http://dl.acm.org/citation.cfm?id=1743718 http://portal.acm.org/citation.cfm?doid=1743666.1743718},
year = {2010}
}
@article{Lasecki2013,
address = {New York, New York, USA},
author = {Lasecki, Walter S. and Miller, Christopher D. and Bigham, Jeffrey P.},
doi = {10.1145/2470654.2466269},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2033},
publisher = {ACM Press},
title = {{Warping time for more effective real-time crowdsourcing}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466269},
year = {2013}
}
@inproceedings{Kelner2007,
abstract = {Software usability is highly dependent on the supported interaction at the interface level. A number of techniques have been described in the literature, and some of those used in the context of virtual and augmented realities together with implementation aspects are presented in this chapter.},
address = {Petropolis},
author = {Kelner, Judith and Teichrieb, Veronica},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2007/Machado/Machado - 2007 - Dispositivos H\'{a}pticos para Interfaces de Realidade Virtual e Aumentada.pdf:pdf},
pages = {52--70},
publisher = {SBC},
title = {{T\'{e}cnicas de Intera\c{c}\~{a}o para Ambientes de Realidade Virtual e Aumentada}},
year = {2007}
}
@inproceedings{Reitmayr2006,
abstract = {This paper presents a model-based hybrid tracking system for outdoor augmented reality in urban environments enabling accurate, realtime overlays for a handheld device. The system combines several well-known approaches to provide a robust experience that surpasses each of the individual components alone: an edge-based tracker for accurate localisation, gyroscope measurements to deal with fast motions, measurements of gravity and magnetic field to avoid drift, and a back store of reference frames with online frame selection to re-initialize automatically after dynamic occlusions or failures. A novel edge-based tracker dispenses with the conventional edge model, and uses instead a coarse, but textured, 3D model. This yields several advantages: scale-based detail culling is automatic, appearance-based edge signatures can be used to improve matching and the models needed are more commonly available. The accuracy and robustness of the resulting system is demonstrated with comparisons to map-based ground truth data.},
address = {Santa Barbard, CA},
author = {Reitmayr, Gerhard and Drummond, Tom},
booktitle = {2006 IEEE/ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2006.297801},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2006 IEEEACM International Symposium on Mixed and Augmented Reality/2006/Reitmayr, Drummond/Reitmayr, Drummond - 2006 - Going out robust model-based tracking for outdoor augmented reality.pdf:pdf},
isbn = {1-4244-0650-1},
keywords = {augmented reality,outdoors},
mendeley-tags = {augmented reality,outdoors},
month = oct,
pages = {109--118},
publisher = {IEEE},
title = {{Going out: robust model-based tracking for outdoor augmented reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4079263 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4079263},
year = {2006}
}
@article{Benford1998,
author = {Benford, Steve and Greenhalgh, Chris and Reynard, Gail and Brown, Chris and Koleva, Boriana},
doi = {10.1145/292834.292836},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM Transactions on Computer-Human Interaction/1998/Benford et al/Benford et al. - 1998 - Understanding and constructing shared spaces with mixed-reality boundaries.pdf:pdf},
issn = {10730516},
journal = {ACM Transactions on Computer-Human Interaction},
month = sep,
number = {3},
pages = {185--223},
title = {{Understanding and constructing shared spaces with mixed-reality boundaries}},
url = {http://portal.acm.org/citation.cfm?doid=292834.292836},
volume = {5},
year = {1998}
}
@article{Matsushita2004,
abstract = {We present a method for estimating intrinsic images from a fixed-viewpoint image sequence captured under changing illumination directions. Previous work on this problem reduces the influence of shadows on reflectance images, but does not address shading effects which can significantly degrade reflectance image estimation under the typically biased sampling of illumination directions. In this paper, we describe how biased illumination sampling leads to biased estimates of reflectance image derivatives. To avoid the effects of illumination bias, we propose a solution that explicitly models spatial and temporal constraints over the image sequence. With this constraint network, our technique minimizes a regularization function that takes advantage of the biased image derivatives to yield reflectance images less influenced by shading.},
author = {Matsushita, Yasuyuki and Lin, Stephen and Kang, Sing Bing and Shum, Heung-yeung},
doi = {10.1007/978-3-540-24671-8\_22},
journal = {Computer Vision – ECCV},
number = {2004},
pages = {274--286},
title = {{Estimating Intrinsic Images from Image Sequences with Biased Illumination}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-24671-8\_22},
volume = {3022},
year = {2004}
}
@article{Istance2009,
abstract = {Massively multiplayer online role-playing games, such as World of Warcraft, have become the most widespread 3D graphical environments with millions of active subscribers worldwide. People with severe motor impairments should be able to take part in these games without the extent of their disability being apparent to others online. Eye gaze is a high bandwidth modality that can support this. We have developed a software device that uses gaze input in different modes for emulating mouse and keyboard events appropriate for interacting with on-line games. We report an evaluation study that investigated gaze-based interaction with World of Warcraft using the device. We have found that it is feasible to carry out tasks representative of game play at a beginners skill level using gaze alone. The results from the locomotion task part of the study show similar performance for gaze-based interaction compared with a keyboard and mouse. We discuss the usability issues that arose when completing three types of tasks in the game and the implications of these for playing of this type of game using gaze as the only input modality.},
author = {Istance, H and Hyrskykari, A and Vickers, S and Chaves, T},
doi = {10.1007/978-3-642-03655-2\_36},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Human-Computer Interaction – INTERACT 2009/2009/Istance et al/Istance et al. - 2009 - For your eyes only Controlling 3d online games by eye-gaze.pdf:pdf},
journal = {Human-Computer Interaction – INTERACT 2009},
keywords = {eye tracking,video game},
mendeley-tags = {eye tracking,video game},
pages = {314--327},
title = {{For your eyes only: Controlling 3d online games by eye-gaze}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-03655-2\_36},
volume = {5726},
year = {2009}
}
@inproceedings{Kuksenok2013,
abstract = {Like most online content, user-generated content (UGC) poses accessibility barriers to users with disabilities. However, the accessibility difficulties pervasive in UGC warrant discussion and analysis distinct from other kinds of online content. Content authors, community culture, and the authoring tool itself all affect UGC accessibility. The choices, resources available, and strategies in use to ensure accessibility are different than for other types of online content. We contribute case studies of two UGC communities with accessible content: Wikipedia, where authors focus on access to visual materials and navigation, and an online health support forum where users moderate the cognitive accessibility of posts. Our data demonstrate real world moderation strategies and illuminate factors affecting success, such as community culture. We conclude with recommended strategies for creating a culture of accessibility around UGC.},
address = {New York, New York, USA},
author = {Kuksenok, Katie and Brooks, Michael and Mankoff, Jennifer},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470664},
isbn = {9781450318990},
pages = {59--68},
publisher = {ACM Press},
title = {{Accessible online content creation by end users}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470664},
year = {2013}
}
@article{Lammel2012,
author = {L\"{a}mmel, G and Plaue, M},
journal = {Pedestrian and Evacuation Dynamics},
title = {{Getting out of the way: collision avoiding pedestrian models compared to the real world}},
url = {https://svn.vsp.tu-berlin.de/repos/public-svn/publications/vspwp/2011/11-25/20120511accepted.pdf},
year = {2012}
}
@article{Carvalho2011,
author = {Carvalho, Felipe and Trindade, Daniel R. and Dam, Peter F. and Raposo, Alberto and Santos, Ismael H. F. Dos},
doi = {10.1109/SVR.2011.30},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 XIII Symposium on Virtual Reality/2011/Carvalho et al/Carvalho et al. - 2011 - Dynamic Adjustment of Stereo Parameters for Virtual Reality Tools.pdf:pdf},
isbn = {978-1-4577-0661-5},
journal = {2011 XIII Symposium on Virtual Reality},
keywords = {-virtual},
month = may,
pages = {66--72},
publisher = {Ieee},
title = {{Dynamic Adjustment of Stereo Parameters for Virtual Reality Tools}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951836},
year = {2011}
}
@inproceedings{Sunkavalli2008,
abstract = {In an extended image sequence of an outdoor scene, one observes changes in color induced by variations in the spectral composition of daylight. This paper proposes a model for these temporal color changes and explores its use for the analysis of outdoor scenes from time-lapse video data. We show that the time-varying changes in direct sunlight and ambient skylight can be recovered with this model, and that an image sequence can be decomposed into two corresponding components. The decomposition provides access to both radiometric and geometric information about a scene, and we demonstrate how this can be exploited for a variety of visual tasks, including color-constancy, background subtraction, shadow detection, scene reconstruction, and camera geo-location.},
address = {Anchorage, AK},
author = {Sunkavalli, Kalyan and Romeiro, Fabiano and Matusik, Wojciech and Zickler, Todd and Pfister, Hanspeter},
booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2008.4587411},
isbn = {978-1-4244-2242-5},
month = jun,
pages = {1--8},
publisher = {IEEE},
title = {{What do color changes reveal about an outdoor scene?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4587411},
year = {2008}
}
@misc{Majumder2011,
author = {Majumder, Aditi},
keywords = {SBGames},
mendeley-tags = {SBGames},
title = {{View-Perspective Projection}},
url = {http://www.ics.uci.edu/~majumder/CG/classes/wk3-cls1-persp.pdf},
urldate = {2013-07-22},
year = {2011}
}
@article{Wang2005,
abstract = {This paper mainly focuses on the problem of camera calibration and 3D reconstruction from a single view of structured scene. It is well known that three constraints on the intrinsic parameters of a camera can be obtained from the vanishing points of three mutually orthogonal directions. However, there usually exist one or several pairs of line segments, which are mutually orthogonal and lie in the pencil of planes defined by two of the vanishing directions in the structured scenes. It is proved in this paper that a new independent constraint to the image of the absolute conic can be obtained if the pair of line segments is of equal length or with known length ratio in space. The constraint is further studied both in terms of the vanishing points and the images of circular points. Hence, four independent constraints on a camera are obtained from one image, and the camera can be calibrated under the widely accepted assumption of zero-skew. This paper also presents a simple method for the recovery of camera extrinsic parameters and projection matrix with respect to a given world coordinate system. Furthermore, several methods are presented to estimate the positions and poses of space planar surfaces from the recovered projection matrix and scene constraints. Thus, a scene structure can be reconstructed by combining the planar patches. Extensive experiments on simulated data and real images, as well as a comparative test with other methods in the literature, validate our proposed methods.},
author = {Wang, Guanghui and Tsui, Hung-Tat and Hu, Zhanyi and Wu, Fuchao},
doi = {10.1016/j.imavis.2004.07.008},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Image and Vision Computing/2005/Wang et al/Wang et al. - 2005 - Camera calibration and 3D reconstruction from a single view based on scene constraints.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = mar,
number = {3},
pages = {311--323},
title = {{Camera calibration and 3D reconstruction from a single view based on scene constraints}},
url = {http://www.sciencedirect.com/science/article/pii/S026288560400191X http://linkinghub.elsevier.com/retrieve/pii/S026288560400191X},
volume = {23},
year = {2005}
}
@inproceedings{Belluta1989,
author = {Belluta, P and Collini, G and Verri, V and Torre, V},
booktitle = {IEEE Workshop on interpretation of 3D scenes},
pages = {41--49},
title = {{3D visual information from vanishing points}},
year = {1989}
}
@article{Marple-Horvat1996,
annote = {- keyword coletado},
author = {Marple-Horvat, DE and Gilbey, SL and Hollands, MA},
doi = {10.1016/0165-0270(96)00049-0},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of Neuroscience Methods/1996/Marple-Horvat, Gilbey, Hollands/Marple-Horvat, Gilbey, Hollands - 1996 - A method for automatic identification of saccades from eye movement recordings.pdf:pdf},
issn = {01650270},
journal = {Journal of Neuroscience Methods},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
month = aug,
number = {2},
pages = {191--195},
title = {{A method for automatic identification of saccades from eye movement recordings}},
url = {http://www.sciencedirect.com/science/article/pii/0165027096000490 http://linkinghub.elsevier.com/retrieve/pii/0165027096000490},
volume = {67},
year = {1996}
}
@inproceedings{Mohammadi2011,
abstract = {Pupil center localization is fundamental to calculate eye orientation and gaze direction in video-based systems. In previous techniques, either it is assumed that the pupil center is same as iris center or active illumination is utilized. In this paper, we have developed a new technique which utilizes eye geometry to localize pupil center without requiring special lighting. The main idea in the proposed technique is to restrict search space of the pupil center to the minor diameter of the iris ellipse proved from the geometrical model of eye proposed in this paper. In the proposed technique, we fit an ellipse to iris boundary, which is very simpler than ellipse fitting for pupil boundary in images on the common illumination, and then estimate pupil center by searching minor diameter of the iris ellipse. The performance of the method has been evaluated on both synthetic and real images.},
author = {Mohammadi, Mohammad Reza and Raie, Abolghasem},
booktitle = {2011 7th Iranian Conference on Machine Vision and Image Processing},
doi = {10.1109/IranianMVIP.2011.6121561},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 7th Iranian Conference on Machine Vision and Image Processing/2011/Mohammadi, Raie/Mohammadi, Raie - 2011 - A Novel Technique for Pupil Center Localization Based on Projective Geometry.pdf:pdf},
isbn = {978-1-4577-1535-8},
month = nov,
pages = {1--5},
publisher = {IEEE},
title = {{A Novel Technique for Pupil Center Localization Based on Projective Geometry}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6121561\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=.QT.projective+geometry.QT.},
year = {2011}
}
@article{Wither2007,
author = {Wither, Jason and DiVerdi, Stephen and Hollerer, Tobias},
doi = {10.1109/ISMAR.2007.4538832},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality/2007/Wither, DiVerdi, Hollerer/Wither, DiVerdi, Hollerer - 2007 - Evaluating Display Types for AR Selection and Annotation.pdf:pdf},
isbn = {978-1-4244-1749-0},
journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
month = nov,
pages = {1--4},
publisher = {Ieee},
title = {{Evaluating Display Types for AR Selection and Annotation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538832},
year = {2007}
}
@inproceedings{Gong2009,
abstract = {This paper has summarized the conceptions of human-machine systems and human-computer interface,studied and analyzed the ways of human-computer interaction. Under these instructions, it proposed the design process and methods of human-computer interface,as well as some attentive points and the basic principles to be abided by during the design process, which gives some interface designation basis for the design of human machine systems and has certain guidance significance.The major ideas of this paper are to explain how to improve the human-machine systems, how to make machines adapt to and serve humans effectively and better,thereby brings the humans, machines and environment into a harmonious relationship.},
address = {Bangkok},
author = {Gong, Chaohua},
booktitle = {2009 International Conference on Computer and Automation Engineering},
doi = {10.1109/ICCAE.2009.23},
isbn = {978-0-7695-3569-2},
month = mar,
pages = {230--233},
publisher = {IEEE},
title = {{Human-Computer Interaction: Process and Principles of Human-Computer Interface Design}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4804523},
year = {2009}
}
@article{Huff2011,
author = {Huff, Rafael and Gierlinger, Thomas and Kuijper, Arjan and Stork, Andre and Fellner, Dieter},
doi = {10.1109/SVR.2011.18},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 XIII Symposium on Virtual Reality/2011/Huff et al/Huff et al. - 2011 - A Comparison of xPU Platforms Exemplified with Ray Tracing Algorithms.pdf:pdf},
isbn = {978-1-4577-0661-5},
journal = {2011 XIII Symposium on Virtual Reality},
keywords = {-systems and software architectures,application study,cpus,hybrid computer systems,multi-core},
month = may,
pages = {1--8},
publisher = {Ieee},
title = {{A Comparison of xPU Platforms Exemplified with Ray Tracing Algorithms}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951829},
year = {2011}
}
@inproceedings{Santos2012a,
author = {Santos, Artur Lira Dos and Lemos, Diego and Lindoso, Jorge Eduardo Falcao and Teichrieb, Veronica},
booktitle = {2012 14th Symposium on Virtual and Augmented Reality},
doi = {10.1109/SVR.2012.8},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/1968/Lemos, Teichrieb/Lemos, Teichrieb - 1968 - Real Time Ray Tracing for Augmented Reality.pdf:pdf},
isbn = {978-1-4673-1929-4},
keywords = {2,a coprocessor for executing,augmented reality,cuda,highly parallel algorithms,including ray tracing in,kinect,occlusion,ray tracing,real,real time,reflection,refraction,they are used as},
month = may,
pages = {131--140},
publisher = {IEEE},
title = {{Real Time Ray Tracing for Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297569},
year = {2012}
}
@article{Teixeira2012,
author = {Teixeira, Joao Marcelo and Reis, Bernardo and Macedo, Samuel and Kelner, Judith},
doi = {10.1109/SVR.2012.20},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Teixeira et al/Teixeira et al. - 2012 - OpenClosed Hand Classification Using Kinect Data.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {-hand gesture classification,natural interaction},
month = may,
pages = {18--25},
publisher = {Ieee},
title = {{Open/Closed Hand Classification Using Kinect Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297556},
year = {2012}
}
@inproceedings{herrera,
author = {Herrera, Daniel and Kannala, Juho and Heikkil\"{a}, Janne},
booktitle = {Computer Analysis of Images and Patterns},
organization = {Springer},
pages = {437--445},
title = {{Accurate and practical calibration of a depth and color camera pair}},
year = {2011}
}
@article{Lee2013a,
address = {New York, New York, USA},
author = {Lee, Jaebong and Choi, Seungmoon},
doi = {10.1145/2470654.2481354},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2567},
publisher = {ACM Press},
title = {{Real-time perception-level translation from audio signals to vibrotactile effects}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481354},
year = {2013}
}
@inproceedings{Bostanci2012,
abstract = {An algorithm for finding planar features from a 3D point cloud by Kinect's depth sensor is described in this paper. The algorithm uses the explicit definition of a plane which allows storing only four parameters per plane rather than storing thousands of points. Extraction of multiple planes from the same set of points is prevented using a rejection mechanism. Parallelism is used for an average speed-up of 2.3:1. Details of the algorithm and results are given along with a discussion of how the calibration of the sensor affects the projections.},
author = {Bostanci, Erkan and Kanwal, Nadia and Clark, Adrian F.},
booktitle = {2012 4th Computer Science and Electronic Engineering Conference (CEEC)},
doi = {10.1109/CEEC.2012.6375388},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 4th Computer Science and Electronic Engineering Conference (CEEC)/2012/Bostanci, Kanwal, Clark/Bostanci, Kanwal, Clark - 2012 - Extracting planar features from Kinect sensor.pdf:pdf},
isbn = {978-1-4673-2666-7},
month = sep,
pages = {111--116},
publisher = {IEEE},
title = {{Extracting planar features from Kinect sensor}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6375388\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=kinect+calibration},
year = {2012}
}
@book{Brahmbhatt2013,
author = {Brahmbhatt, Samarth},
isbn = {1430260793},
keywords = {opencv},
mendeley-tags = {opencv},
pages = {244},
publisher = {Apress},
title = {{Practical OpenCV}},
url = {http://www.amazon.com/Practical-OpenCV-Samarth-Brahmbhatt/dp/1430260793},
year = {2013}
}
@article{Howley2013,
address = {New York, New York, USA},
author = {Howley, Iris and Newman, Todd},
doi = {10.1145/2470654.2481314},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13/2013/Howley, Newman/Howley, Newman - 2013 - Factors impacting community response in an interest-sharing network.pdf:pdf},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2283},
publisher = {ACM Press},
title = {{Factors impacting community response in an interest-sharing network}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481314},
year = {2013}
}
@inproceedings{Havran2005,
abstract = {We present an interactive system for fully dynamic scene lighting using captured high dynamic range (HDR) video environment maps. The key component of our system is an algorithm for efficient decomposition of HDR video environment map captured over hemisphere into a set of representative directional light sources, which can be used for the direct lighting computation with shadows using graphics hardware. The resulting lights exhibit good temporal coherence and their number can be adaptively changed to keep a constant framerate while good spatial distribution (stratification) properties are maintained. We can handle a large number of light sources with shadows using a novel technique which reduces the cost of BRDF-based shading and visibility computations. We demonstrate the use of our system in a mixed reality application in which real and synthetic objects are illuminated by consistent lighting at interactive framerates.},
address = {Konstanz, Germany},
author = {Havran, V and Smyk, M and Krawczyk, G and Myszkowski, K},
booktitle = {EGSR'05 Proceedings of the Sixteenth Eurographics conference on Rendering Techniques},
doi = {10.2312/EGWR/EGSR05/031-042},
pages = {31--42},
publisher = {Eurographics Association},
title = {{Interactive System for Dynamic Scene Lighting using Captured Video Environment Maps}},
year = {2005}
}
@article{Taylor2013,
address = {New York, New York, USA},
author = {Taylor, Alex S. and Piterman, Nir and Ishtiaq, Samin and Fisher, Jasmin and Cook, Byron and Cockerton, Caitlin and Bourton, Sam and Benque, David},
doi = {10.1145/2470654.2470725},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {493},
publisher = {ACM Press},
title = {{At the interface of biology and computation}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470725},
year = {2013}
}
@article{Georgel2009,
author = {Georgel, Pierre and Schroeder, Pierre and Navab, Nassir},
doi = {10.1109/MCG.2009.110},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Computer Graphics And Applications/2009/Georgel, Schroeder, Navab/Georgel, Schroeder, Navab - 2009 - Navigation Tools for Augmented CAD Viewing.pdf:pdf},
issn = {0272-1716},
journal = {IEEE Computer Graphics And Applications},
pages = {1--8},
title = {{Navigation Tools for Augmented CAD Viewing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5288521},
year = {2009}
}
@article{Savelsbergh2002,
abstract = {We used a novel methodological approach to examine skill-based differences in anticipation and visual search behaviour during the penalty kick in soccer. Expert and novice goalkeepers were required to move a joystick in response to penalty kicks presented on film. The proportion of penalties saved was assessed, as well as the frequency and time of initiation of joystick corrections. Visual search behaviour was examined using an eye movement registration system. Expert goalkeepers were generally more accurate in predicting the direction of the penalty kick, waited longer before initiating a response and made fewer corrective movements with the joystick. The expert goalkeepers used a more efficient search strategy involving fewer fixations of longer duration to less disparate areas of the display. The novices spent longer fixating on the trunk, arms and hips, whereas the experts found the kicking leg, non-kicking leg and ball areas to be more informative, particularly as the moment of foot-ball contact approached. No differences in visual search behaviour were observed between successful and unsuccessful penalties. The results have implications for improving anticipation skill at penalty kicks.},
author = {Savelsbergh, Geert J P and Williams, A Mark and {Van der Kamp}, John and Ward, Paul},
doi = {10.1080/026404102317284826},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of sports sciences/2002/Savelsbergh et al/Savelsbergh et al. - 2002 - Visual search, anticipation and expertise in soccer goalkeepers.pdf:pdf},
issn = {0264-0414},
journal = {Journal of sports sciences},
keywords = {Adult,Attention,Attention: physiology,Eye Movements,Eye Movements: physiology,Fixation,Humans,Male,Motion Perception,Motion Perception: physiology,Multivariate Analysis,Ocular,Ocular: physiology,Probability,Psychomotor Performance,Psychomotor Performance: physiology,Reaction Time,Reaction Time: physiology,Soccer,Soccer: physiology,Space Perception,Space Perception: physiology,Visual Perception,Visual Perception: physiology},
month = mar,
number = {3},
pages = {279--87},
pmid = {11999482},
title = {{Visual search, anticipation and expertise in soccer goalkeepers.}},
url = {http://www.tandfonline.com/doi/full/10.1080/026404102317284826 http://www.ncbi.nlm.nih.gov/pubmed/11999482},
volume = {20},
year = {2002}
}
@article{Obrist2013,
address = {New York, New York, USA},
author = {Obrist, Marianna and Seah, Sue Ann and Subramanian, Sriram},
doi = {10.1145/2470654.2466220},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1659},
publisher = {ACM Press},
title = {{Talking about tactile experiences}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466220},
year = {2013}
}
@inproceedings{Divjak2008,
address = {Leeds, UK},
author = {Divjak, M and Bischof, H},
booktitle = {First International Workshop on Tracking Humans for the Evaluation of their Motion in Image Sequences - THEMIS},
file = {:home/acmt/Dropbox/Documentos/Mendeley/First International Workshop on Tracking Humans for the Evaluation of their Motion in Image Sequences - THEMIS/2008/Divjak, Bischof/Divjak, Bischof - 2008 - Real-time video-based eye blink analysis for detection of low blink-rate during computer use.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {99--107},
title = {{Real-time video-based eye blink analysis for detection of low blink-rate during computer use}},
url = {http://iselab.cvc.uab.es/themis2008/ProceedingsTHEMIS2008.pdf\#page=107},
year = {2008}
}
@article{Cheriyadat2008,
abstract = {We discuss the problem of detecting dominant motions in dense crowds, a challenging and societally important problem. First, we survey the general literature of computer vision algorithms that deal with crowds of people, including model- and feature-based approaches to segmentation and tracking as well as algorithms that analyze general motion trends. Second, we present a system for automatically identifying dominant motions in a crowded scene. Accurately tracking individual objects in such scenes is difficult due to inter- and intra-object occlusions that cannot be easily resolved. Our approach begins by independently tracking low-level features using optical flow. While many of the feature point tracks are unreliable, we show that they can be clustered into smooth dominant motions using a distance measure for feature trajectories based on longest common subsequences. Results on real video sequences demonstrate that the approach can successfully identify both dominant and anomalous motions in crowded scenes. These fully-automatic algorithms could be easily incorporated into distributed camera networks for autonomous scene analysis.},
author = {a.M. Cheriyadat and Radke, R.J.},
doi = {10.1109/JSTSP.2008.2001306},
issn = {1932-4553},
journal = {IEEE Journal of Selected Topics in Signal Processing},
keywords = {trajectory extraction},
mendeley-tags = {trajectory extraction},
month = aug,
number = {4},
pages = {568--581},
title = {{Detecting Dominant Motions in Dense Crowds}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4629878},
volume = {2},
year = {2008}
}
@article{Bera2014,
abstract = {We present a novel, realtime algorithm to compute the trajectory of each pedestrian in moderately dense crowd scenes. Our formulation is based on an adaptive particle filtering scheme that uses a multi-agent motion model based on velocity-obstacles, and takes into account local interactions as well as physical and personal constraints of each pedestrian. Our method dynamically changes the number of particles allocated to each pedestrian based on different confidence metrics. Additionally, we use a new high-definition crowd video dataset, which is used to evaluate the performance of different pedestrian tracking algorithms. This dataset consists of videos of indoor and outdoor scenes, recorded at different locations with 30-80 pedestrians. We highlight the performance benefits of our algorithm over prior techniques using this dataset. In practice, our algorithm can compute trajectories of tens of pedestrians on a multi-core desktop CPU at interactive rates (27-30 frames per second). To the best of our knowledge, our approach is 4-5 times faster than prior methods, which provide similar accuracy.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.2826v1},
author = {Bera, Aniket and Manocha, Dinesh},
eprint = {arXiv:1402.2826v1},
journal = {CoRR},
title = {{Realtime Multilevel Crowd Tracking using Reciprocal Velocity Obstacles}},
volume = {abs/1402.2},
year = {2014}
}
@inproceedings{Reitsma2004,
abstract = {Realistic and directable humanlike characters are an ongoing goal in animation. Motion graph data structures hold much promise for achieving this goal. However, the quality of the results obtained from a motion graph may not be easy to predict from the input motion segments. This paper introduces the idea of assessing a data structure such as a motion graph for its utility in a particular application. We focus on navigation tasks and define metrics for evaluating expected path quality and coverage for a given environment. One key to evaluating a motion graph for navigation tasks is to first embed it into the environment in a way that captures all possible paths that might result from "playing back" the motion graph within that environment. This paper describes an algorithm for accomplishing this embedding that preserves the flexibility of the original motion graph. We use the metrics defined in this paper to compare motion datasets and to highlight areas where these datasets could be improved.},
address = {New York, New York, USA},
author = {Reitsma, P. S. a. and Pollard, N. S.},
booktitle = {Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation - SCA '04},
doi = {10.1145/1028523.1028536},
isbn = {3905673142},
issn = {17275288},
pages = {89},
publisher = {ACM Press},
title = {{Evaluating motion graphs for character navigation}},
url = {http://portal.acm.org/citation.cfm?doid=1028523.1028536},
year = {2004}
}
@inproceedings{Kan2012a,
abstract = {In this paper we present a novel high-quality rendering system for Augmented Reality (AR). We study ray-tracing based rendering techniques in AR with the goal of achieving real-time performance and improving visual quality as well as visual coherence between real and virtual objects in a final composited image. A number of realistic and physically correct rendering effects are demonstrated, that have not been presented in real-time AR environments before. Examples are high-quality specular effects such as caustics, refraction, reflection, together with a depth of field effect and anti-aliasing. We present a new GPU implementation of photon mapping and its application for the calculation of caustics in environments where real and virtual objects are combined. The composited image is produced on-the-fly without the need of any preprocessing step. A main contribution of our work is the achievement of interactive rendering speed for high-quality ray-tracing algorithms in AR setups. Finally we performed an evaluation to study how users perceive visual quality and visual coherence with different realistic rendering effects. The results of our user study show that in 40.1\% cases users mistakenly judged virtual objects as real ones. Moreover we show that high-quality rendering positively affects the perceived visual coherence.},
address = {Atlanta, GA},
author = {Kan, P. and Kaufmann, H.},
booktitle = {2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2012.6402546},
isbn = {978-1-4673-4662-7},
month = nov,
pages = {99--108},
publisher = {IEEE},
title = {{High-quality reflections, refractions, and caustics in Augmented Reality and their contribution to visual coherence}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6402546},
year = {2012}
}
@inproceedings{Fukiage2012,
abstract = {One of the challenges in mixed reality (MR) applications is handling contradictory occlusions between real and virtual objects. The previous studies have tried to solve the occlusion problem by extracting the foreground region from the real image. However, real-time occlusion handling is still difficult since it takes too much computational cost to precisely segment foreground regions in a complex scene. In this study, therefore, we proposed an alternative solution to the occlusion problem that does not require precise foreground-background segmentation. In our method, a virtual object is blended with a real scene so that the virtual object can be perceived as being behind the foreground region. For this purpose, we first investigated characteristics of human transparency perception in a psychophysical experiment. Then we made a blending algorithm applicable to real scenes based on the results of the experiment.},
address = {Atlanta, GA},
author = {Fukiage, Taiki and Oishi, Takeshi and Ikeuchi, Katsushi},
booktitle = {2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2012.6402549},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)/2012/Fukiage, Oishi, Ikeuchi/Fukiage, Oishi, Ikeuchi - 2012 - Reduction of contradictory partial occlusion in mixed reality by using characteristics of transparency.pdf:pdf},
isbn = {978-1-4673-4662-7},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = nov,
pages = {129--139},
publisher = {IEEE},
title = {{Reduction of contradictory partial occlusion in mixed reality by using characteristics of transparency perception}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6402549 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6402549},
year = {2012}
}
@article{Krevelen2010,
abstract = {We are on the verge of ubiquitously adopting Augmented Reality (AR) technologies to enhance our percep- tion and help us see, hear, and feel our environments in new and enriched ways. AR will support us in fields such as education, maintenance, design and reconnaissance, to name but a few. This paper describes the field of AR, including a brief definition and development history, the enabling technologies and their characteristics. It surveys the state of the art by reviewing some recent applications of AR technology as well as some known limitations regarding human factors in the use of AR systems that developers will need to overcome.},
author = {Krevelen, DWF Van and Poelman, R},
journal = {International Journal of Virtual Reality},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
number = {2},
pages = {1--20},
title = {{A survey of augmented reality technologies, applications and limitations}},
url = {http://kjcomps.6te.net/upload/paper1 .pdf},
volume = {9},
year = {2010}
}
@inproceedings{Takemura2010,
abstract = {The portability of an eye tracking system encourages us to develop a technique for estimating 3D point-of-regard. Unlike conventional methods, which estimate the position in the 2D image coordinates of the mounted camera, such a technique can represent richer gaze information of the human moving in the larger area. In this paper, we propose a method for estimating the 3D point-of-regard and a visualization technique of gaze trajectories under natural head movements for the head-mounted device. We employ visual SLAM technique to estimate head configuration and extract environmental information. Even in cases where the head moves dynamically, the proposed method could obtain 3D point-of-regard. Additionally, gaze trajectories are appropriately overlaid on the scene camera image.},
address = {New York, New York, USA},
author = {Takemura, Kentaro and Kohashi, Yuji and Suenaga, Tsuyoshi and Takamatsu, Jun and Ogasawara, Tsukasa},
booktitle = {Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10},
doi = {10.1145/1743666.1743705},
file = {::},
isbn = {9781605589947},
keywords = {gaze 3D},
mendeley-tags = {gaze 3D},
pages = {157},
publisher = {ACM Press},
title = {{Estimating 3D point-of-regard and visualizing gaze trajectories under natural head movements}},
url = {http://dl.acm.org/citation.cfm?id=1743705 http://portal.acm.org/citation.cfm?doid=1743666.1743705},
year = {2010}
}
@inproceedings{Xiao-ming2001,
abstract = {Collaborative modeling and simulation take strong advantage of resource sharing across the Internet for an extended enterprise or a virtual enterprise. Collaborative graphical modeling is more attractive, but also more difficult, when applied on the World Wide Web. One solution for integrating 2D graphical modeling and a Web-based online analyzer is discussed, and a prototype is realized through a campus network. An order management model is designed collaboratively using the EXTEND modeling tool to map the operations involved in the scheduling, production and transportation of a geographically distributed iron-ore enterprise. The distributed managers can submit orders and then obtain the simulation results online from a Web online analyzer which is realized by an Apache Web server, PHP and mySQL tools. The prototype shows effective experimental results. It is intended to study the system further in creating a complete model base for a simulation server and a graphical user interface for browsers},
address = {London},
author = {Xiao-ming, Z. and Zi-qiong, D.},
booktitle = {Proceedings of the Sixth International Conference on Computer Supported Cooperative Work in Design (IEEE Cat. No.01EX472)},
doi = {10.1109/CSCWD.2001.942301},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the Sixth International Conference on Computer Supported Cooperative Work in Design (IEEE Cat. No.01EX472)/2001/Xiao-ming, Zi-qiong/Xiao-ming, Zi-qiong - 2001 - Collaborative modeling and simulation for order management in virtual enterprise.pdf:pdf},
isbn = {0-660-18493-1},
pages = {444--448},
publisher = {NRC Res. Press},
title = {{Collaborative modeling and simulation for order management in virtual enterprise}},
url = {http://en.cnki.com.cn/Article\_en/CJFDTOTAL-JSJZ200206019.htm http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=942301},
year = {2001}
}
@article{Warnock2013,
address = {New York, New York, USA},
author = {Warnock, David and McGee-Lennon, Marilyn and Brewster, Stephen},
doi = {10.1145/2470654.2466139},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Multimodal, Older Users, Notifications, Reminders},
pages = {1091},
publisher = {ACM Press},
title = {{Multiple notification modalities and older users}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466139},
year = {2013}
}
@article{Chaves2012,
author = {Chaves, Thiago and Figueiredo, Lucas and Gama, Alana Da and Araujo, Cristiano De and Teichrieb, Veronica},
doi = {10.1109/SVR.2012.16},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Chaves et al/Chaves et al. - 2012 - Human Body Motion and Gestures Recognition Based on Checkpoints.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {body tracking,human motion analysis,rgb-d},
month = may,
pages = {271--278},
publisher = {Ieee},
title = {{Human Body Motion and Gestures Recognition Based on Checkpoints}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297539},
year = {2012}
}
@article{Fischer2005a,
abstract = {The ultimate goal of many applications of augmented reality is to immerse the user into the augmented scene, which is enriched with virtual models. In order to achieve this immersion, it is necessary to create the visual impression that the graphical objects are a natural part of the user’s environment. Producing this effect with conventional computer graphics algorithms is a complex task. Various rendering artifacts in the three-dimensional graphics create a noticeable visual discrepancy between the real background image and virtual objects. We have recently proposed a novel approach to generating an augmented video stream. With this new method, the output images are a non-photorealistic reproduction of the augmented environment. Special stylization methods are applied to both the background camera image and the virtual objects. This way the visual realism of both the graphical foreground and the real background image is reduced, so that they are less distinguishable from each other. Here, we present a new method for the cartoon-like stylization of augmented reality images, which uses a novel post-processing filter for cartoon-like color segmentation and high-contrast silhouettes. In order to make a fast postprocessing of rendered images possible, the programmability of modern graphics hardware is exploited. We describe an implementation of the algorithm using the OpenGL Shading Language. The system is capable of generating a stylized augmented video stream of high visual quality at real-time frame rates. As an example application, we demonstrate the visualization of dinosaur bone datasets in stylized augmented reality.},
author = {Fischer, J and Bartz, D},
file = {:home/acmt/Dropbox/Documentos/Mendeley/WSI/2005/Fischer, Bartz/Fischer, Bartz - 2005 - Real-time Cartoon-like Stylization of AR Video Streams on the GPU.pdf:pdf},
journal = {WSI},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
title = {{Real-time Cartoon-like Stylization of AR Video Streams on the GPU}},
url = {http://w210.ub.uni-tuebingen.de/volltexte/2005/1987/},
volume = {18},
year = {2005}
}
@article{Hughes2003,
abstract = {The modern study of a crowd as a flowing continuum is a recent development. Distinct from a classical fluid because of the property that a crowd has the capacity to think, interesting new physical ideas are involved in its study. An appealing property of a crowd in motion is that the nonlinear, time-dependent, simultaneous equations representing a crowd are conformably mappable. This property makes many interesting applications analytically tractable. In this review examples are given in which the theory has been used to provide possible assistance in the annual Muslim Hajj, to understand the Battle of Agincourt, and, surprisingly, to locate barriers that actually increase the flow of pedestrians above that when there are no barriers present. Modern developments may help prevent some of the approximately two thousand deaths that annually occur in accidents owing to crowding.The field of crowd motion, that is, the field of “thinking fluids,” is an intriguing area of research with great promise.},
author = {Hughes, Roger L.},
doi = {10.1146/annurev.fluid.35.101101.161136},
issn = {0066-4189},
journal = {Annual Review of Fluid Mechanics},
keywords = {an appeal-,are involved in its,as a flowing continuum,capacity to think,conformal mapping,distinct from a classical,fluid because of the,has the,interesting new physical ideas,is a recent de-,pedestrians,property that a crowd,s abstract the modern,safety,study,study of a crowd,velopment},
month = jan,
number = {1},
pages = {169--182},
title = {{The Flow of Human Crowds}},
url = {http://www.annualreviews.org/doi/abs/10.1146/annurev.fluid.35.101101.161136},
volume = {35},
year = {2003}
}
@inproceedings{Holland2013a,
abstract = {This paper presents an objective evaluation of previously unexplored biometric techniques utilizing patterns identifiable in human eye movements to distinguish individuals. The distribution of primitive eye movement features are compared between eye movement recordings using algorithms based on the following statistical tests: the Ansari-Bradley test, the Mann-Whitney U-test, the two-sample Kolmogorov-Smirnov test, the two-sample t-test, and the two-sample Cramer-von Mises test. Score-level information fusion is applied and evaluated by: weighted mean, support vector machine, random forest, and likelihood ratio. The accuracy of each comparison/jusion algorithm is evaluated, with results suggesting that, on high resolution eye tracking equipment, it is possible to obtain equal error rates of 16.5\% and rank-1 identification rates of 82.6\% using the two-sample Cramér-von Mises test and score-level information fusion by random forest, the highest accuracy results on the considered dataset.},
address = {Madrid},
annote = {- cited by: 1
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Holland, Corey D. and Komogortsev, Oleg V},
booktitle = {2013 International Conference on Biometrics (ICB)},
doi = {10.1109/ICB.2013.6612953},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2013 International Conference on Biometrics (ICB)/2013/Holland, Komogortsev/Holland, Komogortsev - 2013 - Complex eye movement pattern biometrics Analyzing fixations and saccades.pdf:pdf},
isbn = {978-1-4799-0310-8},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
month = jun,
pages = {1--8},
publisher = {IEEE},
title = {{Complex eye movement pattern biometrics: Analyzing fixations and saccades}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6612953 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6612953},
year = {2013}
}
@article{Wallace2013,
address = {New York, New York, USA},
author = {Wallace, Jayne and Wright, Peter C. and McCarthy, John and Green, David Philip and Thomas, James and Olivier, Patrick},
doi = {10.1145/2468356.2479560},
file = {:home/acmt/Dropbox/Documentos/Mendeley/CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13/2013/Wallace et al/Wallace et al. - 2013 - A design-led inquiry into personhood in dementia.pdf:pdf},
isbn = {9781450319522},
journal = {CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13},
pages = {2883},
publisher = {ACM Press},
title = {{A design-led inquiry into personhood in dementia}},
url = {http://dl.acm.org/citation.cfm?doid=2468356.2479560},
year = {2013}
}
@book{Pressley2010,
address = {London},
author = {Pressley, Andrew},
edition = {2},
isbn = {9781848828902},
keywords = {Differential Geometry},
mendeley-tags = {Differential Geometry},
pages = {486},
publisher = {Springer},
title = {{Elementary Differential Geometry}},
url = {http://books.google.com/books?hl=en\&lr=\&id=OtbNXAIve\_AC\&oi=fnd\&pg=PP2\&dq=Elementary+Differential+Geometry\&ots=aKGg6OIMCi\&sig=ac5GkbDbZPJa7EWBr-aFjgNrYKA},
year = {2010}
}
@inproceedings{Kim2013,
abstract = {Prototyping of gestural interactions in the early phase of design is one of the most challenging tasks for designers without advanced programming skills. Relating users' input from gesture-based sensor values requires a great deal of effort on the designer's part and disturbs their reflective and creative thinking. To deal with this problem, we present EventHurdle, a visual gesture-authoring tool to support designers' explorative prototyping. It supports remote gestures from a camera, handheld gestures with physical sensors, and touch gestures by utilizing touch screens. EventHurdle allows designers to visually define and modify gestures through interaction workspace and graphical markup language with hurdles. Because the created gestures can be integrated into a prototype as programming code and automatically recognized, designers do not need to pay attention in sensor-related implementation. Two user studies and a recognition test are reported to discuss the acceptance and implications of explorative prototyping tools for designers.},
address = {New York, New York, USA},
author = {Kim, Ju-whan and Nam, Tek-jin},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470691},
isbn = {9781450318990},
pages = {267--276},
publisher = {ACM Press},
title = {{EventHurdle: supporting designers' exploratory interaction prototyping with gesture-based sensors}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470691},
year = {2013}
}
@inproceedings{Bimber2006a,
abstract = {We present techniques which create a consistent illumination between real and virtual objects inside an application specific optical see-through display: the virtual showcase. We use projectors and cameras to capture reflectance information from diffuse real objects and to illuminate them under new synthetic lighting conditions. Matching direct and indirect lighting effects, such as shading, shadows, reflections and color bleeding can be approximated at interactive rates in such a controlled mixed environment.},
address = {Tokyo, Japan},
author = {Bimber, O. and Grundhofer, A. and Wetzstein, G. and Knodel, S.},
booktitle = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
doi = {10.1109/ISMAR.2003.1240703},
isbn = {0-7695-2006-5},
pages = {198--207},
publisher = {IEEE Comput. Soc},
title = {{Consistent illumination within optical see-through augmented environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240703},
year = {2006}
}
@phdthesis{Pessoa2010a,
abstract = {A habilidade de interativamente mesclar o mundo real com o virtual abriu um leque de novas possibilidades na \'{a}rea de sistemas multim\'{\i}dia. O campo de pesquisa que trata desse problema \'{e} chamado de Realidade Aumentada. Em Realidade Aumentada, os elementos virtuais podem aparecer destacados do s objetos r eais ou fotorreali sticamente inseridos no mundo real. Dentro desse seg undo tipo de aplica\c{c}\~{a}o , pode - se citar: ferramentas de aux\'{\i}lio ao projeto de interiores, jogos eletr\^{o}nicos aumentados e aplica\c{c}\~{o}es para visualiza\c{c}\~{a}o de s\'{\i}tios hist\'{o}ricos. Na literatura pesquisada existe uma lacuna para ferramentas que auxiliem a cria\c{c}\~{a}o des se tipo de aplica\c{c}\~{a}o. Na tentativa de contorna r isso, e sta disserta\c{c}\~{a}o prop\~{o}e um pipeline para renderiza\c{c}\~{a}o fotorreal\'{\i}stica em aplica\c{c}\~{o}es de Realidade Aumentada que leva em considera\c{c}\~{a}o aspectos como: a ilumina\c{c}\~{a}o, as propriedades de reflet\^{a}ncia dos mater iais, o sombreamento, a composi\c{c}\~{a}o do mundo real com o mundo virtual e os efeitos de c\^{a}mera. Esse pipeline foi implementado como uma API , permitindo a realiza\c{c}\~{a}o de dois estudos de caso: uma ferramenta de edi\c{c}\~{a}o de materiais e uma ferramenta de aux\'{\i}lio ao projeto de interiores. Para obter taxas interativas de renderiza\c{c}\~{a}o, os gargalos do pipeline foram implementado s em GPU. Os resultados obtidos mostram que o pipeline proposto oferece ganhos consider\'{a}veis de realismo com rela\c{c}\~{a}o \`{a} visualiza\c{c}\~{a}o dos objetos virtuais},
author = {Pessoa, Saulo Andrade},
keywords = {Computa\c{c}\~{a}o Gr\'{a}fica,Fotorrealismo,GPU,Pipeline,Realidade Aumentada},
pages = {108},
school = {University of Pernambuco},
title = {{Um Pipeline para Renderiza\c{c}\~{a}o Fotorreal\'{\i}stica em Aplica\c{c}\~{o}es de Realidade Aumentada}},
year = {2010}
}
@inproceedings{Liu2012,
abstract = {In this paper, we present a method that can calibrates color camera and depth camera of Kinect simultaneously, and finally get the relative pose between them. The calibration process for color camera and depth camera is designed by taking advantage of two different method respectively. As we know, the depth information can be acquired readily from the depth image, however, under the consideration of the fuzzy boundary and the overlapping of object in depth image, we choose using one-dimensional object for calibrating depth camera and propose a nonlinear method to optimize its intrinsic parameter. As for color camera calibration, we design a cross shape object for calibration and validation. Both methods are strongly robust to noise and much easier to implement. The experiment result shows a better accuracy in comparison with the proprietary calibration procedure of the manufacturer.},
author = {Liu, Weihua and Fan, Yangyu and Zhong, Zhang and Lei, Tao},
booktitle = {2012 International Conference on Audio, Language and Image Processing},
doi = {10.1109/ICALIP.2012.6376614},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 International Conference on Audio, Language and Image Processing/2012/Liu et al/Liu et al. - 2012 - A new method for calibrating depth and color camera pair based on Kinect.pdf:pdf},
isbn = {978-1-4673-0174-9},
month = jul,
pages = {212--217},
publisher = {IEEE},
title = {{A new method for calibrating depth and color camera pair based on Kinect}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6376614\&contentType=Conference+Publications\&searchField\%3DSearch\_All\%26queryText\%3Dkinect+calibration http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6376614},
year = {2012}
}
@article{Kelley2013,
address = {New York, New York, USA},
author = {Kelley, Patrick Gage and Cranor, Lorrie Faith and Sadeh, Norman},
doi = {10.1145/2470654.2466466},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3393},
publisher = {ACM Press},
title = {{Privacy as part of the app decision-making process}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466466},
year = {2013}
}
@inproceedings{Knecht2010,
abstract = {In this paper we present a novel plausible realistic rendering method for mixed reality systems, which is useful for many real life application scenarios, like architecture, product visualization or edutainment. To allow virtual objects to seamlessly blend into the real environment, the real lighting conditions and the mutual illumination effects between real and virtual objects must be considered, while maintaining interactive frame rates (20-30fps). The most important such effects are indirect illumination and shadows cast between real and virtual objects. Our approach combines Instant Radiosity and Differential Rendering. In contrast to some previous solutions, we only need to render the scene once in order to find the mutual effects of virtual and real scenes. The dynamic real illumination is derived from the image stream of a fish-eye lens camera. We describe a new method to assign virtual point lights to multiple primary light sources, which can be real or virtual. We use imperfect shadow maps for calculating illumination from virtual point lights and have significantly improved their accuracy by taking the surface normal of a shadow caster into account. Temporal coherence is exploited to reduce flickering artifacts. Our results show that the presented method highly improves the illusion in mixed reality applications and significantly diminishes the artificial look of virtual objects superimposed onto real scenes.},
author = {Knecht, Martin and Traxler, Christoph and Mattausch, Oliver and Purgathofer, Werner and Wimmer, Michael},
booktitle = {2010 IEEE International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2010.5643556},
file = {::;:home/acmt/Dropbox/Documentos/Mendeley/2010 IEEE International Symposium on Mixed and Augmented Reality/2010/Knecht et al/Knecht et al. - 2010 - Differential Instant Radiosity for mixed reality.pdf:pdf},
isbn = {978-1-4244-9343-2},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = oct,
pages = {99--107},
publisher = {IEEE},
title = {{Differential Instant Radiosity for mixed reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5643556 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5643556},
year = {2010}
}
@inproceedings{Jung2004,
author = {Jung, Chang-Yun and Kim, Jin-Sung and Yoo, Chun-Sik and Kim, Yong-Sung},
booktitle = {2004 International Conference on Cyberworlds},
doi = {10.1109/CW.2004.13},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2004 International Conference on Cyberworlds/2004/Jung et al/Jung et al. - 2004 - A SMIL Document Generating System Using Temporal Scripts of Animation Component.pdf:pdf},
isbn = {0-7695-2140-1},
pages = {188--193},
publisher = {IEEE},
title = {{A SMIL Document Generating System Using Temporal Scripts of Animation Component}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/CW.2004.13 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1366173},
year = {2004}
}
@book{Dunn2002,
address = {Plano-TX},
author = {Dunn, Fletcher and Parberry, Ian},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2002/Dunn, Parberry/Dunn, Parberry - 2002 - 3D Math Primer for Graphics and Game Development.pdf:pdf},
isbn = {1556229119},
keywords = {3D Math,Euler,Quaternions,Transforming matrix,Translation,rotation,scale,skew},
mendeley-tags = {3D Math,Euler,Quaternions,Transforming matrix,Translation,rotation,scale,skew},
pages = {448},
publisher = {Wordware Game Math Library},
title = {{3D Math Primer for Graphics and Game Development}},
year = {2002}
}
@article{Cardoso2010,
abstract = {This chapter presents some possibilities of using Virtual Reality (VR) and Augmented Reality (AR) in Medicine and phobia treatment, showing some successful experiment examples. The main objective is to highlight fundamental interaction techniques to support this kind of ap- plication. Furthermore, important concepts and the main reasons why using VR/AR technology in such area are discussed by the authors.},
author = {Cardoso, Alexandre and Jr, Edgard Lamounier},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Intera\c{c}\~{a}o em Realidade Virtual e Aumentada/2010/Cardoso, Jr/Cardoso, Jr - 2010 - T\'{e}cnicas de Intera\c{c}\~{a}o de RV e RA na Medicina.pdf:pdf},
journal = {Intera\c{c}\~{a}o em Realidade Virtual e Aumentada},
pages = {55--81},
title = {{T\'{e}cnicas de Intera\c{c}\~{a}o de RV e RA na Medicina}},
year = {2010}
}
@article{Makris2002,
author = {Makris, D and Ellis, T},
journal = {Image and Vision Computing},
keywords = {trajectory extraction},
mendeley-tags = {trajectory extraction},
title = {{Path detection in video surveillance}},
url = {http://www.sciencedirect.com/science/article/pii/S0262885602000987},
year = {2002}
}
@article{Criminisi2000,
abstract = {The previous chapter has investigated how measurements can be taken on planar surfaces from uncalibrated images. However, the world is not just one big plane (as Cristoforo Colombo discovered five centuries ago); it is a complex three-dimensional structure. Therefore, a more general analysis of the threedimensional scene is required; this is achieved in this chapter. In particular, this chapter describes how aspects of the affine three-dimensional geometry of a scene may be measured from a single perspective image (see also [25, 26, 28]). The techniques described still concentrate on scenes containing planes and parallel lines, although the methods are not so restricted. The algorithms developed here extend and generalize previous results on single-view metrology [59, 68, 96, 100].},
author = {Criminisi, A and Reid, I and Zisserman, A},
doi = {10.1007/978-0-85729-327-5\_5},
file = {:home/acmt/Dropbox/Documentos/Mendeley/International Journal of Computer Vision/2000/Criminisi, Reid, Zisserman/Criminisi, Reid, Zisserman - 2000 - Single view metrology.pdf:pdf},
journal = {International Journal of Computer Vision},
number = {2},
pages = {69--105},
title = {{Single view metrology}},
url = {http://link.springer.com/article/10.1023/A:1026598000963},
volume = {40},
year = {2000}
}
@article{Roedl2013,
address = {New York, New York, USA},
author = {Roedl, David J. and Stolterman, Erik},
doi = {10.1145/2470654.2466257},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1951},
publisher = {ACM Press},
title = {{Design research at CHI and its applicability to design practice}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466257},
year = {2013}
}
@article{Humphrey2010,
author = {Humphrey, K and Underwood, G},
journal = {Journal of Vision},
keywords = {eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
title = {{The potency of people in pictures: Evidence from sequences of eye fixations}},
url = {http://w.journalofvision.org/content/10/10/19.short},
year = {2010}
}
@inproceedings{Matsuda2011,
abstract = {Few previous eye-tracking studies incorporated the psychological notion of chunking, a meaningful cognitive unit of information. In the present work, we constructed fixation sequence lists nesting chunks, using isolated saccades as a delimiter. Chunks were extracted from the time-stamped records of the fixation sequences that were coded according to the 5x5 segments imposed on the display. The overwhelming majority chunks consisted of one or two fixations. Most within-chunk distances were zero or one, while the between-chunk distance was relatively dispersed with the modal distance at one, followed by zero. There was good agreement between the rankings of between- and within-chunk loops among the primary and secondary segments on the total number of loops. We found some indications about the layout effect, possible attributable to the presence of sub-area on the right most segments.},
annote = {- cited by: 0
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Matsuda, N and Takeuchi, H},
booktitle = {Proceedings of IADIS-IHCI},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of IADIS-IHCI/2011/Matsuda, Takeuchi/Matsuda, Takeuchi - 2011 - Cognitive Chunks Extracted From Eye-Tracking Records of Web Page Readers, Using Isolated Saccades as a Delimi.pdf:pdf},
keywords = {chunking,eye-tracking,fixations,gaze analysis,loops,saccades},
mendeley-tags = {gaze analysis},
pages = {169--176},
title = {{Cognitive Chunks Extracted From Eye-Tracking Records of Web Page Readers, Using Isolated Saccades as a Delimiter}},
url = {http://infoshako.sk.tsukuba.ac.jp/~mazda/dPapers/IHCI11.pdf},
year = {2011}
}
@inproceedings{Latif2004,
abstract = {The paper will cover the issues of Collective Behavior in complex and critical event in Virtual Environment and its Application by Visualizing Space and Information. This is related to the on-going research results concerning development of the crowd simulation for interactive virtual environments. The simulation aims to reproduce realistic scenarios involving large number of the virtual human agents. We define interactive VE as an architecture of multi-agent system allowing behaviors of the agents to interact among them, with the virtual environment as well as with the real human participants. The first behavior is known as Collective behavior. One of collective behavior to be described in this paper is maximum dispersion for the group of three agents. There are some complexities in identifying the procedure for maximum dispersion behavior among three agents. For experimenting with the determined procedures, the path planning of crowd dispersion in the building environment at the time of emergency situation is applied. With this complex and critical environment an experiment is carried out and the result of simulating maximum dispersion behavior of agents is, discussed.},
address = {New York, New York, USA},
author = {Latif, Muhammad Shafie Abdul and Widyarto, Setyawan},
booktitle = {Proceedings of the 2004 ACM SIGGRAPH international conference on Virtual Reality continuum and its applications in industry - VRCAI '04},
doi = {10.1145/1044588.1044647},
isbn = {1581138849},
keywords = {collective behavior,multi-agent coordination,the A* algorithm},
pages = {278},
publisher = {ACM Press},
title = {{The crowd simulation for interactive virtual environments}},
url = {http://portal.acm.org/citation.cfm?doid=1044588.1044647},
year = {2004}
}
@article{Cassidy2013,
address = {New York, New York, USA},
author = {Cassidy, Brendan and Antani, Dipti Saurabh and Read, Janet C C.},
doi = {10.1145/2470654.2481315},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13/2013/Cassidy, Antani, Read/Cassidy, Antani, Read - 2013 - Using an open card sort with children to categorize games in a mobile phone application store.pdf:pdf},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
number = {2287},
pages = {2287},
publisher = {ACM Press},
title = {{Using an open card sort with children to categorize games in a mobile phone application store}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481315},
year = {2013}
}
@inproceedings{Klein2007,
abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
address = {Oxford},
annote = {        From Duplicate 1 (                   Parallel Tracking and Mapping for Small AR Workspaces                 - Klein, Georg; Murray, David )
                
        From Duplicate 1 (                           Parallel Tracking and Mapping for Small AR Workspaces                         - Klein, Georg; Murray, David )
And  Duplicate 2 (                           Parallel Tracking and Mapping for Small AR Workspaces                         - Klein, Georg; Murray, David )
                
        
        
        
        
      },
author = {Klein, Georg and Murrayt, David and Murray, David},
booktitle = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2007.4538852},
isbn = {978-1-4244-1749-0},
keywords = {Algorithm design and analysis,Cameras,Concurrent computing,Handheld computers,Layout,Robot vision systems,Robustness,Simultaneous localization and mapping,Tracking,Yarn},
month = nov,
pages = {1--10},
publisher = {Ieee},
title = {{Parallel Tracking and Mapping for Small AR Workspaces}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538852},
year = {2007}
}
@article{Conforto2011,
abstract = {A revis\~{a}o bibliogr\'{a}fica sistem\'{a}tica \'{e} um m\'{e}todo cient\'{\i}fico para busca e an\'{a}lise de artigos de uma determinada \'{a}rea da ci\^{e}ncia. \'{E} amplamente utilizada em pesquisas na medicina, psicologia e ci\^{e}ncias sociais, onde h\'{a} grandes massas de dados e fontes de informa\c{c}\~{o}es. Pesquisas na \'{a}rea de gest\~{a}o de opera\c{c}\~{o}es tamb\'{e}m necessitam analisar crescentes quantidades de artigos e informa\c{c}\~{o}es. No entanto, t\'{e}cnicas de revis\~{a}o sistem\'{a}tica s\~{a}o pouco difundidas nessa \'{a}rea, em especial em desenvolvimento de produtos e gerenciamento de projetos. O objetivo desse artigo \'{e} apresentar um roteiro para a condu\c{c}\~{a}o de revis\~{a}o bibliogr\'{a}fica sistem\'{a}tica (RBS) na \'{a}rea de gest\~{a}o de opera\c{c}\~{o}es com foco em pesquisas nos temas “desenvolvimento de produtos” e “gerenciamento de projetos”. O roteiro foi intitulado RBS Roadmap e foi criado a partir das melhores pr\'{a}ticas preconizadas em \'{a}reas pioneiras nesse tipo revis\~{a}o, combinada com uma pesquisa-a\c{c}\~{a}o em tr\^{e}s pesquisas na \'{a}rea de gest\~{a}o de opera\c{c}\~{o}es. A principal contribui\c{c}\~{a}o para a teoria e pr\'{a}tica \'{e} a sistematiza\c{c}\~{a}o do procedimento para revis\~{a}o sistem\'{a}tica voltado especificamente para pesquisas na \'{a}rea de desenvolvimento de produtos e gerenciamento de projetos, que pode ser utilizado como refer\^{e}ncia para pesquisadores nessa \'{a}rea.},
author = {Conforto, Edivandro Carlos and Amaral, Daniel Capaldo and da Silva, S\'{e}rgio Luis},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Congresso Brasileiro de Gest\~{a}o de Desenvolvimento de Produtos/2011/Conforto, Amaral, Silva/Conforto, Amaral, Silva - 2011 - Roteiro para revis\~{a}o bibliogr\'{a}fica sistem\'{a}tica aplica\c{c}\~{a}o no desenvolvimento de produtos e gerencia.pdf:pdf},
journal = {Congresso Brasileiro de Gest\~{a}o de Desenvolvimento de Produtos},
keywords = {Desenvolvimento de Produtos,Gerenciamento de Projetos.,RBS,RBS Roadmap,Roteiro para Revis\~{a}o Bibliogr\'{a}fica Sistem\'{a}tica},
mendeley-tags = {RBS},
number = {1998},
pages = {1--12},
publisher = {UFRGS},
title = {{Roteiro para revis\~{a}o bibliogr\'{a}fica sistem\'{a}tica: aplica\c{c}\~{a}o no desenvolvimento de produtos e gerenciamento de projetos}},
url = {http://vision.ime.usp.br/~acmt/conforto.pdf},
volume = {8},
year = {2011}
}
@book{Green2010,
address = {Burlington, MA, USA},
edition = {2},
editor = {Green, Todd},
isbn = {9780123750792},
keywords = {Physical Based Rendering,Ray Tracing},
mendeley-tags = {Physical Based Rendering,Ray Tracing},
pages = {1200},
publisher = {Morgan Kaufmann},
title = {{Physically Based Rendering: From Theory to Implementation}},
year = {2010}
}
@book{van2007eye,
author = {van Gompel, R P G and Gompel, RPG Van},
isbn = {9780080474915},
publisher = {Elsevier Science},
series = {Educational psychology},
title = {{Eye Movements: A Window on Mind and Brain}},
url = {http://books.google.com.br/books?hl=en\&lr=\&id=d00Ie8ftEnwC\&oi=fnd\&pg=PP2\&dq=Eye+Movements:+A+Window+on+Mind+and+Brain\&ots=JxW6BUN6-F\&sig=rOafx1OhLALBp4jM8FLqOAWJb6U http://books.google.com.br/books?id=d00Ie8ftEnwC},
year = {2007}
}
@inproceedings{Pierce2013,
abstract = {This paper offers new theoretical and design insights into nteractive technology. By initially considering electric technology broadly, our work informs how HCI approaches a range of specific interactive or digital things and materials. Theoretically, we contribute a rigorous analysis of electric technology using the experiential lens of phenomenology. A major result is to characterize electric technology by three forms of materiality: the electric object, its electric materiality, and electric power. In terms of design, we present and analyze novel interactive form prototypes. Our theoretical contributions offer new insight into design artifacts, just as our novel design artifacts help reveal new theoretical insight.},
address = {New York, New York, USA},
author = {Pierce, James and Paulos, Eric},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470672},
isbn = {9781450318990},
pages = {119--128},
publisher = {ACM Press},
title = {{Electric materialities and interactive technology}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470672},
year = {2013}
}
@inproceedings{Olsen2012a,
abstract = {Selecting values for fixation filters is a difficult task as not only the specifics of the selected filter algorithm has to be taken into account, but also what it is going to be used for and by whom. In this paper the selection and testing process of values for an I-VT fixation filter algorithm implementation is described.},
address = {New York, New York, USA},
annote = {- cited by: 2
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Olsen, Anneli and Matos, Ricardo},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12},
doi = {10.1145/2168556.2168625},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12/2012/Olsen, Matos/Olsen, Matos - 2012 - Identifying parameter values for an I-VT fixation filter suitable for handling data sampled with various sampling.pdf:pdf},
isbn = {9781450312219},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {317},
publisher = {ACM Press},
title = {{Identifying parameter values for an I-VT fixation filter suitable for handling data sampled with various sampling frequencies}},
url = {http://dl.acm.org/citation.cfm?id=2168625 http://dl.acm.org/citation.cfm?doid=2168556.2168625},
year = {2012}
}
@inproceedings{Heminghous2006,
abstract = {To evaluate differences across viewers' visual attentional patterns, scanpath comparison has recently gained popularity in eye tracking studies, supplementing traditional objective (performance) and subjective measures (e.g., heat maps or [retrospective] talk-aloud). We introduce iComp, an open-source visualization tool that implements quantitative scanpath comparison in loci and sequence. iComp can be used to objectively compare the attentional qualities of synthetic images.},
address = {New York, New York, USA},
author = {Heminghous, John and Duchowski, Andrew T.},
booktitle = {ACM SIGGRAPH 2006 Research posters on - SIGGRAPH '06},
doi = {10.1145/1179622.1179836},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM SIGGRAPH 2006 Research posters on - SIGGRAPH '06/2006/Heminghous, Duchowski/Heminghous, Duchowski - 2006 - iComp a tool for scanpath visualization and comparison.pdf:pdf},
isbn = {1595933646},
keywords = {eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
pages = {186},
publisher = {ACM Press},
title = {{iComp: a tool for scanpath visualization and comparison}},
url = {http://dl.acm.org/citation.cfm?id=1179836 http://portal.acm.org/citation.cfm?doid=1179622.1179836},
year = {2006}
}
@article{Juhola1988,
abstract = {A method based on a recursive digital filter is presented that recognizes minima and maxima of nystagmus. The method does not require user assistance. However, it takes into account the type and changes of input by adapting itself to these. The method has been tested with nystagmus data. If the algorithm is slightly modified, it can detect other eye movements.},
author = {Juhola, M},
doi = {10.1109/10.1398},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE transactions on bio-medical engineering/1988/Juhola/Juhola - 1988 - Detection of nystagmus eye movements using a recursive digital filter.pdf:pdf},
issn = {0018-9294},
journal = {IEEE transactions on bio-medical engineering},
keywords = {Algorithms,Computer-Assisted,Electronystagmography,Electronystagmography: methods,Humans,Signal Processing,gaze analysis,nystagmus},
mendeley-tags = {gaze analysis,nystagmus},
month = may,
number = {5},
pages = {389--95},
pmid = {3397089},
title = {{Detection of nystagmus eye movements using a recursive digital filter.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1398 http://www.ncbi.nlm.nih.gov/pubmed/3397089},
volume = {35},
year = {1988}
}
@inproceedings{Audet2009,
abstract = {Projector-camera systems drive applications in many fields such as measurement and spatial augmented reality. When needed, we can find their internal and external parameters via geometric calibration. For this process, we have to use both a printed pattern and a projector pattern, but they can easily interfere with each other. Current methods compensate by decoupling their calibrations or by leveraging structured light and color channels, but the required manipulations are not user-friendly. Therefore, we cannot expect normal users to execute the procedure, which can also become a burden for researchers. Although not always required, knowledge of the geometric parameters can often facilitate development of new systems. To make the calibration process easier, we propose a method that uses fiducial markers, from which we can easily derive a prewarp that, once applied to the projector calibration pattern, prevents its interference. Using our method, we confirmed that users can easily calibrate a projector-camera system in less than one minute, which we consider to be user-friendly, while still achieving typical subpixel accuracy.},
author = {Audet, S and Okutomi, M},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2009.5204319},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops/2009/Audet, Okutomi/Audet, Okutomi - 2009 - A user-friendly method to geometrically calibrate projector-camera systems.pdf:pdf},
isbn = {978-1-4244-3994-2},
keywords = {anamorphism},
mendeley-tags = {anamorphism},
month = jun,
pages = {47--54},
publisher = {IEEE},
title = {{A user-friendly method to geometrically calibrate projector-camera systems}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5204319 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5204319},
year = {2009}
}
@article{Deutscher2002,
abstract = {We present a completely automatic method for obtaining the approximate calibration of a camera (alignment to a world frame and focal length) from a single image of an unknown scene, provided only that the scene satisfies a Manhattan world assumption. This assumption states that the imaged scene contains three orthogonal, dominant directions, and is often satisfied by outdoor or indoor views of man-made structures and environments. The proposed method combines the calibration likelihood introduced in [5] with a stochastic search algorithm to obtain a MAP estimate of the camera’s focal length and alignment. Results on real images of indoor scenes are presented. The calibrations obtained are less accurate than those from standard methods employing a calibration pattern or multiple images. However, the outputs are certainly good enough for common vision tasks such as tracking. Moreover, the results are obtained without any user intervention, from a single image, and without use of a calibration pattern.},
author = {Deutscher, J and Isard, M and MacCormick, J},
doi = {10.1007/3-540-47979-1\_12},
journal = {Computer Vision — ECCV 2002},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {175--188},
title = {{Automatic camera calibration from a single manhattan image}},
url = {http://link.springer.com/content/pdf/10.1007/3-540-47979-1\_12.pdf},
volume = {2353},
year = {2002}
}
@article{Drettakis2007,
abstract = {In this paper we present a user-centered design approach to the development of a Virtual Environment (VE), by utilizing an iterative, user-informed process throughout the entire design and development cycle. A preliminary survey was first undertaken with end users, that is, architects, chief engineers, and decision makers of a real-world architectural and urban planning project, followed by a study of the traditional workflow employed. We then determined the elements required to make the VE useful in the real-world setting, choosing appropriate graphical and auditory techniques to develop audiovisual VEs with a high level of realism. Our user-centered design approach guided the development of an appropriate interface and an evaluation methodology to test the overall usability of the system. The VE was evaluated both in the laboratory and, most importantly, in the users' natural work environments. In this study we present the choices we made as part of the design and evaluation methodologies employed, which successfully combined research goals with those of a real-world project. Among other results, this evaluation suggests that involving users and designers from the beginning improves the effectiveness of the VE in the context of the real world urban planning project. Furthermore, it demonstrates that appropriate levels of realism, in particular spatialized 3D sound, high-detail vegetation, and shadows, as well as the presence of rendered crowds, are significant for the design process and for communicating about designs; they enable better appreciation of overall ambience of the VE, perception of space and physical objects, as well as the sense of scale. We believe this study is of interest to VE researchers, designers, and practitioners, as well as professionals interested in using VR in their workplace.},
author = {Drettakis, George and Roussou, Maria and Reche, Alex and Tsingos, Nicolas},
doi = {10.1162/pres.16.3.318},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Presence Teleoperators and Virtual Environments/2007/Drettakis et al/Drettakis et al. - 2007 - Design and Evaluation of a Real-World Virtual Environment for Architecture and Urban Planning.pdf:pdf},
issn = {1054-7460},
journal = {Presence: Teleoperators and Virtual Environments},
keywords = {crowd simulation},
mendeley-tags = {crowd simulation},
month = jun,
number = {3},
pages = {318--332},
title = {{Design and Evaluation of a Real-World Virtual Environment for Architecture and Urban Planning}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/pres.16.3.318},
volume = {16},
year = {2007}
}
@inproceedings{Danisman2010,
abstract = {This paper presents an automatic drowsy driver monitoring and accident prevention system that is based on monitoring the changes in the eye blink duration. Our proposed method detects visual changes in eye locations using the proposed horizontal symmetry feature of the eyes. Our new method detects eye blinks via a standard webcam in real-time at 110fps for a 320×240 resolution. Experimental results in the JZU eye-blink database showed that the proposed system detects eye blinks with a 94\% accuracy with a 1\% false positive rate.},
author = {Danisman, Taner and Bilasco, Ian M and Djeraba, Chabane and Ihaddadene, Nacim},
booktitle = {2010 International Conference on Machine and Web Intelligence},
doi = {10.1109/ICMWI.2010.5648121},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2010 International Conference on Machine and Web Intelligence/2010/Danisman et al/Danisman et al. - 2010 - Drowsy driver detection system using eye blink patterns.pdf:pdf},
isbn = {978-1-4244-8608-3},
keywords = {blink,gaze analysis},
mendeley-tags = {blink,gaze analysis},
month = oct,
pages = {230--233},
publisher = {IEEE},
title = {{Drowsy driver detection system using eye blink patterns}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5648121 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5648121},
year = {2010}
}
@inproceedings{Son2012,
abstract = {This paper presents a mobile augmented reality system using environment illumination information. After obtaining the location and intensity of the surrounding lights, we generate AR images on a mobile phone reflecting the environment light conditions.},
address = {Las Vegas, NV},
author = {Son, Wonyoung and Nam, Bodam and Kim, Taehyub and Hong, Hyunki},
booktitle = {2012 IEEE International Conference on Consumer Electronics (ICCE)},
doi = {10.1109/ICCE.2012.6161985},
isbn = {978-1-4577-0231-0},
month = jan,
pages = {588--589},
publisher = {IEEE},
title = {{Using environment illumination information in mobile Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6161985},
year = {2012}
}
@article{Xing2012,
abstract = {In augmented reality, consistent illumination is an important aspect of integrating a virtual object into a video of a real scene. In this paper, we propose a novel image-based framework to estimate online the dynamically changing illumination parameters of outdoor video sequences captured by a fixed or pure rotation camera. Unlike previous approaches, which either require knowledge of the scene geometry or significant storage to preserve time- and view-dependent basis images or statistical parameters, our approach requires a very simple interaction at the initialization stage: a few brushes are used to select areas with a specified normal surface, and these areas are used to calculate the sunlight parameters. Additional areas with the same normal parameters will be detected automatically, and an optimization procedure is applied to ensure the robustness and precision of our estimation. Our new approach adapts to videos of the same scene with varied sun positions and camera panning. The experimental results demonstrate the effectiveness and flexibility of our proposed approach.},
author = {Xing, Guanyu and Liu, Yanli and Qin, Xueying and Peng, Qunsheng},
doi = {10.1016/j.cag.2012.07.005},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = nov,
number = {7},
pages = {857--865},
title = {{A practical approach for real-time illumination estimation of outdoor videos}},
url = {http://www.sciencedirect.com/science/article/pii/S0097849312001343 http://linkinghub.elsevier.com/retrieve/pii/S0097849312001343},
volume = {36},
year = {2012}
}
@inproceedings{Ashdown2004,
abstract = {We present a novel multi-planar display system based on an uncalibrated projector-camera pair. Our system exploits the juxtaposition of planar surfaces in a room to create ad-hoc visualization and display capabilities. In an office setting, for example, a desk pushed against a wall provides two perpendicular surfaces that can simultaneously display elevation and plan views of an architectural model. In contrast to previous room-level projector-camera systems, our method is based on a flexible calibration procedure that requires a minimum amount of information for the geometry of the multi-planar surface scenario. A number of display configurations can be created on any available planar surfaces using a single commodity projector and camera. The key to our calibration approach is an efficient technique for simultaneously localizing multiple planes and a robust planar metric rectification method, which can tolerate a restricted camera field-of-view and requires no special calibration objects. We demonstrate the robustness of our calibration method using real and synthetic images and present several applications of our display system.},
author = {Ashdown, M and Flagg, M and Sukthankar, R. and Rehg, J.M.},
booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
doi = {10.1109/CVPR.2004.1315159},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004/2004/Ashdown et al/Ashdown et al. - 2004 - A flexible projector-camera system for multi-planar displays.pdf:pdf},
isbn = {0-7695-2158-4},
keywords = {anamorphism},
mendeley-tags = {anamorphism},
pages = {165--172},
publisher = {IEEE},
title = {{A flexible projector-camera system for multi-planar displays}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1315159 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1315159},
volume = {2},
year = {2004}
}
@inproceedings{Rabaud2006,
abstract = {In its full generality, motion analysis of crowded objects necessitates recognition and segmentation of each moving entity. The difficulty of these tasks increases considerably with occlusions and therefore with crowding. When the objects are constrained to be of the same kind, however, partitioning of densely crowded semi-rigid objects can be accomplished by means of clustering tracked feature points. We base our approach on a highly parallelized version of the KLT tracker in order to process the video into a set of feature trajectories. While such a set of trajectories provides a substrate for motion analysis, their unequal lengths and fragmented nature present difficulties for subsequent processing. To address this, we propose a simple means of spatially and temporally conditioning the trajectories. Given this representation, we integrate it with a learned object descriptor to achieve a segmentation of the constituent motions. We present experimental results for the problem of estimating the number of moving objects in a dense crowd as a function of time.},
author = {Rabaud, Vincent and Belongie, Serge},
booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 (CVPR'06)},
doi = {10.1109/CVPR.2006.92},
isbn = {0-7695-2597-0},
pages = {705--711},
publisher = {IEEE},
title = {{Counting Crowded Moving Objects}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1640823},
volume = {1},
year = {2006}
}
@inproceedings{beyond,
address = {New York, New York, USA},
author = {Lee, Jinha and Ishii, Hiroshi},
booktitle = {Proceedings of the 28th of the international conference extended abstracts on Human factors in computing systems - CHI EA '10},
doi = {10.1145/1753846.1754081},
isbn = {9781605589305},
keywords = {SBGames},
mendeley-tags = {SBGames},
organization = {ACM},
pages = {3931--3936},
publisher = {ACM Press},
title = {{Beyond: collapsible tools and gestures for computational design}},
url = {http://dl.acm.org/citation.cfm?id=1754081 http://portal.acm.org/citation.cfm?doid=1753846.1754081},
year = {2010}
}
@article{Gould1985,
abstract = {This article is both theoretical and empirical. Theoretically, it describes three principles of system design which we believe must be followed to produce a useful and easy to use computer system. These principles are: early and continual focus on users; empirical measurement of usage; and iterative design whereby the system (simulated, prototype, and real) is modified, tested, modified again, tested again, and the cycle is repeated again and again. This approach is contrasted to other principled design approaches, for example, get it right the first time, reliance on design guidelines. Empirically, the article presents data which show that our design principles are not always intuitive to designers; identifies the arguments which designers often offer for not using these principles—and answers them; and provides an example in which our principles have been used successfully.},
author = {Gould, John D and Lewis, Clayton},
doi = {10.1145/3166.3170},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Communications of the ACM/1985/Gould, Lewis/Gould, Lewis - 1985 - Designing for usability key principles and what designers think.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = mar,
number = {3},
pages = {300--311},
title = {{Designing for usability: key principles and what designers think}},
url = {http://dl.acm.org/citation.cfm?id=3170 http://portal.acm.org/citation.cfm?doid=3166.3170},
volume = {28},
year = {1985}
}
@article{Tavakoli2013,
abstract = {In this article, a novel technique for fixation prediction and saccade generation will be introduced. The proposed model simulates saccadic eye movement to incorporate the underlying eye movement mechanism into saliency estimation. To this end, a simple salience measure is introduced. Afterwards, we derive a system model for saccade generation and apply it in a stochastic filtering framework. The proposed model will dynamically make a saccade toward the next predicted fixation and produces saliency maps. Evaluation of the proposed model is carried out in terms of saccade generation performance and saliency estimation. Saccade generation evaluation reveals that the proposed model outperforms inhibition of return. Also, experiments signify integration of eye movement mechanism into saliency estimation boosts the results. Finally, comparison with several saliency models shows the proposed model performs aptly.},
author = {{Rezazadegan Tavakoli}, Hamed and Rahtu, Esa and Heikkil\"{a}, Janne},
doi = {10.1016/j.imavis.2013.06.006},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
month = sep,
number = {9},
pages = {686--693},
title = {{Stochastic bottom–up fixation prediction and saccade generation}},
url = {http://www.sciencedirect.com/science/article/pii/S0262885613001017 http://linkinghub.elsevier.com/retrieve/pii/S0262885613001017},
volume = {31},
year = {2013}
}
@inproceedings{Blythe2013,
abstract = {The paper reflects on three approaches to the dissemination and display of digital art. “s[edition]” is a novel, web-based service that offers limited editions of “digital prints”. Analysis of user comments suggests that the metaphor of a “limited digital edition” raises issues and to some extent is resisted. The second approach is the Flickr Brushes Gallery, where digital painters post images and comment on one another’s work. Analysis of comment boards indicates that the shared art and comments are a form of gift exchange. Finally, the paper discusses a field study in which artists exhibited their work as it develops over time in digital frames and also in an immersive digital projection room. Analysis of field notes and interviews indicate that the digital frame approach was unsuccessful because of aesthetic and environmental concerns. The immersive projection suggested that more experiential approaches may be more interesting. It is argued that there is an inherent resistance in digital media to previous models of art commoditization. None of the approaches discussed here resolve the dilemma but rather indicate the scope and complexity of the issues.},
address = {New York, New York, USA},
author = {Blythe, Mark and Briggs, Jo and Hook, Jonathan and Wright, Peter and Olivier, Patrick},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470674},
isbn = {9781450318990},
pages = {139--148},
publisher = {ACM Press},
title = {{Unlimited editions: three approaches to the dissemination and display of digital art}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470674},
year = {2013}
}
@article{Wilczkowiak2005,
abstract = {This paper concerns the incorporation of geometric information in camera calibration and 3D modeling. Using geometric constraints enables more stable results and allows us to perform tasks with fewer images. Our approach is motivated and developed within a framework of semi-automatic 3D modeling, where the user defines geometric primitives and constraints between them. It is based on the observation that constraints, such as coplanarity, parallelism, or orthogonality, are often embedded intuitively in parallelepipeds. Moreover, parallelepipeds are easy to delineate by a user and are well adapted to model the main structure of, e.g., architectural scenes. In this paper, first a duality that exists between the shape parameters of a parallelepiped and the intrinsic parameters of a camera is described. Then, a factorization-based algorithm exploiting this relation is developed. Using images of parallelepipeds, it allows us to simultaneously calibrate cameras, recover shapes of parallelepipeds, and estimate the relative pose of all entities. Besides geometric constraints expressed via parallelepipeds, our approach simultaneously takes into account the usual self-calibration constraints on cameras. The proposed algorithm is completed by a study of the singular cases of the calibration method. A complete method for the reconstruction of scene primitives that are not modeled by parallelepipeds is also briefly described. The proposed methods are validated by various experiments with real and simulated data, for single-view as well as multiview cases.},
author = {Wilczkowiak, Marta and Sturm, Peter and Boyer, Edmond},
doi = {10.1109/TPAMI.2005.40},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Pattern Analysis and Machine Intelligence/2005/Wilczkowiak, Sturm, Boyer/Wilczkowiak, Sturm, Boyer - 2005 - Using geometric constraints through parallelepipeds for calibration and 3D modeling.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Calibration,Calibration: standards,Computer-Assisted,Computer-Assisted: methods,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Photogrammetry,Photogrammetry: instrumentation,Photogrammetry: methods,Photogrammetry: standards,Reproducibility of Results,Sensitivity and Specificity,Three-Dimensional,Three-Dimensional: instrumentation,Three-Dimensional: methods,Three-Dimensional: standards,perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = feb,
number = {2},
pages = {194--207},
pmid = {15688557},
title = {{Using geometric constraints through parallelepipeds for calibration and 3D modeling.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1374866 http://www.ncbi.nlm.nih.gov/pubmed/15688557},
volume = {27},
year = {2005}
}
@article{Juhola,
author = {Juhola, M and Aalto, H and Joutsijoki, H and Hirvonen, TP},
file = {:home/acmt/Dropbox/Documentos/Mendeley/downloads.hindawi.com/Unknown/Juhola et al/Juhola et al. - Unknown - The Classification of Valid and Invalid Beats of Three-Dimensional Nystagmus Eye Movement Signals Using Machin.pdf:pdf},
journal = {downloads.hindawi.com},
keywords = {gaze analysis,nystagmus},
mendeley-tags = {gaze analysis,nystagmus},
title = {{The Classification of Valid and Invalid Beats of Three-Dimensional Nystagmus Eye Movement Signals Using Machine Learning Methods}},
url = {http://downloads.hindawi.com/journals/aans/aip/972412.pdf}
}
@inproceedings{Krassanakis2013,
abstract = {The features processed in a primary stage of vision can be of guid- ance to visual search. The list of preattentive features indicated by psychologi- cal research bare many similarities to the fundamental cartographic design tools (visual and dynamic variables). This paper presents the performance of two ex- perimental studies, which examine the influence of preattentive vision in map reading process. The experimental studies involve both “top-down” and “bot- tom-up” procedures. The methodology of eye tracking is used in the experi- ments.},
address = {Scarborough, UK.},
annote = {- cited by: 2
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Krassanakis, V},
booktitle = {Proceedings of the 1st International Workshop on Eye Tracking for Spatial Research},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 1st International Workshop on Eye Tracking for Spatial Research/2013/Krassanakis/Krassanakis - 2013 - Exploring the map reading process with eye movement analysis.pdf:pdf},
keywords = {Preattentive vision,cartographic experimentation,dynamic variables,eye move-,gaze analysis,ment analysis,visual variables},
mendeley-tags = {gaze analysis},
pages = {2--7},
title = {{Exploring the map reading process with eye movement analysis}},
url = {http://spatialeyetracking.org/wp-content/uploads/2013/10/et4s\_2013\_paper1.pdf},
year = {2013}
}
@article{Matsumoto2000,
author = {Matsumoto, Y},
file = {::},
journal = {Intelligent Robots and  \ldots},
title = {{Behavior recognition based on head pose and gaze direction measurement}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=895285},
year = {2000}
}
@article{Saxena2008,
abstract = {We consider the task of 3-d depth estimation from a single still image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured indoor and outdoor environments which include forests, sidewalks, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the value of the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a hierarchical, multiscale Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models the depths and the relation between depths at different points in the image. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps. We further propose a model that incorporates both monocular cues and stereo (triangulation) cues, to obtain significantly more accurate depth estimates than is possible using either monocular or stereo cues alone.},
author = {Saxena, Ashutosh and Chung, Sung H. and Ng, Andrew Y.},
doi = {10.1007/s11263-007-0071-y},
file = {:home/acmt/Dropbox/Documentos/Mendeley/International Journal of Computer Vision/2007/Saxena, Chung, Ng/Saxena, Chung, Ng - 2007 - 3-D Depth Reconstruction from a Single Still Image.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = aug,
number = {1},
pages = {53--69},
title = {{3-D Depth Reconstruction from a Single Still Image}},
url = {http://link.springer.com/article/10.1007/s11263-007-0071-y http://link.springer.com/10.1007/s11263-007-0071-y},
volume = {76},
year = {2007}
}
@inproceedings{Yu1999,
address = {New York, New York, USA},
author = {Yu, Yizhou and Debevec, Paul and Malik, Jitendra and Hawkins, Tim},
booktitle = {Proceedings of the 26th annual conference on Computer graphics and interactive techniques - SIGGRAPH '99},
doi = {10.1145/311535.311559},
isbn = {0201485605},
keywords = {acm,albedo maps,ance,berkeley,brdf models,cs,debevec,dering,edu,email,global illumination,image-based modeling and ren-,malik,org,radi-,radiosity,reflectance recovery,rendering,tsh,yizhouy,yyz},
pages = {215--224},
publisher = {ACM Press},
title = {{Inverse global illumination: recovering reflectance models of real scenes from photographs}},
url = {http://portal.acm.org/citation.cfm?doid=311535.311559},
year = {1999}
}
@article{Park2006a,
abstract = {For projector-based augmented reality systems, geometric correction is a crucial function. There have been many researches on the geometric correction in the literature. However, most of them focused only on static projection surfaces and could not give us a solution for dynamic surfaces (with varying geometry in time). In this paper, we aim at providing a simple and robust framework for projecting augmented reality images onto dynamic surfaces without image distortion. For this purpose, a new technique for embedding pattern images into the augmented reality images, which allows simultaneous display and correction, is proposed and its validity is shown in experimental results.},
author = {Park, H and Lee, MH and Seo, BK and Park, JI},
doi = {10.1007/11949534\_58},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Advances in Image and Video Technology/2006/Park et al/Park et al. - 2006 - Undistorted projection onto dynamic surface.pdf:pdf},
journal = {Advances in Image and Video Technology},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
pages = {582--590},
title = {{Undistorted projection onto dynamic surface}},
url = {http://link.springer.com/chapter/10.1007/11949534\_58},
volume = {4319},
year = {2006}
}
@inproceedings{Geiger2012,
abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera-to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experiments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions.},
author = {Geiger, Andreas and Moosmann, Frank and Car, Omer and Schuster, Bernhard},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224570},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 IEEE International Conference on Robotics and Automation/2012/Geiger et al/Geiger et al. - 2012 - Automatic camera and range sensor calibration using a single shot.pdf:pdf},
isbn = {978-1-4673-1405-3},
month = may,
pages = {3936--3943},
publisher = {IEEE},
title = {{Automatic camera and range sensor calibration using a single shot}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6224570\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=kinect+calibration},
year = {2012}
}
@misc{Raman2009,
author = {Raman, Assaf and Frydrych, Holger and Buck, Jim and Rogers, Dave and Furst, Mattan and Gat, Noam and Johnstone, Brian},
booktitle = {OGRE - Open Source Graphics Engine},
title = {{OGRE3D}},
url = {http://www.ogre3d.org/},
urldate = {04/10/2011},
year = {2009}
}
@article{Helbing2005,
abstract = {To test simulation models of pedestrian flows, we have performed experiments for corridors, bottleneck areas, and intersections. Our evaluations of video recordings show that the geometric boundary conditions are not only relevant for the capacity of the elements of pedestrian facilities, they also influence the time gap distribution of pedestrians, indicating the existence of self-organization phenomena. After calibration of suitable models, these findings can be used to improve design elements of pedestrian facilities and egress routes. It turns out that “obstacles” can stabilize flow patterns and make them more fluid. Moreover, intersecting flows can be optimized, utilizing the phenomenon of “stripe formation.” We also suggest increasing diameters of egress routes in stadia, theaters, and lecture halls to avoid long waiting times for people in the back, and shock waves due to impatience in cases of emergency evacuation. Moreover, zigzag-shaped geometries and columns can reduce the pressure in panicking crowds. The proposed design solutions are expected to increase the efficiency and safety of train stations, airport terminals, stadia, theaters, public buildings, and mass events in the future. As application examples we mention the evacuation of passenger ships and the simulation of pilgrim streams on the Jamarat bridge. Adaptive escape guidance systems, optimal way systems, and simulations of urban pedestrian flows are addressed as well.},
author = {Helbing, Dirk and Buzna, Lubos and Johansson, Anders and Werner, Torsten},
doi = {10.1287/trsc.1040.0108},
issn = {0041-1655},
journal = {Transportation Science},
keywords = {evacuation dynamics,granular flow,panic behavior,pedestrian crowd dynamics,self-organization},
month = feb,
number = {1},
pages = {1--24},
title = {{Self-Organized Pedestrian Crowd Dynamics: Experiments, Simulations, and Design Solutions}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/trsc.1040.0108},
volume = {39},
year = {2005}
}
@article{Shaw1995,
abstract = {The formal literature review within the broader genre of academic writing remains problematic, and despite vigorous interest in other aspects of the genre, largely avoided by ESP researchers. Because such reviews are considered highly personal and interpretative, our reluctance to intervene is not surprising. However, the difficulties students are faced with are at least partially linguistic, and therefore the language teacher would seem to have something to offer. Recent developments in schema theory seem to offer a way out of the impasse, and the chance to develop language teaching technologies appropriate to language enhancement programs offered to graduate students in various fields, most particularly engineering. This paper presents results from classroom-initiated research into the use of text-graphing with engineering masters degree students working on their thesis proposals.},
author = {Shaw, Jonathan},
file = {:home/acmt/Dropbox/Documentos/Mendeley/System/1995/Shaw/Shaw - 1995 - A schema approach to the formal literature review in engineering theses.pdf:pdf},
journal = {System},
keywords = {rbs},
mendeley-tags = {rbs},
number = {3},
pages = {325--335},
title = {{A schema approach to the formal literature review in engineering theses}},
url = {http://www.sciencedirect.com/science/article/pii/0346251X9500020K},
volume = {23},
year = {1995}
}
@inproceedings{Gu2011a,
abstract = {Most of existing crowd simulation algorithms focus on the moving trajectories of individual agents, while collective group formations are often roughly learned from video examples or manually specified via various hard constraints (e.g., pre-defined keyframes of exact agent distributions). In this paper, we present an intuitive yet efficient approach to generate arbitrary and precise group formations by sketching formation boundaries. Our approach can automatically compute the desired position of each agent in the target formation and generate the agent correspondences between keyframes. When high-level group formations need to be formed on-the-fly in a dynamic environment such as “switching to the circle formation at about one hundred meters ahead", our algorithm will coordinate and compute appropriate actions for each agent by seamlessly fusing local formation dynamics and global group locomotion. Through a number of experiments, we demonstrate that our approach is efficient and adaptive to variations of group scales (i.e., number of agents), group positions, and environment obstacles.},
author = {Gu, Qin and Deng, Zhigang},
booktitle = {Graphics Interface},
isbn = {978-1-4503-0693-5},
pages = {1--8},
title = {{Formation Sketching : An Approach to Stylize Groups in Crowd Simulation}},
year = {2011}
}
@article{Delphenich2005a,
abstract = {Some concepts of real and complex projective geometry are applied to the fundamental physical notions that relate to Minkowski space and the Lorentz group. In particular, it is shown that the transition from an infinite speed of propagation for light waves to a finite one entails the replacement of a hyperplane at infinity with a light cone and the replacement of an affine hyperplane - or rest space - with a proper time hyperboloid. The transition from the metric theory of electromagnetism to the pre-metric theory is discussed in the context of complex projective geometry, and ultimately it is proposed that the geometrical issues are more general than electromagnetism, namely, they pertain to the transition from point mechanics to wave mechanics.},
author = {Delphenich, David},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Philosophy of Science/2005/Delphenich/Delphenich - 2005 - Projective geometry and special relativity.pdf:pdf},
journal = {Philosophy of Science},
pages = {41},
title = {{Projective geometry and special relativity}},
url = {http://arxiv.org/abs/gr-qc/0512125},
volume = {46},
year = {2005}
}
@inproceedings{Tien2012,
abstract = {For gaze-based training in surgery to be meaningful, the similarity between a trainee's gaze and an expert's gaze during performance of surgical tasks must be assessed. As it is difficult to record two people's gaze simultaneously, we produced task videos made by experts, and measured the amount of overlap between the gaze path of the expert surgeon and third-party observers while watching the videos. For this investigation, we developed a new, simple method for displaying and summarizing the proportion of time during which two observers' points of gaze on a common stimulus were separated by no more than a specified visual angle. In a study of single-observer self-review and multiple-observer initial view of a laparoscopic training task, we predicted that self-review would produce the highest overlap. We found relatively low overlap between watchers and the task performer; even operators with detailed task knowledge produce low overlap when watching their own videos. Conversely, there was a high overlap among all watchers. Results indicate that it may be insufficient to improve trainees' eye-hand coordination by just watching a video. Gaze training will need to be integrated with other teaching methods to be effective.},
address = {New York, New York, USA},
author = {Tien, Geoffrey and Atkins, M. Stella and Zheng, Bin},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12},
doi = {10.1145/2168556.2168623},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12/2012/Tien, Atkins, Zheng/Tien, Atkins, Zheng - 2012 - Measuring gaze overlap on videos between multiple observers.pdf:pdf},
isbn = {9781450312219},
keywords = {eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
pages = {309},
publisher = {ACM Press},
title = {{Measuring gaze overlap on videos between multiple observers}},
url = {http://dl.acm.org/citation.cfm?id=2168623 http://dl.acm.org/citation.cfm?doid=2168556.2168623},
year = {2012}
}
@inproceedings{Machado2006,
abstract = {The goal of virtual reality applications to support medical procedures is the planning, the training or the assistance of medical procedures. Some features as reduction of costs, availability and safety can be related to realism and quality procedure assessment to provide several benefits of medical applications of virtual reality.},
address = {Bel\'{e}m-PA},
author = {Machado, Liliane Santos and Moraes, Ronei Marcos},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {358--365},
publisher = {SBC},
title = {{Realidade Virtual Aplicada \`{a} Medicina}},
year = {2006}
}
@article{Hong2004,
abstract = {In this paper, we provide a principled explanation of how knowledge in global 3-D structural invariants, typically captured by a group action on a symmetric structure, can dramatically facilitate the task of reconstructing a 3-D scene from one or more images. More importantly, since every symmetric structure admits a “canonical” coordinate frame with respect to which the group action can be naturally represented, the canonical pose between the viewer and this canonical frame can be recovered too, which explains why symmetric objects (e.g., buildings) provide us overwhelming clues to their orientation and position. We give the necessary and sufficient conditions in terms of the symmetry (group) admitted by a structure under which this pose can be uniquely determined. We also characterize, when such conditions are not satisfied, to what extent this pose can be recovered. We show how algorithms from conventional multiple-view geometry, after properly modified and extended, can be directly applied to perform such recovery, from all “hidden images” of one image of the symmetric structure. We also apply our results to a wide range of applications in computer vision and image processing such as camera self-calibration, image segmentation and global orientation, large baseline feature matching, image rendering and photo editing, as well as visual illusions (caused by symmetry if incorrectly assumed).},
author = {Hong, Wei and Yang, Allen Yang and Huang, Kun and Ma, Yi},
doi = {10.1023/B:VISI.0000036837.76476.10},
file = {:home/acmt/Dropbox/Documentos/Mendeley/International Journal of Computer Vision/2004/Hong et al/Hong et al. - 2004 - On Symmetry and Multiple-View Geometry Structure, Pose, and Calibration from a Single Image.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = dec,
number = {3},
pages = {241--265},
title = {{On Symmetry and Multiple-View Geometry: Structure, Pose, and Calibration from a Single Image}},
url = {http://link.springer.com/article/10.1023/B:VISI.0000036837.76476.10 http://link.springer.com/10.1023/B:VISI.0000036837.76476.10},
volume = {60},
year = {2004}
}
@techreport{Jiang2013,
author = {Jiang, ZS and Rezvankhah, S and Siddiqi, K},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2013/Jiang, Rezvankhah, Siddiqi/Jiang, Rezvankhah, Siddiqi - 2013 - Project Report Light Source Estimation using Kinect.pdf:pdf},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
pages = {13},
title = {{Project Report: Light Source Estimation using Kinect}},
url = {http://www.cim.mcgill.ca/~siddiqi/COMP-558-2012/sam-shayan.pdf},
year = {2013}
}
@article{Lopes2013,
address = {New York, New York, USA},
author = {Lopes, Pedro and Butzmann, Lars and Baudisch, Patrick},
doi = {10.1145/2459236.2459276},
isbn = {9781450319041},
journal = {Proceedings of the 4th Augmented Human International Conference on - AH '13},
keywords = {force feedback, mobile device, muscle Contraction,},
pages = {231--232},
publisher = {ACM Press},
title = {{Muscle-propelled force feedback}},
url = {http://dl.acm.org/citation.cfm?doid=2459236.2459276},
year = {2013}
}
@inproceedings{Sugimoto2007,
author = {Sugimoto, Maki and Kodama, Kazuki and Nakamura, Akihiro and Kojima, Minoru and Inami, Masahiko},
booktitle = {17th International Conference on Artificial Reality and Telexistence (ICAT 2007)},
doi = {10.1109/ICAT.2007.50},
file = {:home/acmt/Dropbox/Documentos/Mendeley/17th International Conference on Artificial Reality and Telexistence (ICAT 2007)/2007/Sugimoto et al/Sugimoto et al. - 2007 - A Display-Based Tracking System Display-Based Computing for Measurement Systems.pdf:pdf},
isbn = {0-7695-3056-7},
month = nov,
pages = {31--38},
publisher = {IEEE},
title = {{A Display-Based Tracking System: Display-Based Computing for Measurement Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4414613},
year = {2007}
}
@article{Nov2013,
address = {New York, New York, USA},
author = {Nov, Oded and Arazy, Ofer and L\'{o}pez, Claudia and Brusilovsky, Peter},
doi = {10.1145/2470654.2470707},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {361},
publisher = {ACM Press},
title = {{Exploring personality-targeted UI design in online social participation systems}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470707},
year = {2013}
}
@article{Xing2011a,
abstract = {Multiple object tracking (MOT) is a very challenging task yet of fundamental importance for many practical applications. In this paper, we focus on the problem of tracking multiple players in sports video which is even more difficult due to the abrupt movements of players and their complex interactions. To handle the difficulties in this problem, we present a new MOT algorithm which contributes both in the observation modeling level and in the tracking strategy level. For the observation modeling, we develop a progressive observation modeling process that is able to provide strong tracking observations and greatly facilitate the tracking task. For the tracking strategy, we propose a dual-mode two-way Bayesian inference approach which dynamically switches between an offline general model and an online dedicated model to deal with single isolated object tracking and multiple occluded object tracking integrally by forward filtering and backward smoothing. Extensive experiments on different kinds of sports videos, including football, basketball, as well as hockey, demonstrate the effectiveness and efficiency of the proposed method.},
author = {Xing, Junliang and Ai, Haizhou and Liu, Liwei and Lao, Shihong},
doi = {10.1109/TIP.2010.2102045},
issn = {1941-0042},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Bayes Theorem,Biometry,Biometry: methods,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Image Enhancement,Image Enhancement: methods,Image Interpretation,Models,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Sports,Statistical,Video Recording,Video Recording: methods,Whole Body Imaging,Whole Body Imaging: methods,basketball,tracking},
mendeley-tags = {basketball,tracking},
month = jun,
number = {6},
pages = {1652--67},
pmid = {21189238},
shorttitle = {Image Processing, IEEE Transactions on},
title = {{Multiple player tracking in sports video: a dual-mode two-way bayesian inference approach with progressive observation modeling.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21189238},
volume = {20},
year = {2011}
}
@article{Privitera2000,
author = {Privitera, CM and Stark, LW},
doi = {10.1109/34.877520},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
number = {9},
pages = {970--982},
title = {{Algorithms for defining visual regions-of-interest: comparison with eye fixations}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=877520 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=877520},
volume = {22},
year = {2000}
}
@article{Yoo2013,
address = {New York, New York, USA},
author = {Yoo, Daisy and Zimmerman, John and Hirsch, Tad},
doi = {10.1145/2470654.2470714},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {409},
publisher = {ACM Press},
title = {{Probing bus stop for insights on transit co-design}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470714},
year = {2013}
}
@article{Tan2006,
abstract = {The proposed method performs the determination of eye blink states by tracking iris and eyelids. Two novelties of this method are the simultaneous exploitation of intensity and edge information for detecting the eye state as well as the record of the patterns of eyelids before closing for tracking the reopened eyes. Experiments show the efficiency of the proposed method.},
author = {Tan, Huachun and Zhang, Yu-Jin},
doi = {10.1016/j.patrec.2005.10.005},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Pattern Recognition Letters/2006/Tan, Zhang/Tan, Zhang - 2006 - Detecting eye blink states by tracking iris and eyelids.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {blink,gaze analysis},
mendeley-tags = {blink,gaze analysis},
month = apr,
number = {6},
pages = {667--675},
title = {{Detecting eye blink states by tracking iris and eyelids}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865505002953 http://linkinghub.elsevier.com/retrieve/pii/S0167865505002953},
volume = {27},
year = {2006}
}
@inproceedings{Santella2004,
abstract = {Characterizing the location and extent of a viewer's interest, in terms of eye movement recordings, informs a range of investigations in image and scene viewing. We present an automatic data-driven method for accomplishing this, which clusters visual point-of-regard (POR) measurements into gazes and regions-of-interest using the mean shift procedure. Clusters produced using this method form a structured representation of viewer interest, and at the same time are replicable and not heavily influenced by noise or outliers. Thus, they are useful in answering fine-grained questions about where and how a viewer examined an image.},
address = {New York, New York, USA},
author = {Santella, Anthony and DeCarlo, Doug},
booktitle = {Proceedings of the Eye tracking research \& applications symposium on Eye tracking research \& applications - ETRA'2004},
doi = {10.1145/968363.968368},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the Eye tracking research \& applications symposium on Eye tracking research \& applications - ETRA'2004/2004/Santella, DeCarlo/Santella, DeCarlo - 2004 - Robust clustering of eye movement recordings for quantification of visual interest.pdf:pdf},
isbn = {1581138253},
keywords = {eye movement,gaze analysis},
mendeley-tags = {eye movement,gaze analysis},
pages = {27--34},
publisher = {ACM Press},
title = {{Robust clustering of eye movement recordings for quantification of visual interest}},
url = {http://dl.acm.org/citation.cfm?id=968368 http://portal.acm.org/citation.cfm?doid=968363.968368},
year = {2004}
}
@phdthesis{Larsson2010,
abstract = {A new method of evaluating eye movement classification algorithms using Precision and Recall is proposed. The method involves recording test subjects looking at known stimuli and then testing various algo- rithms’ ability to classify the eye movements that are anticipated. This method is then used to evaluate the performance of different off-line algorithms. The algorithms I-DT, I-VT and an HMM-based method were tested, as well as eye tracking company Tobii Technology’s algorithms Clear- View and Tobii Fixation Filter. An analysis tool, which is available freely to the public, was developed in order to facilitate the process of developing and evaluating classification algorithms. Precision and Recall gave a clear profile of how accurately different algorithms could identify fixations. The implementations of I-VT and ClearView are essentially the same, and so were the results. The HMM offered no improvements, but should not be dismissed completely. To- bii Fixation Filter performed well due to filtering of the data. Most significantly, I-DT performed better than I-VT for fixation identification, while the reverse was true for extracting accurate sac- cadic information.},
author = {Larsson, G},
booktitle = {nada.kth.se},
file = {:home/acmt/Dropbox/Documentos/Mendeley/nada.kth.se/2010/Larsson/Larsson - 2010 - Evaluation methodology of eye movement classification algorithms.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {56},
school = {Royal Institute of Technology},
title = {{Evaluation methodology of eye movement classification algorithms}},
url = {http://www.nada.kth.se/utbildning/grukth/exjobb/rapportlistor/2010/rapporter10/larsson\_gustav\_10064.pdf},
year = {2010}
}
@article{Fournier1993,
author = {Fournier, A and Gunawan, AS and Romanzin, C},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Graphics Interface/1993/Fournier, Gunawan, Romanzin/Fournier, Gunawan, Romanzin - 1993 - Common illumination between real and computer generated scenes.pdf:pdf},
journal = {Graphics Interface},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
title = {{Common illumination between real and computer generated scenes}},
url = {ftp://mx3.cs.ubc.ca/local/techreports/1992/TR-92-38.pdf},
year = {1993}
}
@inproceedings{Shic2008a,
abstract = {The analysis of eye-tracking data hinges on the ability of automated algorithms to separate rapid saccadic eye movements from stable eye fixations. However, though it has long been known that changing the parameters of fixationidentification algorithms can lead to very different qualitative impressions, less is known about how algorithmic parameters interact with quantitative eye-tracking measures. In this study we show that by manipulating aspects of fixation identification, we can completely reverse the patterns of observed results for mean fixation duration, a measure traditionally associated with cognitive load. However, by linearly mapping mean fixation duration over its parameter space, we obtain a new formulation which addresses many of the deficits of the standard analysis. We use our methods to analyze the gaze patterns of toddlers with autism spectrum disorder and control populations and discuss the observed differences in terms of the physical and cognitive ramifications of our methodology.},
annote = {- cited by: 9
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Shic, F and Chawarska, K and Scassellati, B},
booktitle = {30th Annual Meeting of the Cognitive Science Society},
doi = {10.1.1.145.8123},
file = {:home/acmt/Dropbox/Documentos/Mendeley/30th Annual Meeting of the Cognitive Science Society/2008/Shic, Chawarska, Scassellati/Shic, Chawarska, Scassellati - 2008 - The amorphous fixation measure revisited with applications to autism.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {1--6},
title = {{The amorphous fixation measure revisited: with applications to autism}},
url = {http://scazlab.yale.edu/sites/default/files/files/Shic-CogSci-2008.pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.145.8123},
year = {2008}
}
@article{Jansen2013,
address = {New York, New York, USA},
author = {Jansen, Yvonne and Dragicevic, Pierre and Fekete, Jean-Daniel},
doi = {10.1145/2470654.2481359},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2593},
publisher = {ACM Press},
title = {{Evaluating the efficiency of physical visualizations}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481359},
year = {2013}
}
@inproceedings{Holland2013,
address = {New York, New York, USA},
author = {Holland, Corey and Garza, Atenas and Kurtova, Elena and Cruz, Jose and Komogortsev, Oleg V},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13},
doi = {10.1145/2468356.2468409},
isbn = {9781450319522},
pages = {295},
publisher = {ACM Press},
title = {{Usability evaluation of eye tracking on an unmodified common tablet}},
url = {http://dl.acm.org/citation.cfm?doid=2468356.2468409},
year = {2013}
}
@article{Komogortsev2010a,
abstract = {In an effort towards standardization, this paper evaluates the performance of five eye movement classification algorithms in terms of their assessment of oculomotor fixation and saccadic behavior. The results indicate that performance of these five commonly used algorithms vary dramatically even in the case of a simple stimulus evoked task using a single, common threshold value. The important contributions of this paper are: 1) evaluation and comparison of performance of five algorithms to classify specific oculomotor behavior 2) introduction and comparison of new standardized scores to provide more reliable classification performance 3) logic for a reasonable threshold value selection for any eye movement classification algorithm based on the standardized scores and 4) logic for establishing a criterion-based baseline for performance comparison between any eye movement classification algorithms. Proposed techniques enable efficient and objective clinical applications providing means to assure meaningful automated eye movement classification.},
annote = {- cited by: 29
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Komogortsev, Oleg V and Gobert, D V and Jayarathna, S and Koh, D-H and Gowda, S},
doi = {10.1109/TBME.2010.2057429},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE transactions on bio-medical engineering/2010/Komogortsev et al/Komogortsev et al. - 2010 - Standardization of automated analyses of oculomotor fixation and saccadic behaviors.pdf:pdf},
issn = {0018-9294},
journal = {IEEE transactions on bio-medical engineering},
keywords = {Analysis,baseline,eye-movement classification,gaze analysis,oculomotor behavior.},
mendeley-tags = {gaze analysis},
month = nov,
number = {11},
pages = {2635--2645},
pmid = {20667803},
title = {{Standardization of automated analyses of oculomotor fixation and saccadic behaviors.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5523936 http://www.ncbi.nlm.nih.gov/pubmed/20667803},
volume = {57},
year = {2010}
}
@article{Kittur2013,
address = {New York, New York, USA},
author = {Kittur, Aniket and Peters, Andrew M. and Diriye, Abdigani and Telang, Trupti and Bove, Michael R.},
doi = {10.1145/2470654.2481415},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {1,180 mil-,3 million users and,5,60 systems,alone reported more than,com,delicious,for notetaking and organ-,in 2008,izational tools,lion unique bookmarked urls,tags for web pages,wikipedia lists more than},
pages = {2989},
publisher = {ACM Press},
title = {{Costs and benefits of structured information foraging}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481415},
year = {2013}
}
@article{Crawford1998,
author = {Crawford, George},
doi = {10.1145/333151.333154},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Crossroads/1998/Crawford/Crawford - 1998 - Creating a 3D universe in Java3D.pdf:pdf},
issn = {15284972},
journal = {Crossroads},
month = nov,
number = {2},
pages = {4--10},
title = {{Creating a 3D universe in Java3D}},
url = {http://portal.acm.org/citation.cfm?doid=333151.333154},
volume = {5},
year = {1998}
}
@article{Jacobs2006,
abstract = {A mixed r eality (MR) r epr esents an en vir onment composed both by r eal and virtual objects. MR applications ar e mor e and mor e used, for instance in sur g ery , ar c hitectur e , cultur al herita g e , entertainment, etc. F or some of these it is important to mer g e the r eal and virtual elements using consistent illumination. In this paper , we pr opose a classication of illumination methods for MR applications that aim at g ener ating a mer g ed en vir onment in whic h illumination and shadows ar e consistent. Thr ee dif fer ent illumination methods can be identied: common illumi- nation, r elighting and methods based on in ver se illumination. In this r eport a classication of the illumination methods for MR is given based on their input r equir ements: the amount of g eometry and r adiance known fr om the r eal en vir onment. This led us to dene four cate gories of methods that vary depending on the type of g eometric model used for r epr esenting the r eal scene , and the dif fer ent r adiance information available for eac h point of the r eal scene . V arious methods ar e described within their cate gory . The classication points out that in g ener al the quality of the illumination inter actions incr eases with the amount of input information available . On the other hand, the accessibility of the method decr eases since its pr e-pr ocessing time incr eases to gather the e xtr a information. Recent de veloped tec hniques mana g ed to compensate unknown data with cle ver tec hniques using an iter ative algorithm, har dwar e illumination or r ecent pr o gr ess in ster eo vision. W e complete the r e vie w of illumination tec hniques for MR with a discussion on important pr operties suc h as the possibility of inter activity or the amount of comple xity in the simulated illumination},
author = {Jacobs, Katrien and Loscos, Celine},
doi = {10.1111/j.1467-8659.2006.00816.x},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer Graphics Forum/2006/Jacobs, Loscos/Jacobs, Loscos - 2006 - Classification of Illumination Methods for Mixed Reality.pdf:pdf},
issn = {0167-7055},
journal = {Computer Graphics Forum},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = mar,
number = {1},
pages = {29--51},
title = {{Classification of Illumination Methods for Mixed Reality}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2006.00816.x/full http://doi.wiley.com/10.1111/j.1467-8659.2006.00816.x},
volume = {25},
year = {2006}
}
@misc{Castro2001,
author = {Castro, Aldemar Ara\'{u}jo},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2001/Castro/Castro - 2001 - Revis\~{a}o Sistem\'{a}tica e Meta-an\'{a}lise.PDF:PDF},
keywords = {RBS},
mendeley-tags = {RBS},
title = {{Revis\~{a}o Sistem\'{a}tica e Meta-an\'{a}lise}},
url = {http://metodologia.org/},
urldate = {2013-11-22},
year = {2001}
}
@inproceedings{Smisek2011,
abstract = {We analyze Kinect as a 3D measuring device, experimentally investigate depth measurement resolution and error properties and make a quantitative comparison of Kinect accuracy with stereo reconstruction from SLR cameras and a 3D-TOF camera. We propose Kinect geometrical model and its calibration procedure providing an accurate calibration of Kinect 3D measurement and Kinect cameras. We demonstrate the functionality of Kinect calibration by integrating it into an SfM pipeline where 3D measurements from a moving Kinect are transformed into a common coordinate system by computing relative poses from matches in color camera.},
author = {Smisek, Jan and Jancosek, Michal and Pajdla, Tomas},
booktitle = {2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},
doi = {10.1109/ICCVW.2011.6130380},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)/2011/Smisek, Jancosek, Pajdla/Smisek, Jancosek, Pajdla - 2011 - 3D with Kinect.pdf:pdf},
isbn = {978-1-4673-0063-6},
month = nov,
pages = {1154--1160},
publisher = {IEEE},
title = {{3D with Kinect}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6130380\&contentType=Conference+Publications\&searchField\%3DSearch\_All\%26queryText\%3Dkinect+calibration http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6130380},
year = {2011}
}
@article{Moura2011,
author = {Moura, Guilherme De S. and Pessoa, Saulo a. and Lima, Jo\~{a}o Paulo S. Do M. and Teichrieb, Veronica and Kelner, Judith},
doi = {10.1109/SVR.2011.14},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 XIII Symposium on Virtual Reality/2011/Moura et al/Moura et al. - 2011 - RPR-SORS An Authoring Toolkit for Photorealistic AR.pdf:pdf},
isbn = {978-1-4577-0661-5},
journal = {2011 XIII Symposium on Virtual Reality},
keywords = {-photorealism,augmented,computer graphics,pipeline,toolkit},
month = may,
pages = {178--187},
publisher = {Ieee},
title = {{RPR-SORS: An Authoring Toolkit for Photorealistic AR}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951850},
year = {2011}
}
@inproceedings{Toker2013,
abstract = {There is increasing evidence that users' characteristics such as cognitive abilities and personality have an impact on the effectiveness of information visualization techniques. This paper investigates the relationship between such characteristics and fine-grained user attention patterns. In particular, we present results from an eye tracking user study involving bar graphs and radar graphs, showing that a user's cognitive abilities such as perceptual speed and verbal working memory have a significant impact on gaze behavior, both in general and in relation to task difficulty and visualization type. These results are discussed in view of our long-term goal of designing information visualisation systems that can dynamically adapt to individual user characteristics.},
address = {New York, New York, USA},
author = {Toker, Dereck and Conati, Cristina and Steichen, Ben and Carenini, Giuseppe},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470696},
isbn = {9781450318990},
pages = {295--304},
publisher = {ACM Press},
title = {{Individual user characteristics and information visualization}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470696},
year = {2013}
}
@article{Aittala2010,
author = {Aittala, Miika},
doi = {10.1007/s00371-010-0501-7},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Visual Computer/2010/Aittala/Aittala - 2010 - Inverse lighting and photorealistic rendering for augmented reality.pdf:pdf},
issn = {0178-2789},
journal = {The Visual Computer},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = apr,
number = {6-8},
pages = {669--678},
title = {{Inverse lighting and photorealistic rendering for augmented reality}},
url = {http://link.springer.com/article/10.1007/s00371-010-0501-7 http://link.springer.com/10.1007/s00371-010-0501-7},
volume = {26},
year = {2010}
}
@article{Xu2013,
address = {New York, New York, USA},
author = {Xu, Wenchang and Yu, Chun and Zhao, Songmin and Liu, Jie and Shi, Yuanchun},
doi = {10.1145/2470654.2481297},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2167},
publisher = {ACM Press},
title = {{Facilitating parallel web browsing through multiple-page view}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481297},
year = {2013}
}
@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{Anthony2013,
address = {New York, New York, USA},
author = {Anthony, Lisa and Kim, YooJin and Findlater, Leah},
doi = {10.1145/2470654.2466158},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Touchscreen,YouTube,assistive technology,iPad,iPhone,motor impairments,physical disabilities},
pages = {1223},
publisher = {ACM Press},
title = {{Analyzing user-generated youtube videos to understand touchscreen use by people with motor impairments}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466158},
year = {2013}
}
@article{Tam2013,
address = {New York, New York, USA},
author = {Tam, Diane and MacLean, Karon E. and McGrenere, Joanna and Kuchenbecker, Katherine J.},
doi = {10.1145/2470654.2466223},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1689},
publisher = {ACM Press},
title = {{The design and field observation of a haptic notification system for timing awareness during oral presentations}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466223},
year = {2013}
}
@article{Kapps2012,
author = {Kapps, Graziele Weinchutz and Oliveira, Jauvane Cavalcante De},
doi = {10.1109/SVR.2012.19},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Kapps, Oliveira/Kapps, Oliveira - 2012 - PraCiMA A Training System for Cardiopulmonary Resuscitation Procedure.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {-cardiopulmonary arrest,cardiopulmonary resus-,citation,cpr,wii fit},
month = may,
pages = {219--226},
publisher = {Ieee},
title = {{PraCiMA: A Training System for Cardiopulmonary Resuscitation Procedure}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297533},
year = {2012}
}
@article{Long2007,
author = {Long, X and Tonguz, OK and Kiderman, A},
journal = {Engineering in Medicine and  \ldots},
title = {{A high speed eye tracking system with robust pupil center estimation algorithm}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4353043},
year = {2007}
}
@inproceedings{Figueiredo2012,
abstract = {We describe a method to generate virtual objects with realistic shadows, either in indoor and in outdoor scenes. We create smooth shadows in real time on mobile platforms and discuss the possibilities, limitations and specific implementation details of these platforms.},
address = {Rio de Janeiro},
author = {de Castro, T'ssio Knop and de Figueiredo, Luiz Henrique and Velho, Luiz},
booktitle = {2012 14th Symposium on Virtual and Augmented Reality},
doi = {10.1109/SVR.2012.9},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Castro, Figueiredo, Velho/Castro, Figueiredo, Velho - 2012 - Realistic Shadows for Mobile Augmented Reality.pdf:pdf},
isbn = {978-1-4673-1929-4},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = may,
pages = {36--45},
publisher = {IEEE},
title = {{Realistic Shadows for Mobile Augmented Reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6297558 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297558},
year = {2012}
}
@inproceedings{Lu2013,
abstract = {The prevalence of multi-touch devices opens the space for rich interactions. However, the complexity for creating multi-touch interactions hinders this potential. In this paper, we present Gesture Studio, a tool for creating multi-touch interaction behaviors by combining the strength of two distinct but complementary approaches: programming by demonstration and declaration. We employ an intuitive video-authoring metaphor for developers to demonstrate touch gestures, compose complicated behaviors, test these behaviors in the tool and export them as source code that can be integrated into the developers' project.},
address = {New York, New York, USA},
author = {L\"{u}, Hao and Li, Yang},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470690},
isbn = {9781450318990},
pages = {257--266},
publisher = {ACM Press},
title = {{Gesture studio: authoring multi-touch interactions through demonstration and declaration}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470690},
year = {2013}
}
@inproceedings{Taj2009a,
abstract = {We present a novel multi-camera multi-target fusion and tracking algorithm for noisy data. Information fusion is an important step towards robust multi-camera tracking and allows us to reduce the effect of projection and parallax errors as well as of the sensor noise. Input data from each camera view are projected on a top-view through multi-level homographic transformations. These projected planes are then collapsed onto the top-view to generate a detection volume. To increase track consistency with the generated noisy data we propose to use a track-before-detect particle filter (TBD-PF) on a 5D state-space. TBD-PF is a Bayesian method which extends the target state with the signal intensity and evaluates each image segment against the motion model. This results in filtering components belonging to noise only and enables tracking without the need of hard thresholding the signal. We demonstrate and evaluate the proposed approach on real multi-camera data from a basketball match.},
author = {Taj, Murtaza and Cavallaro, Andrea},
booktitle = {2009 Third ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC)},
doi = {10.1109/ICDSC.2009.5289405},
isbn = {978-1-4244-4620-9},
keywords = {5D state-space,Bayes methods,Bayesian method,Bayesian methods,Cameras,Image segmentation,Noise generators,Noise reduction,Noise robustness,Particle filters,Particle tracking,Sensor fusion,Target tracking,basketball,camera view,filtering component,image sensors,information fusion,multicamera multitarget fusion,multicamera multitarget tracking,multicamera track-before-detect,multilevel homographic transformation,noisy data,particle filtering (numerical methods),robust multicamera tracking,sensor fusion,signal intensity,track-before-detect particle filter,tracking},
language = {English},
mendeley-tags = {basketball,tracking},
month = aug,
pages = {1--6},
publisher = {IEEE},
shorttitle = {Distributed Smart Cameras, 2009. ICDSC 2009. Third},
title = {{Multi-camera track-before-detect}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5289405 http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5289405},
year = {2009}
}
@book{Zeigler2000,
author = {Zeigler, Bernard P. and Praehofer, Herbert and Kim, Tag Gon},
edition = {2},
isbn = {0127784551},
pages = {510},
publisher = {Academic Press},
title = {{Theory of Modeling and Simulation, Second Edition}},
year = {2000}
}
@inproceedings{Tanriverdi2000,
abstract = {Eye movement-based interaction offers the potential of easy, natural, and fast ways of interacting in virtual environments. However, there is little empirical evidence about the advantages or disadvantages of this approach. We developed a new interaction technique for eye movement interaction in a virtual environment and compared it to more conventional 3-D pointing. We conducted an experiment to compare performance of the two interaction types and to assess their impacts on spatial memory of subjects and to explore subjects' satisfaction with the two types of interactions. We found that the eye movement-based interaction was faster than pointing, especially for distant objects. However, subjects' ability to recall spatial information was weaker in the eye condition than the pointing one. Subjects reported equal satisfaction with both types of interactions, despite the technology limitations of current eye tracking equipment.},
author = {Tanriverdi, Vildan and Jacob, RJK Robert J K and Science, Computer},
booktitle = {Proceedings of the SIGCHI conference on \ldots},
doi = {10.1.1.24.3243},
keywords = {Eye,Gaze,IHC,Tracking,and fast ways of,based interaction can provide,easy,in its infancy,interacting in virtual environments,natural,the field is still,we believe eye movement-,work on eye},
mendeley-tags = {Eye,Gaze,IHC,Tracking},
pages = {265--272},
title = {{Interacting with eye movements in virtual environments}},
url = {http://dl.acm.org/citation.cfm?id=332443},
year = {2000}
}
@article{Mould2011,
abstract = {There is no standard method for classifying eye fixations. Thresholds for speed, acceleration, duration, and stability of point of gaze have each been employed to demarcate data, but they have no commonly accepted values. Here, some general distributional properties of eye movements were used to construct a simple method for classifying fixations, without parametric assumptions or expert judgment. The method was primarily speed-based, but the required optimum speed threshold was derived automatically from individual data for each observer and stimulus with the aid of Tibshirani, Walther, and Hastie's 'gap statistic'. An optimum duration threshold, also derived automatically from individual data, was used to eliminate the effects of instrumental noise. The method was tested on data recorded from a video eye-tracker sampling at 250 frames a second while experimental observers viewed static natural scenes in over 30,000 one-second trials. The resulting classifications were compared with those by three independent expert visual classifiers, with 88-94\% agreement, and also against two existing parametric methods. Robustness to instrumental noise and sampling rate were verified in separate simulations. The method was applied to the recorded data to illustrate the variation of mean fixation duration and saccade amplitude across observers and scenes.},
annote = {        From Duplicate 2 (                   A simple nonparametric method for classifying eye fixations.                 - Mould, Matthew S; Foster, David H; Amano, Kinjiro; Oakley, John P )
                
- cited by: 0
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000
        
      },
author = {Mould, Matthew S MS and Foster, DH David H and Amano, Kinjiro and Oakley, JP John P},
doi = {10.1016/j.visres.2011.12.006},
issn = {1878-5646},
journal = {Vision Research},
keywords = {Adult,Female,Fixation,Humans,Male,Nonparametric,Ocular,Ocular: physiology,Photic Stimulation,Photic Stimulation: methods,Saccades,Saccades: physiology,Sensory Thresholds,Sensory Thresholds: physiology,Statistics,Time Factors,Young Adult,gaze analysis},
mendeley-tags = {gaze analysis},
month = mar,
pages = {18--25},
pmid = {22227608},
title = {{A simple nonparametric method for classifying eye fixations}},
url = {http://www.sciencedirect.com/science/article/pii/S0042698911004214 http://personalpages.manchester.ac.uk/staff/david.foster/Research/My\_PDFs/Mould\_etal\_VR\_12\_MS.pdf http://www.ncbi.nlm.nih.gov/pubmed/22227608},
volume = {57},
year = {2012}
}
@article{Eshel2009,
abstract = {Tracking people in a dense crowd is a challenging problem for a single camera tracker due to occlusions and extensive motion that make human segmentation difficult. In this paper we suggest a method for simultaneously tracking all the people in a densely crowded scene using a set of cameras with overlapping fields of view. To overcome occlusions, the cameras are placed at a high elevation and only people’s heads are tracked. Head detection is still difficult since each foreground region may consist of multiple subjects. By combining data from several views, height information is extracted and used for head segmentation. The head tops, which are regarded as 2D patches at various heights, are detected by applying intensity correlation to aligned frames from the different cameras. The detected head tops are then tracked using common assumptions on motion direction and velocity. The method was tested on sequences in indoor and outdoor environments under challenging illumination conditions. It was successful in tracking up to 21 people walking in a small area (2.5 people per m2), in spite of severe and persistent occlusions.},
author = {Eshel, Ran and Moses, Yael},
doi = {10.1007/s11263-009-0307-0},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {detection,multiple-view,tracking},
month = nov,
number = {1},
pages = {129--143},
title = {{Tracking in a Dense Crowd Using Multiple Cameras}},
url = {http://link.springer.com/10.1007/s11263-009-0307-0},
volume = {88},
year = {2009}
}
@article{Blignaut2009,
abstract = {It is hypothesized that the number, position, size, and duration of fixations are functions of the metric used for dispersion in a dispersion-based fixation detection algorithm, as well as of the threshold value. The sensitivity of the I-DT algorithm for the various independent variables was determined through the analysis of gaze data from chess players during a memory recall experiment. A procedure was followed in which scan paths were generated at distinct intervals in a range of threshold values for each of five different metrics of dispersion. The percentage of points of regard (PORs) used, the number of fixations returned, the spatial dispersion of PORs within fixations, and the difference between the scan paths were used as indicators to determine an optimum threshold value. It was found that a fixation radius of 1 degrees provides a threshold that will ensure replicable results in terms of the number and position of fixations while utilizing about 90\% of the gaze data captured.},
annote = {- keyword coletado
- cited by: 24- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Blignaut, Pieter},
doi = {10.3758/APP.71.4.881},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Attention, perception \& psychophysics/2009/Blignaut/Blignaut - 2009 - Fixation identification the optimum threshold for a dispersion algorithm.pdf:pdf},
issn = {1943-3921},
journal = {Attention, perception \& psychophysics},
keywords = {Adolescent,Adult,Algorithms,Attention,Computer Graphics,Discrimination (Psychology),Eye Tracking,Female,Fixation,Humans,Male,Memory,Middle Aged,Ocular,Orientation,Pattern Recognition,Psychomotor Performance,Psychophysics,Reaction Time,Saccades,Segmentation,Sensory Thresholds,Short-Term,Visual,Young Adult},
mendeley-tags = {Eye Tracking,Segmentation},
month = may,
number = {4},
pages = {881--95},
pmid = {19429966},
title = {{Fixation identification: the optimum threshold for a dispersion algorithm.}},
url = {http://link.springer.com/article/10.3758/APP.71.4.881 http://www.ncbi.nlm.nih.gov/pubmed/19429966},
volume = {71},
year = {2009}
}
@inproceedings{Shih2003,
author = {Shih, Timothy K T.K. and Lin, Nigel H N.H. and Chuang, Jung-ken},
booktitle = {23rd International Conference on Distributed Computing Systems Workshops, 2003. Proceedings.},
doi = {10.1109/ICDCSW.2003.1203626},
file = {:home/acmt/Dropbox/Documentos/Mendeley/23rd International Conference on Distributed Computing Systems Workshops, 2003. Proceedings/2003/Shih, Lin, Chuang/Shih, Lin, Chuang - 2003 - Augmented Video Conferencing.pdf:pdf},
isbn = {0-7695-1921-0},
keywords = {analog technology,communication will replace traditional,distance learning,mpeg,multimedia communication system,synchronization,video conferencing},
pages = {646--651},
publisher = {IEEE Computer Society Press},
title = {{Augmented Video Conferencing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1203626},
year = {2003}
}
@inproceedings{Raskar2001,
abstract = {We describe a calibration and rendering technique for a projector that can render rectangular images under keystoned position. The projector utilizes a rigidly attached camera to form a stereo pair. We describe a very easy to use technique for calibration of the projector-camera pair using only black planar surfaces. We present an efficient rendering method to pre-warp images so that they appear correctly on the screen, and show experimental results.},
annote = {        From Duplicate 1 (                   A self-correcting projector                 - Raskar, R; Beardsley, P )
                
        
        
      },
author = {Raskar, R. and Beardsley, P.},
booktitle = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
doi = {10.1109/CVPR.2001.991004},
isbn = {0-7695-1272-0},
issn = {1063-6919},
keywords = {Calibration,Cameras,Computational geometry,Computer vision,Displays,Laboratories,Lenses,Prototypes,Rendering (computer graphics),Stereo vision,anamorphism,black planar surfaces,homography,keystone,keystoned position,optical projectors,rectangular images,rendering,self-correcting projector,stereo pair},
mendeley-tags = {anamorphism,keystone},
pages = {II--504--II--508},
publisher = {IEEE Comput. Soc},
shorttitle = {Computer Vision and Pattern Recognition, 2001. CVP},
title = {{A self-correcting projector}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=991004 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=991004},
volume = {2},
year = {2001}
}
@inproceedings{Knecht2011,
abstract = {We present a novel adaptive color mapping method for virtual objects in mixed-reality environments. In several mixed-reality applications, added virtual objects should be visually indistinguishable from real objects. Recent mixed-reality methods use global-illumination algorithms to approach this goal. However, simulating the light distribution is not enough for visually plausible images. Since the observing camera has its very own transfer function from real-world radiance values to RGB colors, virtual objects look artificial just because their rendered colors do not match with those of the camera. Our approach combines an on-line camera characterization method with a heuristic to map colors of virtual objects to colors as they would be seen by the observing camera. Previous tone-mapping functions were not designed for use in mixed-reality systems and thus did not take the camera-specific behavior into account. In contrast, our method takes the camera into account and thus can also handle changes of its parameters during runtime. The results show that virtual objects look visually more plausible than by just applying tone-mapping operators.},
author = {Knecht, Martin and Traxler, Christoph and Purgathofer, Werner and Wimmer, Michael},
booktitle = {2011 10th IEEE International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2011.6092382},
file = {::},
isbn = {978-1-4577-2185-4},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = oct,
pages = {165--168},
publisher = {IEEE},
title = {{Adaptive camera-based color mapping for mixed-reality applications}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6162884 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6162884},
year = {2011}
}
@misc{Kooima2013,
author = {Kooima, Robert},
keywords = {SBGames},
mendeley-tags = {SBGames},
title = {{Generalized Perspective Projection}},
url = {http://csc.lsu.edu/~kooima/articles/genperspective/index.html},
urldate = {2013-05-07},
year = {2013}
}
@book{Russel1995,
address = {New Jersey},
author = {Russel, Stuart J and Norvig, Peter},
editor = {Pompilli, Mona and Chavez, Sondra and McGuire, Shirley},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/1995/Russel, Norvig/Russel, Norvig - 1995 - Artificial Intelligence A Modern Approach.pdf:pdf},
isbn = {0131038052},
issn = {1682-8658},
number = {2},
pages = {946},
pmid = {21560649},
publisher = {Prentice-Hall},
title = {{Artificial Intelligence A Modern Approach}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21932643},
year = {1995}
}
@article{Komogortsev2009b,
abstract = {This paper presents a new saccade amplitude prediction model. The model is based on a Kalman filter and regression analysis. The aim of the model is to predict a saccade’s am-plitude extremely quickly, i.e., within two eye position samples at the onset of a saccade. Specifically, the paper explores saccade amplitude prediction considering one or two sam-ples at the onset of a saccade. The models’ prediction performance was tested with 35 subjects. The amplitude accuracy results yielded approximately 5.26° prediction error, while the error for direction prediction was 5.3\% for the first sample model and 1.5\% for the two samples model. The practical use of the proposed model lays in the area of real-time gaze-contingent compression and extreme eye-gaze aware interaction applications. The paper provides theoretical evaluation of the benefits of saccade amplitude prediction to the gaze-contingent multimedia compression, estimating a 21\% improvement in com-pression for short network delays.},
author = {Komogortsev, Oleg V and Ryu, YS and Koh, DH},
file = {::},
journal = {Journal of Eye Movement Research},
keywords = {Human Computer,Kalman Filter,Prediction,Saccade,eye events,saccade},
mendeley-tags = {eye events,saccade},
number = {1},
pages = {1--13},
title = {{Quick models for saccade amplitude prediction}},
url = {http://www.cs.txstate.edu/~ok11/papers\_published/2009\_JEMR\_Ko\_Ry\_Ko.pdf},
volume = {3(1)},
year = {2009}
}
@inproceedings{Liu2010,
abstract = {We consider the problem of estimating the depth of each pixel in a scene from a single monocular image. Unlike traditional approaches, which attempt to map from appearance features to depth directly, we first perform a semantic segmentation of the scene and use the semantic labels to guide the 3D reconstruction. This approach provides several advantages: By knowing the semantic class of a pixel or region, depth and geometry constraints can be easily enforced (e.g., “sky” is far away and “ground” is horizontal). In addition, depth can be more readily predicted by measuring the difference in appearance with respect to a given semantic class. For example, a tree will have more uniform appearance in the distance than it does close up. Finally, the incorporation of semantic features allows us to achieve state-of-the-art results with a significantly simpler model than previous works.},
address = {San Francisco, CA},
author = {Liu, Beyang and Gould, Stephen and Koller, Daphne},
booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5539823},
isbn = {978-1-4244-6984-0},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = jun,
pages = {1253--1260},
publisher = {IEEE},
title = {{Single image depth estimation from predicted semantic labels}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5539823 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5539823},
year = {2010}
}
@misc{Ho2013,
author = {Ho, Nghia},
keywords = {SBGames},
mendeley-tags = {SBGames},
title = {{Finding Optimal Rotation and Translation Between Corresponding 3D Points}},
url = {http://nghiaho.com/?page\_id=671},
urldate = {2013-07-23},
year = {2013}
}
@article{Yamashita2013,
address = {New York, New York, USA},
author = {Yamashita, Naomi and Kuzuoka, Hideaki and Hirata, Keiji and Kudo, Takashi},
doi = {10.1145/2470654.2481365},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2637},
publisher = {ACM Press},
title = {{Understanding the conflicting demands of family caregivers caring for depressed family members}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481365},
year = {2013}
}
@article{Morimoto2005,
abstract = {This paper presents a review of eye gaze tracking technology and focuses on recent advancements that might facilitate its use in general computer applications. Early eye gaze tracking devices were appropriate for scientific exploration in controlled environments. Although it has been thought for long that they have the potential to become important computer input devices as well, the technology still lacks important usability requirements that hinders its applicability. We present a detailed description of the pupil-corneal reflection technique due to its claimed usability advantages, and show that this method is still not quite appropriate for general interactive applications. Finally, we present several recent techniques for remote eye gaze tracking with improved usability. These new solutions simplify or eliminate the calibration procedure and allow free head motion.},
author = {Morimoto, Carlos H. and Mimica, Marcio R.M.},
doi = {10.1016/j.cviu.2004.07.010},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer Vision and Image Understanding/2005/Morimoto, Mimica/Morimoto, Mimica - 2005 - Eye gaze tracking techniques for interactive applications.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {eye tracker,toshi},
mendeley-tags = {eye tracker,toshi},
month = apr,
number = {1},
pages = {4--24},
publisher = {Elsevier Science Inc.},
title = {{Eye gaze tracking techniques for interactive applications}},
url = {http://dl.acm.org/citation.cfm?id=1061935.1649095},
volume = {98},
year = {2005}
}
@article{Ulicny2001,
author = {Ulicny, B and Thalmann, Daniel},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer Animation and Simulation 2001/2001/Ulicny, Thalmann/Ulicny, Thalmann - 2001 - Crowd simulation for interactive virtual environments and VR training systems.pdf:pdf},
journal = {Computer Animation and Simulation 2001},
keywords = {crowd simulation},
mendeley-tags = {crowd simulation},
title = {{Crowd simulation for interactive virtual environments and VR training systems}},
url = {http://link.springer.com/chapter/10.1007/978-3-7091-6240-8\_15},
year = {2001}
}
@article{Mantiuk2013,
abstract = {In this paper we model the process of temporal adaptation of the human visual system to varying luminance conditions. An eye tracker is used to capture the location of an observer’s gaze in a high dynamic range image displayed on the screen. We apply a novel technique of eye tracker data filtering to avoid flickering caused by incorrect gaze estimation. Temporary adaptation luminance is then determined in the area surrounding the gaze point. We use its value to compress the high dynamic range image and display it on the low dynamic range display. The applied tone mapping technique uses a global compression curve in which location is shifted along the luminance axis according to a value of the adaptation luminance. This technique models the natural process of adaptation occurring in the human eyes, also taking into account the time-dependent visual adaptation to dark and bright backgrounds.},
author = {Mantiuk, R and Markowski, M},
doi = {10.1007/978-3-642-39094-4\_48},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Image Analysis and Recognition/2013/Mantiuk, Markowski/Mantiuk, Markowski - 2013 - Gaze-Dependent Tone Mapping.pdf:pdf},
journal = {Image Analysis and Recognition},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {426--433},
title = {{Gaze-Dependent Tone Mapping}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-39094-4\_48},
volume = {7950},
year = {2013}
}
@inproceedings{Rakprayoon2011,
abstract = {This paper presents a method to distinguish between obstacles and manipulator when they share the same workspace. Microsoft Kinect is used as a capturing device. A Kinect calibration method is explained. Furthermore, calibration between Kinect and the manipulator is addressed by iterative least-square method. 3D model of manipulator is generated using OpenGL library. Finally, the manipulator surface is deleted from the scene by intersection of data between the manipulator model and its corresponding point cloud.},
author = {Rakprayoon, Panjawee and Ruchanurucks, Miti and Coundoul, Ada},
booktitle = {2011 IEEE/SICE International Symposium on System Integration (SII)},
doi = {10.1109/SII.2011.6147421},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 IEEESICE International Symposium on System Integration (SII)/2011/Rakprayoon, Ruchanurucks, Coundoul/Rakprayoon, Ruchanurucks, Coundoul - 2011 - Kinect-based obstacle detection for manipulator.pdf:pdf},
isbn = {978-1-4577-1524-2},
month = dec,
pages = {68--73},
publisher = {IEEE},
title = {{Kinect-based obstacle detection for manipulator}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6147421\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=kinect+calibration},
year = {2011}
}
@article{Blackmon1997,
author = {Blackmon, TT and Ho, YF},
journal = {\ldots  in Medicine and  \ldots},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Eye movements while viewing dynamic and static stimuli}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=756912},
year = {1997}
}
@inproceedings{Breuer2008,
abstract = {This paper presents a fully automated algorithm for reconstructing a textured 3D model of a face from a single photograph or a raw video stream. The algorithm is based on a combination of Support Vector Machines (SVMs) and a Morphable Model of 3D faces. After SVM face detection, individual facial features are detected using a novel regression- and classification-based approach, and probabilistically plausible configurations of features are selected to produce a list of candidates for several facial feature positions. In the next step, the configurations of feature points are evaluated using a novel criterion that is based on a Morphable Model and a combination of linear projections. To make the algorithm robust with respect to head orientation, this process is iterated while the estimate of pose is refined. Finally, the feature points initialize a model-fitting procedure of the Morphable Model. The result is a high resolution 3D surface model.},
author = {Breuer, Pia and Kim, Kwang-In and Kienzle, Wolf and Scholkopf, Bernhard and Blanz, Volker},
booktitle = {2008 8th IEEE International Conference on Automatic Face \& Gesture Recognition},
doi = {10.1109/AFGR.2008.4813339},
isbn = {978-1-4244-2153-4},
keywords = {Biological system modeling,Computer vision,Deformable models,Face detection,Facial features,Image reconstruction,Shape,Streaming media,Support vector machine classification,Support vector machines,face recognition,face reconstruction,facial feature,reconstruction},
mendeley-tags = {reconstruction},
month = sep,
pages = {1--8},
publisher = {IEEE},
shorttitle = {Automatic Face \& Gesture Recognition, 2008. FG '08},
title = {{Automatic 3D face reconstruction from single images or video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4813339},
year = {2008}
}
@article{Widdel1984,
abstract = {Eye movement measurement with cornea-pupil reflection technique requires an operational definition of a fixation to be able to analyse eye movements by data reduction strategies. The present paper illustrates some effects of variable fixation definitions on the calculation of eye movement characteristics conducting an experiment with simple visual scanning tasks. The results of eye movement evaluation are dependent on the defined size of a fixation. They are varying in alternation with stimulus aspects provoking different densities of fixation locations as well as with individual eye movement behavior.},
author = {Widdel, Heino},
doi = {10.1016/S0166-4115(08)61814-2},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Advances in Psychology/1984/Widdel/Widdel - 1984 - Operational Problems in Analysing Eye Movements.pdf:pdf},
journal = {Advances in Psychology},
keywords = {dispersion,eye movement},
mendeley-tags = {dispersion,eye movement},
pages = {21--29},
title = {{Operational Problems in Analysing Eye Movements}},
url = {http://www.sciencedirect.com/science/article/pii/S0166411508618142},
volume = {22},
year = {1984}
}
@misc{Madsen2007,
abstract = {Abstract: Visually realistic Augmented Reality (AR) entails addressing several difficult problems. The most difficult problem is that of rendering the virtual objects with illumination which is consistent with the illumination of the real scene. The paper describes a complete AR rendering system centered around the use of High Dynamic Range environment maps for representing the real scene illumination. The main contribution lies in a novel, physically-based approach to rendering shadows cast by virtual objects without changing the shadows already present in the images of the real scene. The proposed approach effectively involves real-time estimation of the diffuse albedos of the real scene, and essentially relighting these areas to take virtual shadows into account. Another contribution lies in the fact that the proposed approach is designed to run on graphics hardware and is scalable in the sense that it offers a simple way to balance performance with visual quality.},
author = {Madsen, C and Laursen, R},
doi = {10.1.1.122.5598},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2007/Madsen, Laursen/Madsen, Laursen - 2007 - A scalable gpu-based approach to shading and shadowing for photo-realistic real-time augmented reality.pdf:pdf},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
publisher = {Citeseer},
title = {{A scalable gpu-based approach to shading and shadowing for photo-realistic real-time augmented reality}},
url = {http://vbn.aau.dk/files/12699282/grapp07.pdf},
year = {2007}
}
@book{Wolfe2009,
author = {Wolfe, Jeremy M. and Kluender, Keith R. and Levi, Dennis M. and Bartoshuk, Linda M. and Herz, Rachel S. and Klatzky, Roberta L. and Lederman, Susan J. and Merfeld, Daniel M.},
edition = {2},
isbn = {9780878939534},
keywords = {SBGames},
mendeley-tags = {SBGames},
pages = {450},
publisher = {Sinauer Associates},
title = {{Sensation \& Perception}},
url = {http://www.amazon.com/Sensation-Perception-second-Jeremy-Wolfe/dp/B006OMH66E},
year = {2009}
}
@inproceedings{Jacobs2005,
abstract = {In the context of mixed reality, it is difficult to simulate shadow interaction between real and virtual objects when only an approximate geometry of the real scene and the light source is known. In this paper, we present a real-time rendering solution to simulate colour-consistent virtual shadows in a real scene. The rendering consists of a three-step mechanism: shadow detection, shadow protection and shadow generation. In the shadow detection step, the shadows due to real objects are automatically identified using the texture information and an initial estimate of the shadow region. In the next step, a protection mask is created to prevent further rendering in those shadow regions. Finally, the virtual shadows are generated using shadow volumes and a pre-defined scaling factor that adapts the intensity of the virtual shadows to the real shadow. The procedure detects and generates shadows in real time, consistent with those already present in the scene and offers an automatic and real-time solution for common illumination, suitable for augmented reality.},
author = {Jacobs, K and Nahmias, JD and Angus, C},
booktitle = {Proceedings of Graphics Interface 2005},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of Graphics Interface 2005/2005/Jacobs, Nahmias, Angus/Jacobs, Nahmias, Angus - 2005 - Automatic generation of consistent shadows for augmented reality.pdf:pdf},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
pages = {113--120},
publisher = {ACM},
title = {{Automatic generation of consistent shadows for augmented reality}},
url = {http://dl.acm.org/citation.cfm?id=1089527},
year = {2005}
}
@article{AntoniodaSilva2008,
abstract = {The process of teaching-learning based on lessons spoken-exposed, having the blackboard as the main didactic resource, does not currently brings attractiveness consistent and coherent with the environment in which elementary school students are involved with, full of technologies and innovations. Computers had and have a decisive role in changing this scenario. They had in education three generations, and the fourth generation begins to emerge with Virtual Reality (VR). The way of learning depends upon each person, some learn visually, other verbally, some explore and other deduct. An example of learning object that requires the student to make an abstraction or mentally to create a model for understanding is a three physiology. This area of knowledge covers the anatomy of plants, which can not only be taught with the use of traditional tools of education, since it presents complexity and dynamic ways that can only be viewed through simulations. In this context, the Augmented Reality (AR) fits as a tool for viewing, interaction and involvement with objects of learning. The purpose of this paper is to present an architecture for distribution of virtual environments of AR as a tool to support projects in education. A cognitive interface has been created to allow AR user interaction with a virtual environment through interaction menus and also through real markers. With this implementation, the interactions made in an virtual environment are replicated for other environments, creating a system strongly applied to support education, since it provides multiple views of virtual objects. It is understood that when one distributes the interactions made, users can make cooperate in an AR environment, enabling a greater level of understanding for the taught subject.},
author = {{Ant\^{o}nio da Silva}, Wender and Ribeiro, Marcos Wagner de Souza and J\'{u}nior, Edgard Lamounier and Cardoso, Alexandre},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Revista Brasileira de Inform\'{a}tica na Educa\c{c}\~{a}o/2008/Ant\^{o}nio da Silva et al/Ant\^{o}nio da Silva et al. - 2008 - Uma Arquitetura para Distribui\c{c}\~{a}o de Ambientes Virtuais de Realidade Aumentada Aplicada \`{a} Educa\c{c}\~{a}.pdf:pdf},
journal = {Revista Brasileira de Inform\'{a}tica na Educa\c{c}\~{a}o},
number = {3},
pages = {57--69},
title = {{Uma Arquitetura para Distribui\c{c}\~{a}o de Ambientes Virtuais de Realidade Aumentada Aplicada \`{a} Educa\c{c}\~{a}o}},
volume = {16},
year = {2008}
}
@inproceedings{Santos2012,
abstract = {This paper introduces a novel graphics rendering pipeline applied to augmented reality, based on a real time ray tracing paradigm. Ray tracing techniques process pixels independently from each other, allowing an easy integration with image-based tracking techniques, contrary to traditional projection-based rasterization graphics systems, e.g. OpenGL. Therefore, by associating our highly optimized ray tracer with an augmented reality framework, the proposed pipeline is capable to provide high quality rendering with real time interaction between virtual and real objects, such as occlusions, soft shadows, custom shaders, reflections and self-reflections, some of these features only available in our rendering pipeline. As proof of concept, we present a case study with the ARToolKitPlus library and the Microsoft Kinect hardware, both integrated in our pipeline. Furthermore, we show the performance and visual results in high definition of the novel pipeline on modern graphics cards, presenting occlusion and recursive reflection effects between virtual and real objects without the latter ones needing to be previously modeled when using Kinect. Furthermore, an adaptive soft shadow sampling algorithm for ray tracing is presented, generating high quality shadows in real time for most scenes.},
address = {Rio de Janeiro},
author = {Santos, Artur Lira Dos and Lemos, Diego and Lindoso, Jorge Eduardo Falcao and Teichrieb, Veronica},
booktitle = {2012 14th Symposium on Virtual and Augmented Reality},
doi = {10.1109/SVR.2012.8},
isbn = {978-1-4673-1929-4},
keywords = {2,a coprocessor for executing,augmented reality,cuda,highly parallel algorithms,including ray tracing in,kinect,occlusion,ray tracing,real,real time,reflection,refraction,they are used as},
month = may,
pages = {131--140},
publisher = {IEEE},
title = {{Real Time Ray Tracing for Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297569},
year = {2012}
}
@article{Xing2013,
abstract = {Real-time estimation of outdoor illumination is one of the key issues for ensuring the illumination consistency of augmented reality. In this paper, we propose a novel framework to estimate the dynamic illumination of outdoor scenes based on an online video sequence captured by a fixed camera. All existing approaches are based on two assumptions, i.e. there exist some shadow areas in the scene and the distribution of the skylight is uniform over the sky. Both assumptions greatly simplify the problem of illumination estimation of outdoor scenes, but they also limit the applicability as well as the accuracy of these approaches. This paper presents a new approach that breaks these two hard constraints. It recovers the lighting parameters of outdoor scenes containing no shadow area through solving a constrained linear least squares problem. By representing the skylight as a parameterized model incorporating an occlusion coefficient, the proposed approach can handle the dynamic variation of non-uniform skylight distribution. Experimental results demonstrate the potential of our approach.},
author = {Xing, GuanYu and Zhou, XueHong and Liu, YanLi and Qin, XueYing and Peng, QunSheng},
doi = {10.1007/s11432-012-4780-7},
issn = {1674-733X},
journal = {Science China Information Sciences},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = feb,
number = {3},
pages = {1--11},
title = {{Online illumination estimation of outdoor scenes based on videos containing no shadow area}},
url = {http://link.springer.com/article/10.1007/s11432-012-4780-7 http://link.springer.com/10.1007/s11432-012-4780-7},
volume = {56},
year = {2013}
}
@article{Tanenbaum2013,
address = {New York, New York, USA},
author = {Tanenbaum, Joshua G. and Antle, Alissa N. and Robinson, John},
doi = {10.1145/2470654.2466464},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3389},
publisher = {ACM Press},
title = {{Three perspectives on behavior change for serious games}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466464},
year = {2013}
}
@article{Holmqvist2011,
author = {Holmqvist, K and Nystr\"{o}m, M and Andersson, R},
file = {::},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Eye tracking: A comprehensive guide to methods and measures}},
url = {http://books.google.com/books?hl=pt-BR\&lr=\&id=5rIDPV1EoLUC\&oi=fnd\&pg=PT27\&dq=Eye+Tracking:+A+comprehensive+guide+to+methods+and+measures\&ots=\_u6CRZqQpO\&sig=Lple\_koXcqQ4jWwSt0dBu7HdYy0},
year = {2011}
}
@article{Poynter2013,
abstract = {The purpose of this study was to examine individual differences in eye-movement behavior. Six metrics (Fixation Rate, Duration, and Size; Saccade Amplitude; Micro-Saccade Rate and Amplitude) were used to measure individuals' eye-movement behavior profiles (EmBP). We replicate previous research (Andrews \& Coppola, 1999; Castelhano \& Henderson, 2008) by finding consistent individual differences in fixation duration and saccade amplitude across tasks, and present new findings of stable idiosyncrasies in measures of fixational eye-movement (Fixation Size, Micro-Saccade Rate and Amplitude). Moreover, we observed consistent inter-metric correlations across tasks (e.g., individuals that exhibited relatively high Fixation Rates also presented relative low Micro-Saccade Rates and relatively high Micro-Saccade Amplitudes). Factor Analysis linked the six EmBP metrics together with a single factor, which we speculate might be related to the operational effectiveness of the attentional system, given that individual factor scores were correlated with scores on a self-report measure of attentional function. Normal subjects with relatively high scores on this attention-deficit measure exhibited relatively frequent fixations of short duration and large spatial extent, and relatively infrequent micro-saccades of large amplitude. This EmBP is similar to a general pattern of eye-movement behavior observed with ADHD individuals - difficulty controlling eye movements, maintaining fixation, and inhibiting intrusive saccades. Results of this study indicate that normal individuals exhibit idiosyncratic EmBPs that are quite stable across tasks and are related to attentional ability.},
author = {Poynter, William and Barber, Megan and Inman, Jason and Wiggins, Coral},
doi = {10.1016/j.visres.2013.07.002},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Vision Research/2013/Poynter et al/Poynter et al. - 2013 - Individuals exhibit idiosyncratic eye-movement behavior profiles across tasks.pdf:pdf},
issn = {1878-5646},
journal = {Vision Research},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = aug,
pages = {32--8},
pmid = {23867568},
title = {{Individuals exhibit idiosyncratic eye-movement behavior profiles across tasks.}},
url = {http://www.sciencedirect.com/science/article/pii/S0042698913001685 http://www.ncbi.nlm.nih.gov/pubmed/23867568},
volume = {89},
year = {2013}
}
@article{Liston2012,
annote = {- keyword coletado},
author = {Liston, Dorion B. and Krukowski, Anton E. and Stone, Leland S.},
doi = {10.1016/j.displa.2012.10.002},
issn = {01419382},
journal = {Displays},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
month = apr,
number = {2},
pages = {171--176},
title = {{Saccade detection during smooth tracking}},
url = {http://www.sciencedirect.com/science/article/pii/S0141938212000777 http://linkinghub.elsevier.com/retrieve/pii/S0141938212000777},
volume = {34},
year = {2013}
}
@article{Xing2013a,
abstract = {We propose a novel approach to simulate the illumination of augmented outdoor scene based on a legacy photograph. Unlike previous works which only take surface radiosity or lighting related prior information as the basis of illumination estimation, our method integrates both of these two items. By adopting spherical harmonics, we deduce a linear model with only six illumination parameters. The illumination of an outdoor scene is finally calculated by solving a linear least square problem with the color constraint of the sunlight and the skylight. A high quality environment map is then set up, leading to realistic rendering results. We also explore the problem of shadow casting between real and virtual objects without knowing the geometry of objects which cast shadows. An efficient method is proposed to project complex shadows (such as tree's shadows) on the ground of the real scene to the surface of the virtual object with texture mapping. Finally, we present an unified scheme for image composition of a real outdoor scene with virtual objects ensuring their illumination consistency and shadow consistency. Experiments demonstrate the effectiveness and flexibility of our method.},
author = {Xing, Guanyu and Zhou, Xuehong and Peng, Qunsheng and Liu, Yanli and Qin, Xueying},
doi = {10.1111/cgf.12217},
issn = {01677055},
journal = {Computer Graphics Forum},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = oct,
number = {7},
pages = {101--110},
title = {{Lighting Simulation of Augmented Outdoor Scene Based on a Legacy Photograph}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/cgf.12217/full http://doi.wiley.com/10.1111/cgf.12217},
volume = {32},
year = {2013}
}
@incollection{Kale2005,
address = {New York},
author = {Kale, A and Kwan, K and Jaynes, C},
booktitle = {Real-Time Vision for Human-Computer Interaction},
doi = {10.1007/0-387-27890-7\_12},
editor = {Kisa\v{c}anin, Branislav and Pavlovi\'{c}, Vladimir and Huang, Thomas S.},
isbn = {0-387-27697-1},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
pages = {201--214},
publisher = {Springer-Verlag},
title = {{Epipolar Constrained User Pushbutton Selection in Projected Interfaces}},
url = {http://link.springer.com/content/pdf/10.1007/0-387-27890-7\_12.pdf http://link.springer.com/10.1007/0-387-27890-7},
year = {2005}
}
@inproceedings{Wei2013,
abstract = {Due to the demand for better and deeper analysis in sports, organizations (both professional teams and broadcasters) are looking to use spatiotemporal data in the form of player tracking information to obtain an advantage over their competitors. However, due to the large volume of data, its unstructured nature, and lack of associated team activity labels (e.g. strategic/tactical), effective and efficient strategies to deal with such data have yet to be deployed. A bottleneck restricting such solutions is the lack of a suitable representation (i.e. ordering of players) which is immune to the potentially infinite number of possible permutations of player orderings, in addition to the high dimensionality of temporal signal (e.g. a game of soccer last for 90 mins). Leveraging a recent method which utilizes a "role-representation", as well as a feature reduction strategy that uses a spatiotemporal bilinear basis model to form a compact spatiotemporal representation. Using this representation, we find the most likely formation patterns of a team associated with match events across nearly 14 hours of continuous player and ball tracking data in soccer. Additionally, we show that we can accurately segment a match into distinct game phases and detect highlights. (i.e. shots, corners, free-kicks, etc) completely automatically using a decision-tree formulation.},
address = {Hobart, Tasmania},
author = {Wei, Xinyu and Sha, Long and Lucey, Patrick and Morgan, Stuart and Sridharan, Sridha},
booktitle = {2013 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
doi = {10.1109/DICTA.2013.6691503},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2013 International Conference on Digital Image Computing Techniques and Applications (DICTA)/2013/Wei et al/Wei et al. - 2013 - Large-Scale Analysis of Formations in Soccer.pdf:pdf},
isbn = {978-1-4799-2126-3},
month = nov,
pages = {1--8},
publisher = {IEEE},
title = {{Large-Scale Analysis of Formations in Soccer}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6691503},
year = {2013}
}
@article{Hartley2007,
abstract = {We propose a method of simultaneously calibrating the radial distortion function of a camera and the other internal calibration parameters. The method relies on the use of a planar (or, alternatively, nonplanar) calibration grid which is captured in several images. In this way, the determination of the radial distortion is an easy add-on to the popular calibration method proposed by Zhang [24]. The method is entirely noniterative and, hence, is extremely rapid and immune to the problem of local minima. Our method determines the radial distortion in a parameter-free way, not relying on any particular radial distortion model. This makes it applicable to a large range of cameras from narrow-angle to fish-eye lenses. The method also computes the center of radial distortion, which, we argue, is important in obtaining optimal results. Experiments show that this point may be significantly displaced from the center of the image or the principal point of the camera.},
author = {Hartley, Richard and Kang, Sing Bing},
doi = {10.1109/TPAMI.2007.1147},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = aug,
number = {8},
pages = {1309--21},
pmid = {17568137},
title = {{Parameter-free radial distortion correction with center of distortion estimation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17568137},
volume = {29},
year = {2007}
}
@article{Oh2013,
address = {New York, New York, USA},
author = {Oh, Uran and Findlater, Leah},
doi = {10.1145/2470654.2466145},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1129},
publisher = {ACM Press},
title = {{The challenges and potential of end-user gesture customization}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466145},
year = {2013}
}
@article{Lacey2007,
author = {Lacey, Gerard and Ryan, Donncha and Cassidy, Derek and Young, Derek},
doi = {10.1109/MMUL.2007.79},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Multimedia/2007/Lacey et al/Lacey et al. - 2007 - Mixed-Reality Simulation of Minimally Invasive Surgeries.pdf:pdf},
issn = {1070-986X},
journal = {IEEE Multimedia},
month = oct,
number = {4},
pages = {76--87},
title = {{Mixed-Reality Simulation of Minimally Invasive Surgeries}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4354160},
volume = {14},
year = {2007}
}
@article{Vogl2006,
author = {Vogl, W. and Sitti, M.},
doi = {10.1109/TNANO.2006.877421},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions On Nanotechnology/2006/Vogl, Sitti/Vogl, Sitti - 2006 - Augmented reality user interface for an atomic force microscope-based nanorobotic system.pdf:pdf},
issn = {1536-125X},
journal = {IEEE Transactions On Nanotechnology},
month = jul,
number = {4},
pages = {397--406},
title = {{Augmented reality user interface for an atomic force microscope-based nanorobotic system}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1652858},
volume = {5},
year = {2006}
}
@inproceedings{Banerjee2010,
abstract = {Multi-Agent Plan Recognition (MAPR) seeks to identify the dynamic team structures and team behaviors from the obser- vations of the activity-sequences of a set of intelligent agents, based on a library of known team-activities (plan library). It has important applications in analyzing data from automated monitoring, surveillance, and intelligence analysis in general. In this paper, we formalize MAPR using a basic model that explicates the cost of abduction in single agent plan recog- nition by ”flattening” or decompressing the (usually com- pact, hierarchical) plan library. We show that single-agent plan recognition with a decompressed library can be solved in time polynomial in the input size, while it is known that with a compressed (by partial ordering constraints) library it is NP-complete. This leads to an important insight: that al- though the compactness of the plan library plays an important role in the hardness of single-agent plan recognition (as rec- ognized in the existing literature), that is not the case with multiple agents. We show, for the first time, that MAPR is NP-complete even when the (multi-agent) plan library is fully decompressed. As with previous solution approaches, we break the problem into two stages: hypothesis generation and hypothesis search. We show that Knuth’s “Algorithm X” (with the efficient “dancing links” representation) is particu- larly suited for our model, and can be adapted to perform a branch and bound search for the second stage, in this model. We show empirically that this new approach leads to signifi- cant pruning of the hypothesis space in MAPR. Introduction},
address = {Atlanta, GA},
author = {Banerjee, Bikramjit and Kraemer, Landon},
booktitle = {Proceedings of AAAI Conference on Artificial Intelligence},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of AAAI Conference on Artificial Intelligence/2010/Banerjee, Kraemer/Banerjee, Kraemer - 2010 - Multi-Agent Plan Recognition Formalization and Algorithms.pdf:pdf},
keywords = {Processes,Technical Papers -- Reasoning about Plans,and Actions},
pages = {1059--1064},
title = {{Multi-Agent Plan Recognition: Formalization and Algorithms}},
year = {2010}
}
@inproceedings{Rzeszotarski2013,
address = {New York, New York, USA},
author = {Rzeszotarski, Jeffrey M. and Kittur, Aniket},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13},
doi = {10.1145/2468356.2468675},
isbn = {9781450319522},
pages = {1779},
publisher = {ACM Press},
title = {{TouchViz}},
url = {http://dl.acm.org/citation.cfm?doid=2468356.2468675},
year = {2013}
}
@article{BenShitrit2013a,
abstract = {n this paper, we show that tracking multiple people whose paths may intersect can be formulated as a multi-commodity network flow problem. Our proposed framework is designed to exploit image appearance cues to prevent identity switches. Our method is effective even when such cues are only available at distant time intervals. This is unlike many current approaches that depend on appearance being exploitable from frame to frame. Furthermore, our algorithm lends itself to a real-time implementation. We validate our approach on three publicly available datasets that contain long and complex sequences, the APIDIS basketball dataset, the ISSIA soccer dataset and the PETS\&amp;\#8217;09 pedestrian dataset. We also demonstrate its performance on a newer basketball dataset that features complete world championship basketball matches. In all cases, our approach preserves identity better than state-of-the-art tracking algorithms},
author = {{Ben Shitrit}, Horesh and Berclaz, Jerome and Fleuret, Francois and Fua, Pascal},
doi = {10.1109/TPAMI.2013.210},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Linear Programming,Linear programming,MCNF,Multi-Commodity Network Flow,Multi-object tracking,Optimization,Radar tracking,Real-time systems,Target tracking,Tracklet association,Trajectory,basketball,tracking},
language = {English},
mendeley-tags = {basketball,tracking},
number = {99},
pages = {1--1},
publisher = {IEEE},
shorttitle = {Pattern Analysis and Machine Intelligence, IEEE Tr},
title = {{Multi-Commodity Network Flow for Tracking Multiple People}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6636296 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6636296},
volume = {PP},
year = {2013}
}
@inproceedings{Aguilera2005,
author = {Aguilera, D G and Lahoz, J G\'{o}mez and Codes, J Finat},
booktitle = {Proceedings of the ISPRS Working Group V4 Workshop 3DARCH 2005 Virtual Reconstruction and Visualization of Complex Architectures},
keywords = {3d reconstruction,geometry,photogrammetry,single image technique,vanishing points estimation},
title = {{A New Method for Vanishing Point Detection in 3D Reconstruction from a Single View}},
year = {2005}
}
@inproceedings{Willis2013,
abstract = {HideOut is a mobile projector-based system that enables new applications and interaction techniques with tangible objects and surfaces. HideOut uses a device mounted camera to detect hidden markers applied with infrared-absorbing ink. The obtrusive appearance of fiducial markers is avoided and the hidden marker surface doubles as a functional projection surface. We present example applications that demonstrate a wide range of interaction scenarios, including media navigation tools, interactive storytelling applications, and mobile games. We explore the design space enabled by the HideOut system and describe the hidden marker prototyping process. HideOut brings tangible objects to life for interaction with the physical world around us.},
address = {New York, New York, USA},
author = {Willis, Karl D. D. and Shiratori, Takaaki and Mahler, Moshe},
booktitle = {Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction - TEI '13},
doi = {10.1145/2460625.2460682},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction - TEI '13/2013/Willis, Shiratori, Mahler/Willis, Shiratori, Mahler - 2013 - HideOut.pdf:pdf},
isbn = {9781450318983},
keywords = {anamorphism},
mendeley-tags = {anamorphism},
pages = {331},
publisher = {ACM Press},
title = {{HideOut}},
url = {http://dl.acm.org/citation.cfm?id=2460682 http://dl.acm.org/citation.cfm?doid=2460625.2460682},
year = {2013}
}
@article{Phillips2013,
abstract = {PURPOSE: To develop an eye-tracking method applicable to three-dimensional (3D) images, where the abnormality is both moving and changing in size. MATERIALS AND METHODS: Research ethics committee approval was granted to record eye-tracking data from six inexperienced readers who inspected eight short (<30 seconds) endoluminal fly-through videos extracted from computed tomographic (CT) colonography examinations. Cases included true-positive and false-positive polyp detections from a previous study (polyp diameters, 5-25 mm). Eye tracking was performed with a desk-mounted tracker, and readers indicated when they saw a polyp with a mouse click. The polyp location on each video frame was quantified subsequently by using a circular mask. Gaze data related to each video frame were calculated relative to the visible polyp boundary and used to identify eye movements that pursue a polyp target as it changes size and position during fly-through. Gaze data were then related to positive polyp detections by readers. RESULTS: Tracking eye gaze on moving 3D images was technically feasible. Gaze was successfully classified by using pursuit analysis, and pursuit-based gaze metrics were able to help discriminate different reader search behaviors and methods of allocating visual attention during polyp identification. Of a total of 16 perceptual errors, 15 were recognition errors. There was only one visual search error. The largest polyp (25 mm) was seen but not recognized by five of six readers. CONCLUSION: Tracking a reader's gaze during endoluminal interpretation of 3D data sets is technically feasible and can be described with pursuit-based metrics. Perceptual errors can be classified into visual search errors and recognition errors. Recognition errors are more frequent in inexperienced readers.},
author = {Phillips, Peter and Boone, Darren and Mallett, Susan and Taylor, Stuart A and Altman, Douglas G and Manning, David and Gale, Alastair and Halligan, Steve},
doi = {10.1148/radiol.12120062},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Radiology/2013/Phillips et al/Phillips et al. - 2013 - Method for tracking eye gaze during interpretation of endoluminal 3D CT colonography technical description and.pdf:pdf},
issn = {1527-1315},
journal = {Radiology},
keywords = {Clinical Competence,Colonic Polyps,Colonic Polyps: radiography,Colonography,Computed Tomographic,Computer-Assisted,Diagnostic Errors,Eye Movements,Humans,Imaging,Radiographic Image Interpretation,Three-Dimensional,gaze analysis},
mendeley-tags = {gaze analysis},
month = jun,
number = {3},
pages = {924--31},
pmid = {23382289},
title = {{Method for tracking eye gaze during interpretation of endoluminal 3D CT colonography: technical description and proposed metrics for analysis.}},
url = {http://radiology.rsna.org/content/267/3/924.short http://www.ncbi.nlm.nih.gov/pubmed/23382289},
volume = {267},
year = {2013}
}
@article{Glenstrup1995,
abstract = {Today, the human eye-gaze can be recorded by relatively unobtrusive techniques. This thesis argues that it is possible to use the eye-gaze of a computer user in the interface to aid the control of the application. Care must be taken, though, that eye-gaze tracking data is used in a sensible way, since the nature of human eye-movements is a combination of several voluntary and involuntary cognitive processes. The main reason for eye-gaze based user interfaces being attractive is that the direction of the eye-gaze can express the interests of the user -- it is a potential porthole into the current cognitive processes -- and communication through the direction of the eyes is faster than any other mode of human communication. It is argued that eye-gaze tracking data is best used in multimodal interfaces where the user interacts with the data instead of the interface, in so-called noncommand user interfaces. Furthermore, five usability criteria for eye-gaze media are given. This thesis also sugges...},
author = {Glenstrup, Arne John and Engell-nielsen, Theo},
file = {:home/acmt/Dropbox/Documentos/Mendeley/University of Copenhagen, DK-2100/1995/Glenstrup, Engell-nielsen/Glenstrup, Engell-nielsen - 1995 - Eye controlled media Present and future state.pdf:pdf},
journal = {University of Copenhagen, DK-2100},
title = {{Eye controlled media: Present and future state}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.6067\&rep=rep1\&type=pdf},
year = {1995}
}
@article{Pai2004,
abstract = {Systematic reviews and meta-analyses synthesize data from existing primary research, and well-conducted reviews offer clinicians a practical solution to the problem of staying current in their fields of interest. A whole generation of secondary journals, pre-appraised evidence libraries and periodically updated elec- tronic texts are now available to clinicians. However, not all systematic reviews are of high quality, and it is important to be able to critically assess their validity and applicability. This article is an illustrated guide for conducting systematic reviews. A clear understanding of the process will provide clinicians with the tools to judiciously appraise reviews and interpret them. We hope that it will enable clinicians to conduct systematic reviews, generate high-quality evidence, and contribute to the evidence-based medicine movement.},
author = {Pai, M and McCulloch, M and Gorman, JD},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The National Medical Journal of India/2004/Pai, McCulloch, Gorman/Pai, McCulloch, Gorman - 2004 - Systematic reviews and meta-analyses an illustrated, step-by-step guide.pdf:pdf;:home/acmt/Dropbox/Documentos/Mendeley/The National Medical Journal of India/2004/Pai, McCulloch, Gorman/Pai, McCulloch, Gorman - 2004 - Systematic reviews and meta-analyses an illustrated, step-by-step guide(2).pdf:pdf},
journal = {The National Medical Journal of India},
keywords = {RBS},
mendeley-tags = {RBS},
number = {2},
pages = {86--95},
title = {{Systematic reviews and meta-analyses: an illustrated, step-by-step guide.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15141602},
volume = {17},
year = {2004}
}
@article{Laubrock2005,
abstract = {We compared effects of covert spatial-attention shifts induced with exogenous or endogenous cues on microsaccade rate and direction. Separate and dissociated effects were obtained in rate and direction measures. Display changes caused microsaccade rate inhibition, followed by sustained rate enhancement. Effects on microsaccade direction were differentially tied to cue class (exogenous vs. endogenous) and type (neutral vs. directional). For endogenous cues, direction effects were weak and occurred late. Exogenous cues caused a fast direction bias towards the cue (i.e., early automatic triggering of saccade programs), followed by a shift in the opposite direction (i.e, controlled inhibition of cue-directed saccades, leading to a ‘leakage’ of microsaccades in the opposite direction).},
author = {Laubrock, Jochen and Engbert, Ralf and Kliegl, Reinhold},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Vision Research/2005/Laubrock, Engbert, Kliegl/Laubrock, Engbert, Kliegl - 2005 - Microsaccade dynamics during covert attention.pdf:pdf},
journal = {Vision Research},
keywords = {Attention,Eye movements,Fixation,Reaction time,gaze analysis: microsaccade},
mendeley-tags = {gaze analysis: microsaccade},
number = {6},
pages = {721--730},
title = {{Microsaccade dynamics during covert attention}},
url = {http://www.sciencedirect.com/science/article/pii/S004269890400495X},
volume = {45},
year = {2005}
}
@inproceedings{Ryan2010,
abstract = {Analysis of recordings made by a wearable eye tracker is complicated by video stream synchronization, pupil coordinate mapping, eye movement analysis, and tracking of dynamic Areas Of Interest (AOIs) within the scene. In this paper a semi-automatic system is developed to help automate these processes. Synchronization is accomplished via side by side video playback control. A deformable eye template and calibration dot marker allow reliable initialization via simple drag and drop as well as a user-friendly way to correct the algorithm when it fails. Specifically, drift may be corrected by nudging the detected pupil center to the appropriate coordinates. In a case study, the impact of surrogate nature views on physiological health and perceived well-being is examined via analysis of gaze over images of nature. A match-moving methodology was developed to track AOIs for this particular application but is applicable toward similar future studies.},
address = {New York, New York, USA},
annote = {- cited by: 7
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Ryan, Wayne J. and Duchowski, Andrew T. and Vincent, Ellen A. and Battisto, Dina},
booktitle = {Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10},
doi = {10.1145/1743666.1743722},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10/2010/Ryan et al/Ryan et al. - 2010 - Match-moving for area-based analysis of eye movements in natural tasks.pdf:pdf},
isbn = {9781605589947},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {235},
publisher = {ACM Press},
title = {{Match-moving for area-based analysis of eye movements in natural tasks}},
url = {http://dl.acm.org/citation.cfm?id=1743722 http://portal.acm.org/citation.cfm?doid=1743666.1743722},
year = {2010}
}
@article{Inchingolo1985,
author = {Inchingolo, P and Spanio, M},
journal = {\ldots  Engineering, IEEE Transactions on},
title = {{On the identification and analysis of saccadic eye movements-A quantitative study of the processing procedures}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4122143},
year = {1985}
}
@article{Klinker1997,
abstract = {Augmented reality (AR) is a technology in which a user's view of the real world is enhanced or augmented with additional information generated from a computer model. Using AR technology, users can interact with a combination of real and virtual objects in a natural way. This paradigm constitutes the core of a very promising new technology for many applications. However, before it can be applied successfully, AR has to fulfill very strong requirements including precise calibration, registration and tracking of sensors and objects in the scene, as well as a detailed overall understanding of the scene. We see computer vision and image processing technology play an increasing role in acquiring appropriate sensor and scene models. To balance robustness with automation, we integrate automatic image analysis with both interactive user assistance and input from magnetic trackers and CAD-models. Also, in order to meet the requirements of the emerging global information society, future human-computer interaction will be highly collaborative and distributed. We thus conduct research pertaining to distributed and collaborative use of AR technology. We have demonstrated our work in several prototype applications, such as collaborative interior design, and collaborative mechanical repair. This paper describes our approach to AR with examples from applications, as well as the underlying technology.},
author = {Klinker, GJ},
file = {:home/acmt/Dropbox/Documentos/Mendeley/PRESENCE Teleoperations and Virtual Environments/1997/Klinker/Klinker - 1997 - Confluence of computer vision and interactive graphics for augmented reality.pdf:pdf},
journal = {PRESENCE: Teleoperations and Virtual Environments},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {1--26},
title = {{Confluence of computer vision and interactive graphics for augmented reality}},
url = {http://wwwbruegge.informatik.tu-muenchen.de/publications/pdf/77/klinker1997presence.pdf},
year = {1997}
}
@article{Miller2008,
abstract = {Team Cornell's Skynet is an autonomous Chevrolet Tahoe built to compete in the 2007 DARPA Urban Challenge. Skynet consists of many unique subsystems, including actuation and power distribution designed in-house, a tightly coupled attitude and position estimator, a novel obstacle detection and tracking system, a system for augmenting position estimates with vision-based detection algorithms, a path planner based on physical vehicle constraints and a nonlinear optimization routine, and a state-based reasoning agent for obeying traffic laws. This paper describes these subsystems in detail before discussing the system's overall performance in the National Qualifying Event and the Urban Challenge. Logged data recorded at the National Qualifying Event and the Urban Challenge are presented and used to analyze the system's performance. © 2008 Wiley Periodicals, Inc.},
author = {Miller, Isaac and Campbell, Mark and Huttenlocher, Dan and Kline, Frank-Robert and Nathan, Aaron and Lupashin, Sergei and Catlin, Jason and Schimpf, Brian and Moran, Pete and Zych, Noah and Garcia, Ephrahim and Kurdziel, Mike and Fujishima, Hikaru},
doi = {10.1002/rob.20253},
issn = {15564959},
journal = {Journal of Field Robotics},
month = aug,
number = {8},
pages = {493--527},
title = {{Team Cornell's Skynet: Robust perception and planning in an urban environment}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/rob.20253/abstract http://doi.wiley.com/10.1002/rob.20253},
volume = {25},
year = {2008}
}
@inproceedings{Russo2006,
abstract = {This chapter presents some of the main challenges related to the definition and construction of virtual environments for the oil exploration and production (E\&P) industry. Initially the main E\&P processes that may make good use of Virtual Reality technology are presented. Then, the main related challenges are discussed.},
address = {Bel\'{e}m},
author = {Russo, Enio Emanuel Ramos and Raposo, Alberto Barbosa and Fernando, Terrence and Gattass, Marcelo},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {313--318},
publisher = {SBC},
title = {{A Realidade Virtual na Ind\'{u}stria de Explora\c{c}\~{a}o e Produ\c{c}\~{a}o de Petr\'{o}leo}},
year = {2006}
}
@article{Wenzel2003,
author = {Wenzel, S. and Bernhard, J. and Jessen, U.},
doi = {10.1109/WSC.2003.1261489},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693)/2003/Wenzel, Bernhard, Jessen/Wenzel, Bernhard, Jessen - 2003 - A taxonomy of visualization techniques for simulation in production and logistics.pdf:pdf},
isbn = {0-7803-8131-9},
journal = {Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693)},
pages = {729--736},
publisher = {Ieee},
title = {{A taxonomy of visualization techniques for simulation in production and logistics}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1261489},
year = {2003}
}
@phdthesis{Olsson2007,
abstract = {An eye tracker makes it possible to record the gaze point of a person looking at for example a computer monitor. Modern techniques are very flexible and allow the user to behave naturally without the need of cumbersome equipment such as special contact lenses or electrical probes. This is valuable in psychological research, marketing research and Human Computer Interaction. Eye trackers also give people who are severely paralyzed and unable to type and speak means to communicate using their eyes. Measurement noise makes the use of digital filters necessary. An example is an eye-controlled cursor for a desktop environment such as Windows. The cursor has to be stable enough to allow the user to select folders, icons or other items of interest. While this type of application requires a fast real-time filter, others are less sensitive to processing time but demand an even higher level of accuracy. This work explores three areas of eye tracking filtration and aims at enhancing the performance of the filters used in the eye tracking systems built by Tobii Technology, Sweden. First, a post-processing algorithm to find fixations in raw gaze data is detailed. Second, modifications to an existing reading detection algorithm are described to make it more robust to natural irregularities in reading patterns. Third, a real-time filter for an eye-controlled cursor to be used in a desktop environment is designed using a low-pass filter in parallel with a change detector. The fixation filter produced fewer false fixations and was also able to detect fixations lying spatially closer together than the previously used filter. The reading detection algorithm was shown to be robust to natural irregularities in reading such as revisits to previously read text or skipped paragraphs. The eye-cursor filter proved to respond quicker than the previously used moving average filter while maintaining a high level of noise attenuation.},
address = {Stockholm, Sweden},
annote = {- crossRef (Larsson2010)},
author = {Olsson, Pontus},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2007/Olsson/Olsson - 2007 - Real-time and Offline Filters for Eye Tracking.pdf:pdf},
keywords = {Control Engineering,Reglerteknik,Technology,eye tracking,gaze analysis,teknik},
language = {eng},
mendeley-tags = {eye tracking,gaze analysis},
pages = {45},
publisher = {KTH Electrical Engineering},
school = {KTH Electrical Engineering},
title = {{Real-time and Offline Filters for Eye Tracking}},
url = {http://kth.diva-portal.org/smash/record.jsf?pid=diva2:573446},
year = {2007}
}
@article{Nystrom2010,
abstract = {Event detection is used to classify recorded gaze points into periods of fixation, saccade, smooth pursuit, blink, and noise. Although there is an overall consensus that current algorithms for event detection have serious flaws and that a de facto standard for event detection does not exist, surprisingly little work has been done to remedy this problem. We suggest a new velocity-based algorithm that takes several of the previously known limitations into account. Most important, the new algorithm identifies so-called glissades, a wobbling movement at the end of many saccades, as a separate class of eye movements. Part of the solution involves designing an adaptive velocity threshold that makes the event detection less sensitive to variations in noise level and the algorithm settings-free for the user. We demonstrate the performance of the new algorithm on eye movements recorded during reading and scene perception and compare it with two of the most commonly used algorithms today. Results show that, unlike the currently used algorithms, fixations, saccades, and glissades are robustly identified by the new algorithm. Using this algorithm, we found that glissades occur in about half of the saccades, during both reading and scene perception, and that they have an average duration close to 24 msec. Due to the high prevalence and long durations of glissades, we argue that researchers must actively choose whether to assign the glissades to saccades or fixations; the choice affects dependent variables such as fixation and saccade duration significantly. Current algorithms do not offer this choice, and their assignments of each glissade are largely arbitrary.},
annote = {- keyword coletado
- cited by: 65
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000
        
      },
author = {Nystr\"{o}m, Marcus and Holmqvist, Kenneth},
doi = {10.3758/BRM.42.1.188},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Behavior research methods/2010/Nystr\"{o}m, Holmqvist/Nystr\"{o}m, Holmqvist - 2010 - An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data.pdf:pdf},
issn = {1554-3528},
journal = {Behavior research methods},
keywords = {Algorithms,Eye Tracking,Fixation,Humans,Models,Ocular,Psychological,Saccades,Saccades: physiology,Segmentation,Signal Detection,gaze analysis},
mendeley-tags = {Eye Tracking,Segmentation,gaze analysis},
month = feb,
number = {1},
pages = {188--204},
pmid = {20160299},
title = {{An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data.}},
url = {http://link.springer.com/article/10.3758/BRM.42.1.188 http://www.ncbi.nlm.nih.gov/pubmed/20160299},
volume = {42},
year = {2010}
}
@article{Li2006,
author = {Li, D and Parkhurst, D},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the COGAIN Conference/2006/Li, Parkhurst/Li, Parkhurst - 2006 - Open-source software for real-time visible-spectrum eye tracking.pdf:pdf},
journal = {Proceedings of the COGAIN Conference},
keywords = {eye tracker},
mendeley-tags = {eye tracker},
title = {{Open-source software for real-time visible-spectrum eye tracking}},
url = {http://thirtysixthspan.com/openEyes/li\_parkhurst06.pdf},
year = {2006}
}
@article{Pelechano2011,
abstract = {This paper presents an Animation Planning Mediator (APM) designed to synthesize anima- tions efficiently for virtual characters in real time crowd simulation. From a set of animation clips, the APM selects the most appropriate and modifies the skeletal configuration of each character to satisfy desired constraints (e.g. eliminating foot-sliding or restricting upper body torsion), while still providing natural look- ing animations. We use a hardware accelerated character animation library to blend animations increasing the number of possible locomotion types. The APM allows the crowd simulation module to maintain control of path planning, collision avoidance and response. A key advan- tage of our approach is that the APM can be integrated with any crowd simulator working in continuous space. We show visual results achieved in real time for several hundreds of agents, as well as the quantitative accuracy.},
author = {Pelechano, Nuria and Spanlang, Bernhard and Beacco, Alejandro},
file = {::},
journal = {Computer},
keywords = {crowd animation,foot sliding},
pages = {13--19},
title = {{Avatar Locomotion in Crowd Simulation}},
url = {http://www.lsi.upc.edu/~npelechano/Pelechano\_CASA2011.pdf},
volume = {10},
year = {2011}
}
@inproceedings{Bastos2006,
abstract = {This chapter presents some interaction techniques traditionally used in Virtual Reality environments. Beyond, some of these could be used in Augmented Reality ones. As Augmented Reality systems emerge, specially tailored techniques become available. These techniques are also presented in the text.},
address = {Petropolis},
author = {Bastos, Nacha Costa and Teichrieb, Ver\^{o}nica and Kelner, Judith},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {129--148},
publisher = {SBC},
title = {{Intera\c{c}\~{a}o com Realidade Virtual e Aumentada}},
year = {2006}
}
@article{Leahu2013,
address = {New York, New York, USA},
author = {Leahu, Lucian and Cohn, Marisa and March, Wendy},
doi = {10.1145/2470654.2466455},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3331},
publisher = {ACM Press},
title = {{How categories come to matter}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466455},
year = {2013}
}
@inproceedings{Damasceno2011,
abstract = {This work shows an Augmented Reality System produced according to the needs for assessing the extent or angular motion's range, which may be used in health areas where this information is needed to ponder about the physical measuring or motor rehabilitation.},
author = {Damasceno, Eduardo Filgueiras and Cardoso, Alexandre and {Lamounier Jr.}, Edgard Afonso},
booktitle = {2011 XIII Symposium on Virtual Reality},
doi = {10.1109/SVR.2011.37},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 XIII Symposium on Virtual Reality/2011/Damasceno, Cardoso, Lamounier Jr/Damasceno, Cardoso, Lamounier Jr. - 2011 - Augmented Biophotogrammetry.pdf:pdf},
isbn = {978-1-4577-0661-5},
keywords = {augmented reality,photogrammetric analysis},
month = may,
pages = {48--55},
publisher = {IEEE},
title = {{Augmented Biophotogrammetry}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5951834 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951834},
year = {2011}
}
@inproceedings{Herbelin2007,
author = {Herbelin, Bruno and Grillon, Helena and {De Heras Ciechomski}, Pablo and Thalmann, Daniel},
booktitle = {17th International Conference on Artificial Reality and Telexistence (ICAT 2007)},
doi = {10.1109/ICAT.2007.28},
file = {:home/acmt/Dropbox/Documentos/Mendeley/17th International Conference on Artificial Reality and Telexistence (ICAT 2007)/2007/Herbelin et al/Herbelin et al. - 2007 - Coding gaze tracking data with chromatic gradients for VR Exposure Therapy.pdf:pdf},
isbn = {0-7695-3056-7},
month = nov,
pages = {7--14},
publisher = {IEEE},
title = {{Coding gaze tracking data with chromatic gradients for VR Exposure Therapy}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4414610},
year = {2007}
}
@article{Mishra2012,
abstract = {Attention is an integral part of the human visual system and has been widely studied in the visual attention literature. The human eyes fixate at important locations in the scene, and every fixation point lies inside a particular region of arbitrary shape and size, which can either be an entire object or a part of it. Using that fixation point as an identification marker on the object, we propose a method to segment the object of interest by finding the "optimal" closed contour around the fixation point in the polar space, avoiding the perennial problem of scale in the Cartesian space. The proposed segmentation process is carried out in two separate steps: First, all visual cues are combined to generate the probabilistic boundary edge map of the scene; second, in this edge map, the "optimal" closed contour around a given fixation point is found. Having two separate steps also makes it possible to establish a simple feedback between the mid-level cue (regions) and the low-level visual cues (edges). In fact, we propose a segmentation refinement process based on such a feedback process. Finally, our experiments show the promise of the proposed method as an automatic segmentation framework for a general purpose visual system.},
author = {Mishra, Ajay K and Aloimonos, Yiannis and Cheong, Loong-Fah and Kassim, Ashraf A},
doi = {10.1109/TPAMI.2011.171},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Computer-Assisted,Computer-Assisted: methods,Cues,Eye,Eye Tracking,Form Perception,Humans,Image Processing,Ocular,Ocular: physiology,Segmentation,Vision},
mendeley-tags = {Eye Tracking,Segmentation},
month = apr,
number = {4},
pages = {639--53},
pmid = {22383341},
title = {{Active visual segmentation.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5989830 http://www.ncbi.nlm.nih.gov/pubmed/22383341},
volume = {34},
year = {2012}
}
@article{Rigas2012,
abstract = {The last few years a growing research interest has aroused in the field of biometrics, concerning the use of brain dependent characteristics generally known as behavioral features. Human eyes, often referred as the gates to the soul, can possibly comprise a rich source of idiosyncratic information which may be used for the recognition of an individual’s identity. In this paper an innovative experiment and a novel processing approach for the human eye movements is implemented, ultimately aiming at the biometric segregation of individual persons. In our experiment, the subjects observe face images while their eye movements are being monitored, providing information about each participant’s attention spots. The implemented method treats eye trajectories as 2-D distributions of points on the image plane. The efficiency of graph objects in the representation of structural information motivated us on the utilization of a non-parametric multivariate graph-based measure for the comparison of eye movement signals, yielding promising results at the task of identification according to behavioral characteristics of an individual.},
author = {Rigas, Ioannis and Economou, George and Fotopoulos, Spiros},
doi = {10.1016/j.patrec.2012.01.003},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Pattern Recognition Letters/2012/Rigas, Economou, Fotopoulos/Rigas, Economou, Fotopoulos - 2012 - Biometric identification based on the eye movements and graph matching techniques.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = apr,
number = {6},
pages = {786--792},
title = {{Biometric identification based on the eye movements and graph matching techniques}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865512000062 http://linkinghub.elsevier.com/retrieve/pii/S0167865512000062},
volume = {33},
year = {2012}
}
@inproceedings{Newcombe2011,
abstract = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
author = {Newcombe, Richard a. and Davison, Andrew J. and Izadi, Shahram and Kohli, Pushmeet and Hilliges, Otmar and Shotton, Jamie and Molyneaux, David and Hodges, Steve and Kim, David and Fitzgibbon, Andrew},
booktitle = {2011 10th IEEE International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2011.6092378},
isbn = {978-1-4577-2185-4},
keywords = {3,4,8,ar,computer graphics,dense reconstruction,depth cameras,gpu,i,image genera-,image processing and com-,index terms,picture,real-time,scanning,slam,tion - digitizing and,tracking,volumetric representation},
month = oct,
pages = {127--136},
publisher = {IEEE},
title = {{KinectFusion: Real-time dense surface mapping and tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6162880},
year = {2011}
}
@article{Kosecka2005,
abstract = {Man-made environments possess many regularities which can be efficiently exploited for image-based rendering as well as robotic navigation and localization tasks. In this paper, we present an approach for automatic extraction of dominant rectangular structures from a single view and show how they facilitate the recovery of camera pose, planar structure, and matching across widely separated views. In the presented approach, the rectangular hypothesis formation is based on a higher-level information encoded by the presence of orthogonal vanishing directions, the dominant rectangular structures can be detected and matched despite the presence of multiple repetitive structures often encountered in a variety of buildings. Different stages of the approach are demonstrated on various examples of images of indoor and outdoor structured environments.},
author = {Ko\v{s}eck\'{a}, Jana and Zhang, Wei},
doi = {10.1016/j.cviu.2005.04.005},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = dec,
number = {3},
pages = {274--293},
title = {{Extraction, matching, and pose recovery based on dominant rectangular structures}},
url = {http://www.sciencedirect.com/science/article/pii/S1077314205000482 http://linkinghub.elsevier.com/retrieve/pii/S1077314205000482},
volume = {100},
year = {2005}
}
@misc{Simlox2011,
author = {Simlox},
title = {{Simlox}},
url = {http://www.systecon.se/case/C3\_SIMLOX/},
year = {2011}
}
@article{Savelsbergh2010,
abstract = {The aim of the study was to improve the estimation of the direction of the ball during penalty kicks by changing the visual search behaviour. Inexperienced goalkeepers divided into three groups moved a joystick in response to penalty kick situations presented on a large screen in pre-test, training and post-test phases of an experiment. The perceptual learning group practised with film clips that were edited to highlight relevant information in the run-up sequence of the kicker. The training group practised with the same film clips but without any highlights. A third group served as control and only performed the pre- and post-tests. The results showed that the visual search behaviour of the perceptual training group changed significantly and improved the initiation of the joystick movement. This initiation coincided with the timing of the most important visual information and led to significantly better performance than the other two groups (i.e. more penalties were stopped).},
author = {Savelsbergh, GJP and van Gastel, P.J. and van Kampen, P.M.},
journal = {International Journal of Sport Psychology},
pages = {24--41},
title = {{Anticipation of penalty kicking direction can be improved by directing attention through perceptual learning.}},
url = {http://www.cabdirect.org/abstracts/20113055154.html},
volume = {41},
year = {2010}
}
@article{Gustafson2013,
address = {New York, New York, USA},
author = {Gustafson, Sean G. and Rabe, Bernhard and Baudisch, Patrick M.},
doi = {10.1145/2470654.2466114},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {889},
publisher = {ACM Press},
title = {{Understanding palm-based imaginary interfaces}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466114},
year = {2013}
}
@misc{Eugene,
author = {Eugene},
title = {{In2AR}},
url = {http://www.in2ar.com},
urldate = {17/01/2012}
}
@article{Correll2013,
address = {New York, New York, USA},
author = {Correll, Michael a. and Alexander, Eric C. and Gleicher, Michael},
doi = {10.1145/2470654.2481373},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2697},
publisher = {ACM Press},
title = {{Quantity estimation in visualizations of tagged text}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481373},
year = {2013}
}
@inproceedings{Chen2011,
abstract = {Free viewpoint video presentation is a new challenge in multimedia analysis. This paper presents an innovative physics-based scheme to reconstruct the 3D ball trajectory from single-camera volleyball video sequences for free viewpoint virtual replay. The problem of 2D-to-3D inference is arduous due to the loss of 3D information in projection to 2D images. The proposed scheme incorporates the domain knowledge of court specification and the physical characteristics of ball motion to accomplish the 2D-to-3D inference. Motion equations with the parameters are set up to define the 3D trajectories based on physical characteristics. Utilizing the geometric transformation of camera calibration, the 2D ball coordinates extracted over frames are used to approximate the parameters of the 3D motion equations, and finally the 3D ball trajectory can be reconstructed from single-camera sequences. The experiments show promising results. The reconstructed 3D trajectory enables the free viewpoint virtual replay and enriched visual presentation, making game watching a whole new experience.},
author = {Chen, Hua-Tsung and Chou, Chien-Li and Tsai, Wen-Jiin and Lee, Suh-Yin},
booktitle = {2011 Visual Communications and Image Processing (VCIP)},
doi = {10.1109/VCIP.2011.6115930},
isbn = {978-1-4577-1322-4},
keywords = {2D-to-3D inference problem,3D ball trajectory reconstruction,3D trajectories,Cameras,Equations,Games,Image reconstruction,Mathematical model,Three dimensional displays,Trajectory,camera calibration,free viewpoint video presentation,free viewpoint virtual replay,geometric transformation,image sequences,multimedia analysis,reconstruction,single-camera sports video,single-camera volleyball video sequences,sport,video cameras},
mendeley-tags = {reconstruction},
month = nov,
pages = {1--4},
publisher = {IEEE},
shorttitle = {Visual Communications and Image Processing (VCIP),},
title = {{3D ball trajectory reconstruction from single-camera sports video for free viewpoint virtual replay}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6115930},
year = {2011}
}
@inproceedings{Macal2009,
author = {Macal, Charles M and North, Michael J},
booktitle = {Proceedings of the 2009 Winter Simulation Conference (WSC)},
doi = {10.1109/WSC.2009.5429318},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2009 Winter Simulation Conference (WSC)/2009/Macal, North/Macal, North - 2009 - Agent-based modeling and simulation.pdf:pdf},
isbn = {978-1-4244-5770-0},
month = dec,
pages = {86--98},
publisher = {IEEE},
title = {{Agent-based modeling and simulation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5429318},
year = {2009}
}
@article{Dewhurst2012,
abstract = {Eye movement sequences-or scanpaths-vary depending on the stimulus characteristics and the task (Foulsham \& Underwood Journal of Vision, 8(2), 6:1-17, 2008; Land, Mennie, \& Rusted, Perception, 28, 1311-1328, 1999). Common methods for comparing scanpaths, however, are limited in their ability to capture both the spatial and temporal properties of which a scanpath consists. Here, we validated a new method for scanpath comparison based on geometric vectors, which compares scanpaths over multiple dimensions while retaining positional and sequential information (Jarodzka, Holmqvist, \& Nystr\"{o}m, Symposium on Eye-Tracking Research and Applications (pp. 211-218), 2010). "MultiMatch" was tested in two experiments and pitted against ScanMatch (Cristino, Math\^{o}t, Theeuwes, \& Gilchrist, Behavior Research Methods, 42, 692-700, 2010), the most comprehensive adaptation of the popular Levenshtein method. In Experiment 1, we used synthetic data, demonstrating the greater sensitivity of MultiMatch to variations in spatial position. In Experiment 2, real eye movement recordings were taken from participants viewing sequences of dots, designed to elicit scanpath pairs with commonalities known to be problematic for algorithms (e.g., when one scanpath is shifted in locus or when fixations fall on either side of an AOI boundary). The results illustrate the advantages of a multidimensional approach, revealing how two scanpaths differ. For instance, if one scanpath is the reverse copy of another, the difference is in the direction but not the positions of fixations; or if a scanpath is scaled down, the difference is in the length of the saccadic vectors but not in the overall shape. As well as having enormous potential for any task in which consistency in eye movements is important (e.g., learning), MultiMatch is particularly relevant for "eye movements to nothing" in mental imagery and embodiment-of-cognition research, where satisfactory scanpath comparison algorithms are lacking.},
author = {Dewhurst, Richard and Nystr\"{o}m, Marcus and Jarodzka, Halszka and Foulsham, Tom and Johansson, Roger and Holmqvist, Kenneth},
doi = {10.3758/s13428-012-0212-2},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Behavior research methods/2012/Dewhurst et al/Dewhurst et al. - 2012 - It depends on how you look at it scanpath comparison in multiple dimensions with MultiMatch, a vector-based app.pdf:pdf},
issn = {1554-3528},
journal = {Behavior research methods},
keywords = {Adult,Algorithms,Attention,Attention: physiology,Cognition,Episodic,Eye Movements,Eye Movements: physiology,Female,Humans,Imagination,Imagination: physiology,Male,Memory,Models,Posture,Psychological,Saccades,Saccades: physiology,eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
month = dec,
number = {4},
pages = {1079--100},
pmid = {22648695},
title = {{It depends on how you look at it: scanpath comparison in multiple dimensions with MultiMatch, a vector-based approach.}},
url = {http://link.springer.com/article/10.3758/s13428-012-0212-2 http://www.ncbi.nlm.nih.gov/pubmed/22648695},
volume = {44},
year = {2012}
}
@book{duda1973pattern,
author = {Duda, Richard O and Hart, Peter E and Others},
publisher = {Wiley New York},
title = {{Pattern classification and scene analysis}},
volume = {3},
year = {1973}
}
@article{Steinicke2011,
abstract = {The display units integrated in today's head-mounted displays (HMDs) provide only a limited field of view (FOV) to the virtual world. In order to present an undistorted view to the virtual environment (VE), the perspective projection used to render the VE has to be adjusted to the limitations caused by the HMD characteristics. In particular, the geometric field of view (GFOV), which defines the virtual aperture angle used for rendering of the 3D scene, is set up according to the display field of view (DFOV). A discrepancy between these two fields of view distorts the geometry of the VE in a way that either minifies or magnifies the imagery displayed to the user. It has been shown that this distortion has the potential to affect a user's perception of the virtual space, sense of presence, and performance on visual search tasks. In this paper, we analyze the user's perception of a VE displayed in a HMD, which is rendered with different GFOVs. We introduce a psychophysical calibration method to determine the HMD's actual field of view, which may vary from the nominal values specified by the manufacturer. Furthermore, we conducted two experiments to identify perspective projections for HMDs, which are identified as natural by subjects--even if these perspectives deviate from the perspectives that are inherently defined by the DFOV. In the first experiment, subjects had to adjust the GFOV for a rendered virtual laboratory such that their perception of the virtual replica matched the perception of the real laboratory, which they saw before the virtual one. In the second experiment, we displayed the same virtual laboratory, but restricted the viewing condition in the real world to simulate the limited viewing condition in a HMD environment. We found that subjects evaluate a GFOV as natural when it is larger than the actual DFOV of the HMD--in some cases up to 50 percent--even when subjects viewed the real space with a limited field of view.},
author = {Steinicke, Frank and Bruder, Gerd and Kuhl, Scott and Willemsen, Pete and Lappe, Markus and Hinrichs, Klaus H},
doi = {10.1109/TVCG.2010.248},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE transactions on visualization and computer graphics/2011/Steinicke et al/Steinicke et al. - 2011 - Natural perspective projections for head-mounted displays.pdf:pdf},
isbn = {2010030060},
issn = {1941-0506},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Adult,Calibration,Computer-Assisted,Female,Head,Humans,Image Processing,Male,Man-Machine Systems,Middle Aged,Psychophysics,User-Computer Interface,Visual Fields,Visual Fields: physiology,Visual Perception,Visual Perception: physiology},
month = jul,
number = {7},
pages = {888--99},
pmid = {21546652},
title = {{Natural perspective projections for head-mounted displays.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5620907 http://www.ncbi.nlm.nih.gov/pubmed/21546652},
volume = {17},
year = {2011}
}
@article{Masoud2007,
abstract = {In this paper, we address the problem of recovering the intrinsic and extrinsic parameters of a camera or a group of cameras in a setting overlooking a traffic scene. Unlike many other settings, conventional camera calibration techniques are not applicable in this case. We present a method that uses certain geometric primitives commonly found in traffic scenes, such as straight and curved lanes, lane markings, and poles in order to recover calibration parameters. We show experimentally that these primitives provide the needed redundancy and are capable of achieving accurate results suitable for most traffic monitoring applications.},
author = {Masoud, Osama and Papanikolopoulos, Nikolaos P.},
doi = {10.1016/j.trc.2007.05.005},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Transportation Research Part C Emerging Technologies/2007/Masoud, Papanikolopoulos/Masoud, Papanikolopoulos - 2007 - Using geometric primitives to calibrate traffic scenes.pdf:pdf},
issn = {0968090X},
journal = {Transportation Research Part C: Emerging Technologies},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = dec,
number = {6},
pages = {361--379},
title = {{Using geometric primitives to calibrate traffic scenes}},
url = {http://www.sciencedirect.com/science/article/pii/S0968090X07000368 http://linkinghub.elsevier.com/retrieve/pii/S0968090X07000368},
volume = {15},
year = {2007}
}
@article{Zhang2002,
abstract = {This paper presents a novel approach for reconstructing free-form, texture-mapped, 3D scene models from a single painting or photograph. Given a sparse set of user-specified constraints on the local shape of the scene, a smooth 3D surface that satisfies the constraints is generated. This problem is formulated as a constrained variational optimization problem. In contrast to previous work in single-view reconstruction, our technique enables high-quality reconstructions of free-form curved surfaces with arbitrary reflectance properties. A key feature of the approach is a novel hierarchical transformation technique for accelerating convergence on a non-uniform, piecewise continuous grid. The technique is interactive and updates the model in real time as constraints are added, allowing fast reconstruction of photorealistic scene models. The approach is shown to yield high-quality results on a large variety of images.},
author = {Zhang, Li and Dugas-Phocion, Guillaume and Samson, Jean-Sebastien and Seitz, Steven M.},
doi = {10.1002/vis.291},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Journal of Visualization and Computer Animation/2002/Zhang et al/Zhang et al. - 2002 - Single-view modelling of free-form scenes.pdf:pdf},
issn = {1049-8907},
journal = {The Journal of Visualization and Computer Animation},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = sep,
number = {4},
pages = {225--235},
title = {{Single-view modelling of free-form scenes}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/vis.291/abstract http://doi.wiley.com/10.1002/vis.291},
volume = {13},
year = {2002}
}
@article{Sampaio2007,
abstract = {Introdu\c{c}\~{a}o: Agregar evid\^{e}ncias de pesquisa para guiar a pr\'{a}tica cl\'{\i}nica \'{e} uma das principais raz\~{o}es para se desenvolverem estudos que sintetizam a literatura, mas n\~{a}o \'{e} a \'{u}nica. As revis\~{o}es sistem\'{a}ticas s\~{a}o desenhadas para ser met\'{o}dicas, expl\'{\i}citas e pass\'{\i}veis de reprodu\c{c}\~{a}o. Esse tipo de estudo serve para nortear o desenvolvimento de projetos, indicando novos rumos para futuras investiga\c{c}\~{o}es e identificando quais m\'{e}todos de pesquisa foram utilizados em uma \'{a}rea. M\'{e}todos: Uma revis\~{a}o sistem\'{a}tica requer uma pergunta clara, a defini\c{c}\~{a}o de uma estrat\'{e}gia de busca, o estabelecimento de crit\'{e}rios de inclus\~{a}o e exclus\~{a}o dos artigos e, acima de tudo, uma an\'{a}lise criteriosa da qualidade da literatura selecionada. O processo de desenvolvimento desse tipo de estudo de revis\~{a}o inclui caracterizar cada estudo selecionado, avaliar a qualidade deles, identificar conceitos importantes, comparar as an\'{a}lises estat\'{\i}sticas apresentadas e concluir sobre o que a literatura informa em rela\c{c}\~{a}o a determinada interven\c{c}\~{a}o, apontando ainda problemas/quest\~{o}es que necessitam de novos estudos. Um trabalho de revis\~{a}o sistem\'{a}tica segue a estrutura de um artigo original. Conclus\~{a}o: Boas revis\~{o}es sistem\'{a}ticas s\~{a}o recursos importantes ante o crescimento acelerado da informa\c{c}\~{a}o cient\'{\i}fica. Esses estudos ajudam a sintetizar a evid\^{e}ncia dispon\'{\i}vel na literatura sobre uma interven\c{c}\~{a}o, podendo auxiliar profissionais cl\'{\i}nicos e pesquisadores no seu cotidiano de trabalho.},
author = {Sampaio, RF and Mancini, MC},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Revista Brasileira de Fisioterapia/2007/Sampaio, Mancini/Sampaio, Mancini - 2007 - Estudos de revis\~{a}o sistem\'{a}tica um guia para s\'{\i}ntese criteriosa da evid\^{e}ncia cient\'{\i}fica.pdf:pdf},
journal = {Revista Brasileira de Fisioterapia},
keywords = {ECA,RBS,revis\~{a}o sistem\'{a}tica,s\'{\i}ntese da literatura},
mendeley-tags = {RBS},
number = {1},
pages = {83--39},
title = {{Estudos de revis\~{a}o sistem\'{a}tica: um guia para s\'{\i}ntese criteriosa da evid\^{e}ncia cient\'{\i}fica}},
url = {http://bases.bireme.br/cgi-bin/wxislind.exe/iah/online/?IsisScript=iah/iah.xis\&src=google\&base=LILACS\&lang=p\&nextAction=lnk\&exprSearch=446088\&indexSearch=ID},
volume = {11},
year = {2007}
}
@article{Kumar2013b,
address = {New York, New York, USA},
author = {Kumar, Neha and Rangaswamy, Nimmi},
doi = {10.1145/2470654.2466263},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1989},
publisher = {ACM Press},
title = {{The mobile media actor-network in urban India}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466263},
year = {2013}
}
@article{Sato2013,
address = {New York, New York, USA},
author = {Sato, Takashi G. and Kamamoto, Yutaka and Harada, Noboru and Moriya, Takehiro},
doi = {10.1145/2468356.2468541},
file = {:home/acmt/Dropbox/Documentos/Mendeley/CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13/2013/Sato et al/Sato et al. - 2013 - A playback system that synchronizes the musical phrases with listener's respiration phases.pdf:pdf},
isbn = {9781450319522},
journal = {CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13},
pages = {1035},
publisher = {ACM Press},
title = {{A playback system that synchronizes the musical phrases with listener's respiration phases}},
url = {http://dl.acm.org/citation.cfm?doid=2468356.2468541},
year = {2013}
}
@inproceedings{Rau2013,
abstract = {Designing interactive learning environments (ILEs; e.g., intelligent tutoring systems, educational games, etc.) is a challenging interdisciplinary process that needs to satisfy multiple stakeholders. ILEs need to function in real educational settings (e.g., schools) in which a number of goals interact. Several instructional design methodologies exist to help developers address these goals. However, they often lead to conflicting recommendations. Due to the lack of an established methodology to resolve such conflicts, developers of ILEs have to rely on ad-hoc solutions. We present a principled methodology to resolve such conflicts. We build on a well-established design process for creating Cognitive Tutors, a highly effective type of ILE. We extend this process by integrating methods from multiple disciplines to resolve design conflicts. We illustrate our methodology's effectiveness by describing the iterative development of the Fractions Tutor, which has proven to be effective in classroom studies with 3,000 4th-6th graders.},
address = {New York, New York, USA},
author = {Rau, Martina A and Aleven, Vincent and Rummel, Nikol and Rohrbach, Stacie},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470670},
isbn = {9781450318990},
pages = {109--118},
publisher = {ACM Press},
title = {{Why interactive learning environments can have it all}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470670},
year = {2013}
}
@article{Park2007,
abstract = {We modify a projected image so as to compensate for changes in the viewer’s location. We use the concept of a virtual camera in the viewing space to achieve a transformable display with improved visibility. The 3D space and virtual camera are initialized and then the image is translated, rotated, scaled and projected. The user can modify the position and size of the image freely within the allowable projection area. They can also change its orientation as seen from their viewpoint, which can be off the axis of projection.},
author = {Park, J and Kim, MH},
doi = {10.1007/978-3-540-73281-5\_75},
journal = {Universal Access in Human-Computer Interaction. \ldots},
keywords = {anamorphism},
mendeley-tags = {anamorphism},
pages = {691--698},
title = {{Controlling an anamorphic projected image for off-axis viewing}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-73281-5\_75},
volume = {4555},
year = {2007}
}
@book{Anton2010,
author = {Anton, Howard and Rorres, Chris},
isbn = {0470432055},
pages = {773},
publisher = {Wiley},
title = {{Elementary Linear Algebra: Applications Version}},
url = {http://www.amazon.com/Elementary-Linear-Algebra-Applications-Version/dp/0470432055},
year = {2010}
}
@article{Pohl2013,
address = {New York, New York, USA},
author = {Pohl, Norman and Hodges, Steve and Helmes, John and Villar, Nicolas and Paek, Tim},
doi = {10.1145/2470654.2466194},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13/2013/Pohl et al/Pohl et al. - 2013 - An interactive belt-worn badge with a retractable string-based input mechanism.pdf:pdf},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1465},
publisher = {ACM Press},
title = {{An interactive belt-worn badge with a retractable string-based input mechanism}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466194},
year = {2013}
}
@article{Barwolff2012,
author = {B\"{a}rwolff, G and Chen, M and Huth, F},
journal = {Pedestrian and \ldots},
title = {{Methods for Modeling and Simulation of Multi-Destination Pedestrian Crowds}},
url = {https://svn.vsp.tu-berlin.de/repos/public-svn/publications/vspwp/2011/11-23/20120511accepted.pdf},
year = {2012}
}
@inproceedings{Raskar2006,
abstract = {Projectors are currently undergoing a transformation as they evolve from static output devices to portable, environment-aware, com- municating systems. An enhanced projector can determine and re- spond to the geometry of the display surface, and can be used in an ad-hoc cluster to create a self-configuring display. Information display is such a prevailing part of everyday life that new and more flexible ways to present data are likely to have significant impact. This paper examines geometrical issues for enhanced projectors, re- lating to customized projection for different shapes of display sur- face, object augmentation, and co-operation between multiple units. We introduce a new technique for adaptive projection on non- planar surfaces using conformal texture mapping. We describe ob- ject augmentation with a hand-held projector, including interaction techniques. We describe the concept of a display created by an ad-hoc cluster of heterogeneous enhanced projectors, with a new global alignment scheme, and new parametric image transfer meth- ods for quadric surfaces, to make a seamless projection. The work is illustrated by several prototypes and applications.},
address = {New York, New York, USA},
author = {Raskar, Ramesh and van Baar, Jeroen and Beardsley, Paul and Willwacher, Thomas and Rao, Srinivas and Forlines, Clifton},
booktitle = {ACM SIGGRAPH 2006 Courses on - SIGGRAPH '06},
doi = {10.1145/1185657.1185802},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM SIGGRAPH 2006 Courses on - SIGGRAPH '06/2006/Raskar et al/Raskar et al. - 2006 - iLamps geometrically aware and self-configuring projectors.pdf:pdf},
isbn = {1595933646},
keywords = {ad-hoc clusters,anamorphism,augmented reality,calibration,keystone,projector,quadric transfer,seamless display},
mendeley-tags = {anamorphism,keystone},
month = jul,
pages = {7},
publisher = {ACM Press},
title = {{iLamps: geometrically aware and self-configuring projectors}},
url = {http://dl.acm.org/citation.cfm?id=1185657.1185802 http://portal.acm.org/citation.cfm?doid=1185657.1185802},
year = {2006}
}
@article{Kaufmann2013,
address = {New York, New York, USA},
author = {Kaufmann, Bonifaz and Ahlstr\"{o}m, David},
doi = {10.1145/2470654.2466434},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Map navigation,handheld projector,peephole interaction,spatial memory,touch interaction},
pages = {3173},
publisher = {ACM Press},
title = {{Studying spatial memory and map navigation performance on projector phones with peephole interaction}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466434},
year = {2013}
}
@article{Cardoso2012,
author = {Cardoso, Alexandre and Junior, Edgard Afonso Lamounier and Faro, Andre De Mattos},
doi = {10.1109/SVR.2012.23},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Cardoso, Junior, Faro/Cardoso, Junior, Faro - 2012 - The Use of Virtual Reality for Simulation and Training of Bovine Artificial Insemination with Haptic Devi.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {artificial,simulation,training,virtual reality},
month = may,
pages = {66--73},
publisher = {Ieee},
title = {{The Use of Virtual Reality for Simulation and Training of Bovine Artificial Insemination with Haptic Devices}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297561},
year = {2012}
}
@inproceedings{Faugeras,
abstract = {This article deals with the problem of recovering the three trifocal tensors between three views from a set of point correspondences. We give a new way of deriving the trifocal tensor based on Grassmann-Cayley algebra that sheds some new light on its structure and leads to a complete characterization of its geometric and algebraic properties which is fairly institute, i.e. geometric. We give a set of algebraic constraints satisfied by the 27 coefficients of the trifocal tensor which allow to parameterize it minimally with 18 coefficients. We then describe a robust method for estimating the trifocal tensor from point and line correspondences that uses this minimal parameterization. Experimental results show that this method as superior to the linear methods which had been previously published},
author = {Faugeras, O. and Papadopoulo, T.},
booktitle = {Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)},
doi = {10.1109/ICCV.1998.710761},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)/Unknown/Faugeras, Papadopoulo/Faugeras, Papadopoulo - Unknown - A nonlinear method for estimating the projective geometry of 3 views.pdf:pdf},
isbn = {81-7319-221-9},
pages = {477--484},
publisher = {Narosa Publishing House},
title = {{A nonlinear method for estimating the projective geometry of 3 views}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=710761\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=.QT.projective+geometry.QT.}
}
@article{Veneri2011,
abstract = {Eye movement is the simplest and repetitive movement that enables humans to interact with the environment. The common daily activities, such as reading a book or watching television, involve this natural activity, which consists of rapidly shifting our gaze from one region to another. In clinical application, the identification of the main components of eye movement during visual exploration, such as fixations and saccades, is the objective of the analysis of eye movements: however, in patients affected by motor control disorder the identification of fixation is not banal. This work presents a new fixation identification algorithm based on the analysis of variance and covariance: the main idea was to use bivariate statistical analysis to compare variance over x and y to identify fixation. We describe the new algorithm, and we compare it with the common fixations algorithm based on dispersion. To demonstrate the performance of our approach, we tested the algorithm in a group of healthy subjects and patients affected by motor control disorder.},
annote = {- cited by: 2
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Veneri, Giacomo and Piu, Pietro and Rosini, Francesca and Federighi, Pamela and Federico, Antonio and Rufa, Alessandra},
doi = {10.1016/j.patrec.2011.06.012},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Pattern Recognition Letters/2011/Veneri et al/Veneri et al. - 2011 - Automatic eye fixations identification based on analysis of variance and covariance.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
month = oct,
number = {13},
pages = {1588--1593},
title = {{Automatic eye fixations identification based on analysis of variance and covariance}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865511001887 http://linkinghub.elsevier.com/retrieve/pii/S0167865511001887},
volume = {32},
year = {2011}
}
@inproceedings{Machado2007,
abstract = {This chapter presents the main technologies related to the use of interaction devices for touch and force feedback in virtual and augmented reality systems. Thus, devices and control packages that compose haptic systems are presented, as well as their use in some applications.},
address = {Petropolis},
author = {Machado, Liliane dos Santos},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2007/Machado/Machado - 2007 - Dispositivos H\'{a}pticos para Interfaces de Realidade Virtual e Aumentada.pdf:pdf},
pages = {152--167},
publisher = {SBC},
title = {{Dispositivos H\'{a}pticos para Interfaces de Realidade Virtual e Aumentada}},
year = {2007}
}
@inproceedings{Salvucci2000,
abstract = {The process of fixation identification—separating and labeling fixations and saccades in eye-tracking protocols—is an essential part of eye-movement data analysis and can have a dramatic impact on higher-level analyses. However, algorithms for performing fixation identification are often described informally and rarely compared in a meaningful way. In this paper we propose a taxonomy of fixation identification algorithms that classifies algorithms in terms of how they utilize spatial and temporal information in eye-tracking protocols. Using this taxonomy, we describe five algorithms that are representative of different classes in the taxonomy and are based on commonly employed techniques. We then evaluate and compare these algorithms with respect to a number of qualitative characteristics. The results of these comparisons offer interesting implications for the use of the various algorithms in future work.},
address = {New York, New York, USA},
annote = {- keyword coletado
        
Resumo: Salvucci [@Salvucci2000] introduz uma taxonomia de algoritmos de identifica\c{c}\~{a}o de fixa\c{c}\~{o}es e sacadas. Esta taxonomia \'{e} baseada em como s\~{a}o usadas as informa\c{c}\~{o}es de tempo e espa\c{c}o. Os algoritmos citados no artigo representam classes de t\'{e}cnicas que compartilham algum crit\'{e}rio de identifica\c{c}\~{a}o. Ele tamb\'{e}m apresenta uma forma de analisar os algoritmos de maneira qualitativa: facilidade de uso, velocidade de interpreta\c{c}\~{a}o, acur\'{a}cia, robustez, e parametriza\c{c}\~{a}o.
        
        I-VT:
Vantagens:        
- f\'{a}cil de implementar
- eficiente
- pode ser executado em tempo real
        Desvantagens:        
- inst\'{a}vel em pontos com velocidade pr\'{o}xima do threshold (precisa lidar com o ru\'{\i}do do equipamento e movimentos do olhar  irrelevantes para a pesquisa).
- Pode provocar altern\^{a}ncias entre classifica\c{c}\~{o}es, implicando em fixa\c{c}\~{o}es e sacadas com poucos pontos, aumentando o n\'{u}mero de fixa\c{c}\~{o}es exclu\'{\i}das pelo crit\'{e}rio de dura\c{c}\~{a}o m\'{\i}nima. 
- N\~{a}o \'{e} robusto.
- Persegui\c{c}\~{o}es podem ser classificados como fixa\c{c}\~{o}es ou sacadas dependendo de sua velocidade.
        
        I-HMM
Vantagens:
        - Modelo probabil\'{\i}stico ao inv\'{e}s de um threshold. Utiliza informa\c{c}\~{a}o sequencial (os vizinhos influenciam o ponto).
- \'{E} mais robusto na presen\c{c}a do ru\'{\i}do.
- Pode expandir o diagrama de estados (incorporando mais movimentos do olhar)
- \'{E} executado em tempo linear e pode ser executado em tempo real.
        Desvantagens:        
- Mais complexo que I-VT.
- Procedimento de reestimar os par\^{a}metros tamb\'{e}m \'{e} complexo.
                  
I-DT
Vantagens:        
- Algoritmo simples
- Tempo linear;
- Tempo real;
- Evita a           
Desvantagens:        
- Par\^{a}metros interdependentes (ex: dura\c{c}\~{a}o m\'{\i}nima alta e limiar de dispers\~{a}o baixa pode n\~{a}o classificar nenhuma fixa\c{c}\~{a}o)
- Sens\'{\i}vel a ru\'{\i}do (pode ultrapassar o limiar)
                  
I-MST
Vantagens:        
- Robusto (pode usar vari\^{a}ncia e m\'{e}dia para lidar com ru\'{\i}do)
- Cria clusters de fixa\c{c}\~{o}es
- 
                  
Desvantagens:        
- Lento (tempo de execu\c{c}\~{a}o exponencial)
- Para cada ponto adicionado, \'{e} necess\'{a}rio achar o ponto mais pr\'{o}ximo dentre v\'{a}rios para restruturar o cluster e separar os clusters.
                  
I - AOI
Vantagens:        
- Tempo real
- Simples
                  
Desvantagens:        
- N\~{a}o lida bem com sacadas (inclu\'{\i}das nas fixa\c{c}\~{o}es se estiverem dentro das regi\~{o}es), aumentando a dura\c{c}\~{a}o da fixa\c{c}\~{a}o.
- Longas sacadas s\~{a}o consideradas fixa\c{c}\~{o}es nas regi\~{o}es intermedi\'{a}rias.
- Depende da aplica\c{c}\~{a}o (distribui\c{c}\~{a}o das regi\~{o}es)},
author = {Salvucci, Dario D. and Goldberg, Joseph H.},
booktitle = {Proceedings of the symposium on Eye tracking research \& applications - ETRA '00},
doi = {10.1145/355017.355028},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the symposium on Eye tracking research \& applications - ETRA '00/2000/Salvucci, Goldberg/Salvucci, Goldberg - 2000 - Identifying fixations and saccades in eye-tracking protocols.pdf:pdf},
isbn = {1581132808},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
pages = {71--78},
publisher = {ACM Press},
title = {{Identifying fixations and saccades in eye-tracking protocols}},
url = {http://dl.acm.org/citation.cfm?id=355028 http://portal.acm.org/citation.cfm?doid=355017.355028},
year = {2000}
}
@article{Sicilia2007,
abstract = {The term systematic review is used to refer to a specific methodology of research, developed in order to gather and evaluate the available evidence pertaining to a focused topic. It represents a secondary study that depends on primary study results to be accomplished. Several primary studies have been conducted in the field of Software Engineering in the last years, determining an increasing improvement in methodology. However, in most cases software is built with technologies and processes for which developers have insufficient evidence to confirm their suitability, limits, qualities, costs, and inherent risks. Conducting systematic reviews in Software Engineering consists in a major methodological tool to scientifically improve the validity of assertions that can be made in the field and, as a consequence, the reliability degree of the methods that are employed for developing software technologies and supporting software processes. This paper aims at discussing the significance of experimental studies, particularly systematic reviews, and their use in supporting software processes. A template designed to support systematic reviews in Software Engineering is presented, and the development of ontologies to describe knowledge regarding such experimental studies is also introduced.},
author = {Biolchini, Jorge Calmon de Almeida and Mian, Paula Gomes and Natali, Ana Candida Cruz and Conte, Tayana Uch\^{o}a and Travassos, Guilherme Horta},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Advanced Engineering Informatics/2007/Biolchini et al/Biolchini et al. - 2007 - Scientific research ontology to support systematic review in software engineering.pdf:pdf},
journal = {Advanced Engineering Informatics},
keywords = {Experimental software engineering,Experimental study,Ontology,Scientific method,Systematic review,rbs},
mendeley-tags = {rbs},
number = {2},
pages = {133--151},
title = {{Scientific research ontology to support systematic review in software engineering}},
url = {http://www.sciencedirect.com/science/article/pii/S147403460600070X},
volume = {21},
year = {2007}
}
@inproceedings{Sukthankar2005,
abstract = {Traditional desktop computing paradigms provide a poor interface for interacting with intelligent physical spaces. Although handheld devices are an important platform for interface agents, their displays are inadequate for many pervasive computing tasks and need to be supplemented by larger high-resolution displays. We propose the notion of augmenting indoor intelligent environments with ambient projection, where large numbers of projectors simultaneously illuminate the environment from multiple directions - analogous to the way in which ambient lighting permeates a room. Ambient projection could enable any suitable surface in an environment to be employed as a display device. Using such displays, the intelligent environment could present high-resolution information, proactively alert users who are not carrying handheld devices and annotate objects in the environment without instrumentation. Several challenges must be solved before such projected displays become a practical solution. This paper provides an overview of our research in computer vision for enabling interactive ambient projected displays.},
author = {Sukthankar, R},
booktitle = {Computer Vision for Interactive and Intelligent Environment (CVIIE'05)},
doi = {10.1109/CVIIE.2005.20},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer Vision for Interactive and Intelligent Environment (CVIIE'05)/2005/Sukthankar/Sukthankar - 2005 - Towards Ambient Projection for Intelligent Environments.pdf:pdf},
isbn = {0-7695-2524-5},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
pages = {162--172},
publisher = {IEEE},
title = {{Towards Ambient Projection for Intelligent Environments}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1623778 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1623778},
year = {2005}
}
@inproceedings{Barreto2005a,
abstract = {When deploying a heterogeneous camera network or when we use cheap zoom cameras like in cell-phones, it is not practical, if not impossible to off-line calibrate the radial distortion of each camera using reference objects. It is rather desirable to have an automatic procedure without strong assumptions about the scene. In this paper, we present a new algorithm for estimating the epipolar geometry of two views where the two views can be radially distorted with different distortion factors. It is the first algorithm in the literature solving the case of different distortion in the left and right view linearly and without assuming the existence of lines in the scene. Points in the projective plane are lifted to a quadric in three-dimensional projective space. A radial distortion of the projective plane results to a matrix transformation in the space of lifted coordinates. The new epipolar constraint depends linearly on a 4 × 4 radial fundamental matrix which has 9 degrees of freedom. A complete algorithm is presented and tested on real imagery.},
author = {Barreto, J.P. and Daniilidis, K.},
booktitle = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
doi = {10.1109/ICCV.2005.103},
isbn = {0-7695-2334-X},
pages = {625--632 Vol. 1},
publisher = {IEEE},
title = {{Fundamental matrix for cameras with radial distortion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1541312},
year = {2005}
}
@techreport{Fischler1981a,
abstract = {The central theme of our research is the recovery of information about the three-dimensional structure and physical characteristics of surfaces depicted in an image -- their shapes, locations, and photometric properties. The main obstacle to surface recovery is the confounding of the desired properties in the sensory data: images are inherentlly ambiguous. Our approach to resolving this ambiguity rests on the application of generic, low-level knowledge (e.g., such basic assumptions as surface continuity and general position) to constrain the interpretation. The problem may be viewed as that of decomposing the image into its physically meaningful constituents -- surface orientation, reflectance, illumination, and so on. The 'intrinsic image model' provides a conceptual and computational framework in which this view is made explicit. Surface perception plays a fundamental role in early visual processing, both in humans and in machines. Work on surface perception has focused on the discrimination of edge types (e.g., extremal boundary or cast shadow), on the three-dimensional interpretation of edges, and on surface reconstruction by interpolating from edges and using texture geometry.},
author = {Fischler, Martin A and Witkin, Andrew P},
institution = {SRI INTERNATIONAL MENLO PARK CA},
keywords = {Edges,Image Processing,Information Processing,Remote Detectors,Shadows,Target Classification,Terrain Avoidance,Terrain Models,Texture,Visual Perception,three dimensional model library},
pages = {20},
title = {{Recovering Intrinsic Scene Characteristics from Images}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord\&metadataPrefix=html\&identifier=ADA110757},
year = {1981}
}
@inproceedings{Dehghan2012,
abstract = {Single camera-based multiple-person tracking is often hindered by difficulties such as occlusion and changes in appearance. In this paper, we address such problems by proposing a robust part-based tracking-by-detection framework. Human detection using part models has become quite popular, yet its extension in tracking has not been fully explored. Our approach learns part-based person-specific SVM classifiers which capture the articulations of the human bodies in dynamically changing appearance and background. With the part-based model, our approach is able to handle partial occlusions in both the detection and the tracking stages. In the detection stage, we select the subset of parts which maximizes the probability of detection, which significantly improves the detection performance in crowded scenes. In the tracking stage, we dynamically handle occlusions by distributing the score of the learned person classifier among its corresponding parts, which allows us to detect and predict partial occlusions, and prevent the performance of the classifiers from being degraded. Extensive experiments using the proposed method on several challenging sequences demonstrate state-of-the-art performance in multiple-people tracking.},
address = {Providence, RI},
author = {Dehghan, a. and Oreifej, O. and Hand, E. and Shah, M.},
booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247879},
isbn = {978-1-4673-1228-8},
month = jun,
pages = {1815--1821},
publisher = {IEEE},
title = {{Part-based multiple-person tracking with partial occlusion handling}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6247879},
year = {2012}
}
@article{Junwen2008,
abstract = {We present a system that simultaneously tracks eyes and detects eye blinks. Two interactive particle filters are used for this purpose, one for the closed eyes and the other one for the open eyes. Each particle filter is used to track the eye locations as well as the scales of the eye subjects. The set of particles that gives higher confidence is defined as the primary set and the other one is defined as the secondary set. The eye location is estimated by the primary particle filter, and whether the eye status is open or closed is also decided by the label of the primary particle filter. When a new frame comes, the secondary particle filter is reinitialized according to the estimates from the primary particle filter. We use autoregression models for describing the state transition and a classification-based model for measuring the observation. Tensor subspace analysis is used for feature extraction which is followed by a logistic regression model to give the posterior estimation. The performance is carefully evaluated from two aspects: the blink detection rate and the tracking accuracy. The blink detection rate is evaluated using videos from varying scenarios, and the tracking accuracy is given by comparing with the benchmark data obtained using the Vicon motion capturing system. The setup for obtaining benchmark data for tracking accuracy evaluation is presented and experimental results are shown. Extensive experimental evaluations validate the capability of the algorithm.},
author = {Wu, Junwen and Trivedi, Mohan M},
doi = {10.1155/2008/823695},
file = {:home/acmt/Dropbox/Documentos/Mendeley/EURASIP Journal on Advances in Signal Processing/2008/Wu, Trivedi/Wu, Trivedi - 2008 - Simultaneous Eye Tracking and Blink Detection with Interactive Particle Filters.pdf:pdf},
issn = {1687-6180},
journal = {EURASIP Journal on Advances in Signal Processing},
keywords = {blink,gaze analysis},
mendeley-tags = {blink,gaze analysis},
number = {1},
pages = {823695},
title = {{Simultaneous Eye Tracking and Blink Detection with Interactive Particle Filters}},
url = {http://scholar.google.com.br/scholar?start=10\&q=morris+detection+blink+2002\&hl=pt-BR\&as\_sdt=0,5\#5 http://asp.eurasipjournals.com/content/2008/1/823695},
volume = {2008},
year = {2008}
}
@article{Cesarelli2000,
abstract = {Visual acuity in congenital nystagmus has proven to be primarily related to the duration of foveation periods, during which the image of a target falls onto the fovea and eye velocity slows down. It was found that the longer the foveation time the higher the visual acuity. However, the cycle-to-cycle variability of the eye position and velocity during foveation periods also contribute to visual acuity. A high variability of the eye position during the foveations hinders a stable placement of the target image on the centralmost fovea and consequently decreases visual acuity. To investigate the relationship between different nystagmus features and visual acuity, infrared- oculographic and electro-oculographic eye position recordings of 20 patients affected by congenital nystagmus were analysed in different gaze positions. In several patients' recordings, a high variability of the eye position during foveations (i.e. greater than 0.5°) was detected. Correspondingly, low visual acuity was measured, in spite of sufficiently long foveation periods. The standard deviation of eye positions during foveation periods was used to measure this variability and it was found to be correlated to visual acuity, in conjunction with the mean duration of the foveation periods. On the basis of the data analysis, an exponential relationship is proposed to relate visual acuity and the standard deviation of the eye position during foveations.},
author = {Cesarelli, M and Bifulco, P and Loffredo, L and Bracale, M},
doi = {10.1023/A:1002702609387},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Documenta Ophthalmologica/2000/Cesarelli et al/Cesarelli et al. - 2000 - Relationship between visual acuity and eye position variability during foveations in congenital nystagmus.pdf:pdf},
journal = {Documenta Ophthalmologica},
keywords = {gaze analysis,nystagmus},
mendeley-tags = {gaze analysis,nystagmus},
number = {1},
pages = {59--72},
title = {{Relationship between visual acuity and eye position variability during foveations in congenital nystagmus}},
url = {http://link.springer.com/article/10.1023/A:1002702609387},
volume = {101},
year = {2000}
}
@article{Sun1995,
author = {Sun, F and Chen, L},
file = {::},
journal = {Neural Networks, 1995. Proceedings., IEEE  \ldots},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Identification of fixations in reading eye movements by a multi-layer neural network}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=487721},
year = {1995}
}
@article{Drusch2012,
abstract = {In this paper, we propose a new method for comparing scanpaths in a bottom-up approach, and a test of the scanpath theory. To do so, we conducted a laboratory experiment in which 113 participants were invited to accomplish a set of tasks on two different websites. For each site, they had to perform two tasks that had to be repeated ounce. The data were analyzed using a procedure similar to the one used by Duchowski et al. [8]. The first step was to automatically identify, then label, AOIs with the mean-shift clustering procedure [19]. Then, scanpaths were compared two by two with a modified version of the string-edit method, which take into account the order of AOIs visualizations [2]. Our results show that scanpaths variability between tasks but within participants seems to be lower than the variability within task for a given participant. In other words participants seem to be more coherent when they perform different tasks, than when they repeat the same tasks. In addition, participants view more of the same AOI when they perform a different task on the same Web page than when they repeated the same task. These results are quite different from what predicts the scanpath theory.},
author = {Drusch, Gautier and Bastien, J M Christian},
doi = {10.3233/WOR-2012-0353-1559},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Work (Reading, Mass.)/2012/Drusch, Bastien/Drusch, Bastien - 2012 - Analyzing Web pages visual scanpaths between and within tasks variability.pdf:pdf},
issn = {1875-9270},
journal = {Work (Reading, Mass.)},
keywords = {eye tracking,scanpaty,similarity},
mendeley-tags = {eye tracking,scanpaty,similarity},
month = jan,
pages = {1559--66},
pmid = {22316937},
title = {{Analyzing Web pages visual scanpaths: between and within tasks variability.}},
url = {http://iospress.metapress.com/index/05J21W761850217T.pdf http://www.ncbi.nlm.nih.gov/pubmed/22316937},
volume = {41 Suppl 1},
year = {2012}
}
@article{Gama2012,
author = {Gama, Alana Da and Chaves, Thiago and Figueiredo, Lucas and Teichrieb, Veronica},
doi = {10.1109/SVR.2012.15},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Gama et al/Gama et al. - 2012 - Guidance and Movement Correction Based on Therapeutics Movements for Motor Rehabilitation Support Systems.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {rehabilitation},
month = may,
pages = {191--200},
publisher = {Ieee},
title = {{Guidance and Movement Correction Based on Therapeutics Movements for Motor Rehabilitation Support Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297576},
year = {2012}
}
@inproceedings{Newman2004,
abstract = {Augmented Reality (AR) provides a natural interface to the "calm" pervasive technology anticipated in large-scale Ubiquitous Computing environments. However, the range of classic AR applications has been limited by the scope, range and cost of sensors used for tracking. Hybrid tracking approaches can go some way to extending this range. We propose an approach, called Ubiquitous Tracking, in which data from widespread and diverse heterogeneous tracking sensors is automatically and dynamically fused, and then transparently provided to applications. A formal model represents spatial relationships between objects as a graph attributed with quality-of-service parameters. This paper presents a software implementation, in which a dynamic data flow network of distributed software components is thereby constructed in response to queries and optimisation criteria specified by applications. This implementation is demonstrated using a small laboratory example, and larger setups modelled in a simulation environment.},
address = {Arlington},
author = {Newman, Joseph and Wagner, Martin and Bauer, Martin and MacWilliams, A. and Pintaric, Thomas and Beyer, Dagmar and Pustka, Daniel and Strasser, Franz and Schmalstieg, Dieter and Klinker, Gudrun},
booktitle = {Third IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2004.62},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Third IEEE and ACM International Symposium on Mixed and Augmented Reality/2004/Newman et al/Newman et al. - 2004 - Ubiquitous Tracking for Augmented Reality.pdf:pdf},
isbn = {0-7695-2191-6},
number = {Ismar},
pages = {192--201},
publisher = {IEEE},
title = {{Ubiquitous Tracking for Augmented Reality}},
url = {http://portal.acm.org/citation.cfm?id=1033716 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1383056},
year = {2004}
}
@misc{Ambierra2013,
author = {Ambierra},
keywords = {SBGames},
mendeley-tags = {SBGames},
title = {{Ambierra irrEdit}},
url = {http://www.ambiera.com/irredit/},
urldate = {2013/07/26},
year = {2013}
}
@inproceedings{Fu2011,
abstract = {In basketball games, screen is a blocking move performed by an offensive player, who stands beside or behind a defender, in order to free a teammate to shoot, to receive a pass, or to drive in to score. Screen is the fundamental essence that most offensive tactics are executed with. In this paper, a screen- strategy analysis system is designed, and through combining the identified screens, what tactics are executed in basketball games can be speculated. The proposed system is capable of court region detection, camera calibration and player extraction. Player trajectories are computed by a Kalman filter-based tracking method and mapped to the real-world court coordinates. The player position/trajectory information greatly assists professional-oriented applications such as screen-strategy analysis and tactic inference. The experiments on broadcast basketball videos show encouraging results.},
author = {Fu, Tsung-Sheng and Chen, Hua-Tsung and Chou, Chien-Li and Tsai, Wen-Jiin and Lee, Suh-Yin},
booktitle = {2011 Visual Communications and Image Processing (VCIP)},
doi = {10.1109/VCIP.2011.6115927},
isbn = {978-1-4577-1322-4},
keywords = {Accuracy,Calibration,Cameras,Color,Games,Kalman filter-based tracking method,Kalman filters,Streaming media,Trajectory,basketball,basketball games,broadcast basketball videos,camera calibration,court region detection,feature extraction,object tracking,player extraction,player tracking,player trajectories,real-world court coordinates,screen-strategy analysis,tracking},
mendeley-tags = {basketball,tracking},
month = nov,
pages = {1--4},
publisher = {IEEE},
shorttitle = {Visual Communications and Image Processing (VCIP),},
title = {{Screen-strategy analysis in broadcast basketball video using player tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6115927},
year = {2011}
}
@article{Xia2007,
author = {Xia, D and Ruan, Z},
journal = {\ldots  Computing, 2007. SNPD 2007. Eighth ACIS  \ldots},
title = {{IR image based eye gaze estimation}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4287506},
year = {2007}
}
@article{Duchowski2000,
author = {Duchowski, AT and Shivashankaraiah, V},
file = {::},
journal = {\ldots Eye tracking research \& \ldots},
title = {{Binocular eye tracking in virtual reality for inspection training}},
url = {http://dl.acm.org/citation.cfm?id=355031},
year = {2000}
}
@article{Abernethy1999,
author = {Abernethy, Bruce and Wood, Joanne M. and Parks, Sheri},
doi = {10.1080/02701367.1999.10608050},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Research Quarterly for Exercise and Sport/1999/Abernethy, Wood, Parks/Abernethy, Wood, Parks - 1999 - Can the Anticipatory Skills of Experts Be Learned by Novices.pdf:pdf},
issn = {0270-1367},
journal = {Research Quarterly for Exercise and Sport},
keywords = {anticipation,expertise,perception,training},
mendeley-tags = {anticipation,expertise,perception,training},
month = sep,
number = {3},
pages = {313--318},
title = {{Can the Anticipatory Skills of Experts Be Learned by Novices?}},
url = {http://www.tandfonline.com/doi/abs/10.1080/02701367.1999.10608050},
volume = {70},
year = {1999}
}
@article{Holone2013,
address = {New York, New York, USA},
author = {Holone, Harald and Herstad, Jo},
doi = {10.1145/2470654.2481401},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2903},
publisher = {ACM Press},
title = {{Three tensions in participatory design for inclusion}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481401},
year = {2013}
}
@inproceedings{Galgani2009,
abstract = {Several studies have analyzed the link between mental dysfunctions and eye movements, using eye tracking techniques to determine where a person is looking, that is, the fixations. In this paper, we present a novel methodology to improve current diagnosis and evaluation methods of attention disorders. We have developed and tested several data-mining methodologies suitable for the automatic analysis and visualization of eye tracking data. In particular three novel methods of classification of subjects are proposed: (i) a method that uses expectation maximization to classify according to statistical likelihood of fixations locations; (ii) a procedure based on the Levenshtein distance method to compare sequences of fixations; and (iii) a method based on the analysis of the transitions frequencies of fixations between regions. Results of evaluation of classification accuracy are finally presented.},
address = {Nashville, TN},
author = {Galgani, Filippo and Sun, Yiwen and Lanzi, Pier Luca and Leigh, Jason},
booktitle = {2009 IEEE Symposium on Computational Intelligence and Data Mining},
doi = {10.1109/CIDM.2009.4938649},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2009 IEEE Symposium on Computational Intelligence and Data Mining/2009/Galgani et al/Galgani et al. - 2009 - Automatic analysis of eye tracking data for medical diagnosis.pdf:pdf},
isbn = {978-1-4244-2765-9},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = mar,
pages = {195--202},
publisher = {IEEE},
title = {{Automatic analysis of eye tracking data for medical diagnosis}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4938649 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4938649},
year = {2009}
}
@inproceedings{Chen2011a,
abstract = {Rendering virtual objects into real scenes with real illumination can greatly increase the realism of virtual objects and the consistency between the virtual and the real. The main challenge lies in illumination estimation from a single image. This article proposes a novel method of single image based illumination estimation for lighting virtual object in real scene. Only a single image, without any knowledge of the 3D geometry or reflectance, is needed, which greatly increases the applicability of the method. We first estimate coarse scene geometry and intrinsic components including shading image and reflectance image. Then the sparse radiance map of the scene is inferred based on the scene geometry and intrinsic components. Finally, the virtual objects are illuminated by the estimated sparse radiance map. Some experimental results show that this method can convincingly light virtual objects into a single real image, without any pre-recorded 3D geometry and reflectance, illumination acquisition equipments or imaging information of the image.},
author = {Chen, Xiaowu and Wang, Ke and Jin, Xin},
booktitle = {2011 12th International Conference on Computer-Aided Design and Computer Graphics},
doi = {10.1109/CAD/Graphics.2011.19},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 12th International Conference on Computer-Aided Design and Computer Graphics/2011/Chen, Wang, Jin/Chen, Wang, Jin - 2011 - Single Image Based Illumination Estimation for Lighting Virtual Object in Real Scene.pdf:pdf},
isbn = {978-1-4577-1079-7},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = sep,
pages = {450--455},
publisher = {IEEE},
title = {{Single Image Based Illumination Estimation for Lighting Virtual Object in Real Scene}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6062827 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6062827},
year = {2011}
}
@inproceedings{Hoiem2005b,
abstract = {Many computer vision algorithms limit their performance by ignoring the underlying 3D geometric structure in the image. We show that we can estimate the coarse geometric properties of a scene by learning appearance-based models of geometric classes, even in cluttered natural scenes. Geometric classes describe the 3D orientation of an image region with respect to the camera. We provide a multiple-hypothesis framework for robustly estimating scene structure from a single image and obtaining confidences for each geometric label. These confidences can then be used to improve the performance of many other applications. We provide a thorough quantitative evaluation of our algorithm on a set of outdoor images and demonstrate its usefulness in two applications: object detection and automatic single-view reconstruction.},
author = {Hoiem, D and Efros, AA and Hebert, M},
booktitle = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
doi = {10.1109/ICCV.2005.107},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1/2005/Hoiem, Efros, Hebert/Hoiem, Efros, Hebert - 2005 - Geometric context from a single image(2).pdf:pdf;:home/acmt/Dropbox/Documentos/Mendeley/Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1/2005/Hoiem, Efros, Hebert/Hoiem, Efros, Hebert - 2005 - Geometric context from a single image.pdf:pdf},
isbn = {0-7695-2334-X},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {654--661 Vol. 1},
publisher = {IEEE},
title = {{Geometric context from a single image}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1541316 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1541316},
year = {2005}
}
@book{Wedel2008,
abstract = {In the last decade there has been a rapid growth in commercial applications of eye-tracking technology to assess the effectiveness of visual marketing efforts. Eye-movements are tightly coupled with visual attention which makes them eminent indicators of the covert visual attention process. Now a sizable and growing body of literature exists on attention to visual marketing stimuli. Eye-Tracking for Visual Marketing provides: 1. The foundations of visual attention and eye-tracking; 2. A conceptual framework for eyetracking research in marketing; 3. A review of the marketing literature within this conceptual framework. Motivated from its rising importance in marketing practice and its potential for theoretical contribution, Eye-Tracking for Visual Marketing examines the structure of the eye, the visual brain, eye-movements, and methods for recording and analyzing them. Next, it describes the authors' theory and reviews eye-tracking applications in marketing based on this theory. It conclude with an outlook on future theory and method development and recommendations for practice.},
annote = {- cited by: 35
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Wedel, Michel and Pieters, Rik},
doi = {10.1561/9781601981554},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2008/Wedel, Pieters/Wedel, Pieters - 2008 - Eye tracking for visual marketing.pdf:pdf},
isbn = {9781601981547},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {93},
publisher = {Now Publishers Inc},
title = {{Eye tracking for visual marketing}},
url = {http://books.google.com.br/books?hl=pt-BR\&lr=\&id=\_yjIeqbC8fMC\&oi=fnd\&pg=PA1\&dq=identification+fixation+eye\&ots=ohmt0F8jvc\&sig=hAnJXqqRZC0NbTNsBDJxKuMrdrM},
year = {2008}
}
@inproceedings{Riera2012,
abstract = {One of the weaknesses in augmented reality (AR) scenes is that, most of the times, the virtual model lacks realism and is not integrated sufficiently into the scene to be credible. The problem lies in lighting conditions differences from real environment and virtual objects superimposed. This problem takes on special significance indoors, to visualize interior design proposals, because a non-integrated scene may seem unlikely and unattractive. In this paper, we describe a teaching experience conducted in the framework of an interior design specialization course. It was carried out by students from the School of Building Construction of Barcelona in November 2011, at the Polytechnic University of Catalonia. Students were required to use AR freeware tools for modeling and visualize their own proposals indoor. The challenging task was to make virtual objects blend harmoniously into the real scene, so they should appear as if they were like real. New modeling techniques have been experienced in order to add lighting conditions maps to virtual objects. We used for that radiance and global illumination simulations from the real environment to get real scene lighting information. Experience was useful to determine the consistency of illumination between the virtual objects and the surrounding real objects in the scene, as well as, to evaluate the importance of the use of this technology in interior design proposals and architectural visualization.},
address = {Madrid},
author = {Riera, AS and Redondo, E and Fonseca, D},
booktitle = {7th Iberian Conference on Information Systems and Technologies (CISTI)},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
pages = {1--6},
publisher = {IEEE},
title = {{Lighting simulation in augmented reality scenes: Teaching experience in interior design}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6263163},
year = {2012}
}
@book{Cassandras2010,
author = {Cassandras, Christos G. and Lafortune, Stephane},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2010/Cassandras, Lafortune/Cassandras, Lafortune - 2010 - Introduction to Discrete Event Systems.pdf:pdf},
isbn = {1441941193},
pages = {800},
publisher = {Springer},
title = {{Introduction to Discrete Event Systems}},
url = {http://www.amazon.com/Introduction-Discrete-Systems-Christos-Cassandras /dp/1441941193},
year = {2010}
}
@article{Singh2008,
abstract = {Interactive virtual worlds feature dynamic characters that must navigate through a variety of landscapes populated with various obstacles and other agents. The process of navigating to a desired lo- cation within a dynamic environment is the problem of steering. While there are many approaches to steering, to our knowledge there is no standard way of evaluating and comparing the quality of such solutions. To address this, we propose a diverse set of benchmarks and a flexi- ble method of evaluation that can be used to compare different steering algorithms. We discuss the challenges and criteria for objectively eval- uating steering behaviors and describe the metrics and scoring method used in our benchmark evaluation. We hope that, with constructive feed- back from the community, our framework will eventually evolve into a standard and comprehensive approach to debug, compare and provide an overall assessment of the effectiveness of steering algorithms.},
author = {Singh, Shawn and Naik, Mishali and Kapadia, Mubbasir and Faloutsos, Petros and Reinman, Glenn},
doi = {10.1007/978-3-540-89220-5\_20},
journal = {Motion in Games},
pages = {200--209},
title = {{Watch Out! A Framework for Evaluating Steering Behaviors}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-89220-5\_20},
volume = {5277},
year = {2008}
}
@article{Kr2013,
author = {Kr, Antonio and Gmbh, Dfki},
isbn = {9781450318990},
pages = {2137--2146},
title = {{A Study on Icon Arrangement by Smartphone Users ¨}},
year = {2013}
}
@inproceedings{Soares2006,
abstract = {To achieve higher realism and interaction levels, every time more powerful resources are developed and available for the community. This chapter is going to discuss about techniques to produce high quality imagery based in supercomputers, specially commodity computer graphic cluster (VR-Cluster). It is also going to be covered immersive high-resolution multiprojection systems, tracking systems and development libraries for virtual reality application.},
address = {Bel\'{e}m},
author = {Soares, Luciano Pereira and Cabral, Marcio Calixto and Zuffo, Marcelo Knorich},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {51--58},
publisher = {SBC},
title = {{Sistemas Avan\c{c}ados de Realidade Virtual}},
year = {2006}
}
@article{Ambrosini2011,
abstract = {When observing someone else acting on an object, people implement goal-specific eye movement programs that are driven by their own motor representation of the observed action. Usually, however, we observe people acting in contexts where more objects, different in shape and size, are present. Is our brain able to select the intended target even when there are different objects in the visual scene? And if this is the case, what kind of information does our motor system capitalize on? We recorded eye movements while participants observed an actor reaching for and grasping one of two objects requiring two different kinds of grip to be picked up. In a control condition, the actor merely reached for and touched one of the two objects without preshaping her hand according to the target features. Results showed higher accuracy and earlier saccadic movements when participants observed an actually grasping hand than when they observed a mere reaching hand devoid of any kind of target-related preshaping. This clearly suggests that the hand preshaping provided the observer with enough motor cues to proactively and reliably saccade toward the object to be grasped, thus identifying it even when the action target was not previously known. Our findings strongly corroborate the direct matching hypothesis suggesting that in processing others' actions, we take advantage of the same motor knowledge that enables us to efficiently perform those actions.},
annote = {- cited by: 18
- kw: identification I-VT eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Ambrosini, E},
doi = {10.​1152/​jn.​00118.​2011},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of neurophysiology/2011/Ambrosini/Ambrosini - 2011 - Grasping with the eyes.pdf:pdf},
journal = {Journal of neurophysiology},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
number = {3},
pages = {1437--1442},
title = {{Grasping with the eyes}},
url = {http://jn.physiology.org/content/106/3/1437.short},
volume = {106},
year = {2011}
}
@inproceedings{Lee2004,
abstract = {Projection technology typically places several constraints on the geometric relationship between the projector and the projection surface to obtain an undistorted, properly sized image. In this paper we describe a simple, robust, fast, and low-cost method for automatic projector calibration that eliminates many of these constraints. We embed light sensors in the target surface, project Gray-coded binary patterns to discover the sensor locations, and then prewarp the image to accurately fit the physical features of the projection surface. This technique can be expanded to automatically stitch multiple projectors, calibrate onto non-planar surfaces for object decoration, and provide a method for simple geometry acquisition.},
address = {New York, New York, USA},
author = {Lee, Johnny C. and Dietz, Paul H. and Maynes-Aminzade, Dan and Raskar, Ramesh and Hudson, Scott E.},
booktitle = {Proceedings of the 17th annual ACM symposium on User interface software and technology - UIST '04},
doi = {10.1145/1029632.1029653},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 17th annual ACM symposium on User interface software and technology - UIST '04/2004/Lee et al/Lee et al. - 2004 - Automatic projector calibration with embedded light sensors.pdf:pdf},
isbn = {1581139578},
keywords = {anamorphism},
mendeley-tags = {anamorphism},
pages = {123},
publisher = {ACM Press},
title = {{Automatic projector calibration with embedded light sensors}},
url = {http://dl.acm.org/citation.cfm?id=1029653 http://portal.acm.org/citation.cfm?doid=1029632.1029653},
year = {2004}
}
@inproceedings{Sandin2005,
abstract = {Virtual reality (VR) has long been hampered by the gear needed to make the experience possible; specifically, stereo glasses and tracking devices. Autostereoscopic display devices are gaining popularity by freeing the user from stereo glasses, however few qualify as VR displays. The Electronic Visualization Laboratory (EVL) at the University of Illinois at Chicago (UIC) has designed and produced a large scale, high resolution head-tracked barrier-strip autostereoscopic display system that produces a VR immersive experience without requiring the user to wear any encumbrances. The resulting system, called Varrier, is a passive parallax barrier 35-panel tiled display that produces a wide field of view, head-tracked VR experience. This paper presents background material related to parallax barrier autostereoscopy, provides system configuration and construction details, examines Varrier interleaving algorithms used to produce the stereo images, introduces calibration and testing, and discusses the camera-based tracking subsystem.},
address = {New York, New York, USA},
author = {Sandin, Daniel J. and Margolis, Todd and Ge, Jinghua and Girado, Javier and Peterka, Tom and DeFanti, Thomas A.},
booktitle = {ACM SIGGRAPH 2005 Papers on - SIGGRAPH '05},
doi = {10.1145/1186822.1073279},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM SIGGRAPH 2005 Papers on - SIGGRAPH '05/2005/Sandin et al/Sandin et al. - 2005 - The Varrier TM autostereoscopic virtual reality display.pdf:pdf},
pages = {894},
publisher = {ACM Press},
title = {{The Varrier TM autostereoscopic virtual reality display}},
url = {http://dl.acm.org/citation.cfm?id=1073279 http://portal.acm.org/citation.cfm?doid=1186822.1073279},
year = {2005}
}
@misc{Adobe,
author = {Adobe},
title = {{Adobe Flash}},
url = {http://www.adobe.com/products/flashplayer.html},
urldate = {17/01/2012}
}
@inproceedings{Debevec1998,
address = {New York, New York, USA},
author = {Debevec, Paul},
booktitle = {Proceedings of the 25th annual conference on Computer graphics and interactive techniques - SIGGRAPH '98},
doi = {10.1145/280814.280864},
isbn = {0897919998},
pages = {189--198},
publisher = {ACM Press},
title = {{Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography}},
url = {http://portal.acm.org/citation.cfm?doid=280814.280864},
year = {1998}
}
@inproceedings{ebm,
author = {Lee, Jae-Young and Yoo, Suk I},
booktitle = {Proc. of the 2002 International Conference on Imaging Science, Systems, and Technology},
organization = {Citeseer},
title = {{An elliptical boundary model for skin color detection}},
year = {2002}
}
@book{Bungartz2013,
author = {Bungartz, Hans-Joachim and Zimmer, Stefan and Buchholz, Martin and Pfl\"{u}ger, Dirk},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2013/Bungartz et al/Bungartz et al. - 2013 - Modeling and Simulation An Application-Oriented Introduction (Springer Undergraduate Texts in Mathematics and T.pdf:pdf},
isbn = {3642395236},
keywords = {modeling and simulation},
mendeley-tags = {modeling and simulation},
pages = {407},
publisher = {Springer},
title = {{Modeling and Simulation: An Application-Oriented Introduction (Springer Undergraduate Texts in Mathematics and Technology)}},
url = {http://www.amazon.com/Modeling-Simulation-Application-Oriented-Introduction-Undergraduate/dp/3642395236},
year = {2013}
}
@book{Hartley2003a,
abstract = {A basic problem in computer vision is to understand the structure of a real world scene. This book covers relevant geometric principles and how to represent objects algebraically so they can be computed and applied. Recent major developments in the theory and practice of scene reconstruction are described in detail in a unified framework. Richard Hartley and Andrew Zisserman provide comprehensive background material and explain how to apply the methods and implement the algorithms. First Edition HB (2000): 0-521-62304-9},
author = {Hartley, Richard},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2003/Hartley/Hartley - 2003 - Multiple View Geometry in Computer Vision.pdf:pdf},
isbn = {1139449141},
pages = {655},
publisher = {Cambridge University Press},
title = {{Multiple View Geometry in Computer Vision}},
url = {http://books.google.com/books?id=si3R3Pfa98QC\&pgis=1},
year = {2003}
}
@inproceedings{O'Neill2013,
abstract = {This paper describes an ethnographic study of an outsourced business process - the digitization of healthcare forms. The aim of the study was to understand how the work is currently organized, with an eye to uncovering the research challenges which need to be addressed if that work is to be crowdsourced. The findings are organised under four emergent themes: Workplace Ecology, Data Entry Skills and Knowledge, Achieving Targets and Collaborative Working. For each theme a description of how the work is undertaken in the outsourcer's Indian office locations is given, followed by the implications for crowdsourcing that work. This research is a first step in understanding how crowdsourcing might be applied to BPO activities. The paper examines features specific to form digitization - extreme distribution and form decomposition - and lightly touches on the crowdsourcing of BPO work more generally.},
address = {New York, New York, USA},
author = {O'Neill, Jacki and Roy, Shourya and Grasso, Antonietta and Martin, David},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470683},
isbn = {9781450318990},
pages = {197--206},
publisher = {ACM Press},
title = {{Form digitization in BPO: from outsourcing to crowdsourcing?}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470683},
year = {2013}
}
@article{Seo2007,
author = {Seo, Byung-Kuk and Lee, Moon-Hyun and Park, Hanhoon and Park, Jong-Il and {Soo Kim}, Young},
doi = {10.1109/ICAT.2007.42},
file = {:home/acmt/Dropbox/Documentos/Mendeley/17th International Conference on Artificial Reality and Telexistence (ICAT 2007)/2007/Seo et al/Seo et al. - 2007 - Direct-Projected AR Based Interactive User Interface for Medical Surgery.pdf:pdf},
isbn = {0-7695-3056-7},
journal = {17th International Conference on Artificial Reality and Telexistence (ICAT 2007)},
month = nov,
pages = {105--112},
publisher = {Ieee},
title = {{Direct-Projected AR Based Interactive User Interface for Medical Surgery}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4414622},
year = {2007}
}
@article{Collewijn1984,
author = {Collewijn, H and Tamminga, EP},
file = {::},
journal = {The Journal of physiology},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Human smooth and saccadic eye movements during voluntary pursuit of different target motions on different backgrounds.}},
url = {http://jp.physoc.org/content/351/1/217.short},
year = {1984}
}
@incollection{Bednarik2013,
abstract = {Inference about high-level cognitive states during interaction is a fundamental task in building proactive intelligent systems that would allow effective offloading of mental operations to a computational architecture. We introduce an improved machine-learning pipeline able to predict user interactive behavior and performance using real-time eye-tracking. The inference is carried out using a support-vector machine (SVM) on a large set of features computed from eye movement data that are linked to concurrent high-level behavioral codes based on think aloud protocols. The differences between cognitive states can be inferred from overt visual attention patterns with accuracy over chance levels, although the overall accuracy is still low. The system can also classify and predict performance of the problem-solving users with up to 79 \% accuracy. We suggest this prediction model as a universal approach for understanding of gaze in complex strategic behavior. The findings confirm that eye movement data carry important information about problem solving processes and that proactive systems can benefit from real-time monitoring of visual attention.},
address = {London},
author = {Bednarik, Roman and Eivazi, S and Vrzakova, H},
booktitle = {Eye Gaze in Intelligent User Interfaces},
chapter = {7},
doi = {10.1007/978-1-4471-4784-8\_7},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Eye Gaze in Intelligent User Interfaces/2013/Bednarik, Eivazi, Vrzakova/Bednarik, Eivazi, Vrzakova - 2013 - A Computational Approach for Prediction of Problem-Solving Behavior Using Support Vector Machines an.pdf:pdf},
isbn = {978-1-4471-4784-8},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {111--134},
publisher = {Springer London},
title = {{A Computational Approach for Prediction of Problem-Solving Behavior Using Support Vector Machines and Eye-Tracking Data}},
url = {http://link.springer.com/chapter/10.1007/978-1-4471-4784-8\_7},
year = {2013}
}
@inproceedings{Eiselein2013,
abstract = {In this paper we present a method of improving a human detector by means of crowd density information. Human detection is especially challenging in crowded scenes which makes it important to introduce additional knowledge into the detection process. We compute crowd density maps in order to estimate the spatial distribution of people in the scene and show how it is possible to enhance the detection results of a state-of-the-art human detector by this information. The proposed method applies a self-adaptive, dynamic parametrization and as an additional contribution uses scene-adaptive learning of the human aspect ratio in order to reduce false positive detections in crowded areas. We evaluate our method on videos from different datasets and demonstrate how our system achieves better results than the baseline algorithm.},
author = {Eiselein, Volker and Fradi, Hajer and Keller, Ivo and Sikora, Thomas and Dugelay, Jean-Luc},
booktitle = {10th IEEE International Conference on Advanced Video and Signal Based Surveillance},
doi = {10.1109/AVSS.2013.6636610},
isbn = {978-1-4799-0703-8},
month = aug,
pages = {19--24},
publisher = {IEEE},
title = {{Enhancing human detection using crowd density measures and an adaptive correction filter}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6636610},
year = {2013}
}
@inproceedings{Lee2012,
abstract = {Surveillance system to improve safety and security is a major demand for the management and control of public area. Crowd management and control system requires a situation recognition technique which can predict accidents and provide alarms to the monitoring personnel. In this paper, we propose an abnormal behavior detection technique by using trajectory extraction of moving objects in video. Abnormal behavior includes running persons. The proposed abnormal behavior detection system separates background and foreground using Gaussian mixture model. And then, foreground image is used to generate the trajectories of moving objects using a Kanade-Lucas-Tomasi algorithm of the optical flow method. In addition, noise removal step is added to improve the accuracy of the created trajectory. From the trajectory of moving objects information, such as length, pixel, coordinate and moving degree is extracted. As the result of the estimation of abnormal behavior, objects' behavior is configured and analyzed based on a priori specified scenarios, such as running persons. In the results, proposed system is able to detect the abnormal behavior in public area.},
address = {Incheon},
author = {Lee, Jae-Jung and Kim, Gyu-Jin and Kim, Moon-Hyun},
booktitle = {9th International Conference \& Expo on Emerging Technologies for a Smarter World (CEWIT)},
doi = {10.1109/CEWIT.2012.6606979},
isbn = {978-1-4673-2504-2},
keywords = {- feature extraction,klt,similarity,trajectory},
month = nov,
pages = {1--5},
publisher = {IEEE},
title = {{Trajectory extraction for abnormal behavior detection in public area}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6606979},
year = {2012}
}
@inproceedings{Kinsman2010,
abstract = {The classification of a large number of images is a familiar problem to the image processing community. It occurs in consumer photography, bioinformatics, biomedical imaging, surveillance, and in the field of mobile eye-tracking studies. During eye-tracking studies, what a person looks at is recorded, and for each frame what the person looked at must then be analyzed and classified. In many cases the data analysis time restricts the scope of the studies. This paper describes the initial use of hierarchical clustering of these images to minimize the time required during analysis. Pre-clustering the images allows the user to classify a large number of images simultaneously. The success of this method is dependent on meeting requirements for human-computer-interactions, which are also discussed.},
address = {Rochester, NY},
annote = {- cited by: 2
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Kinsman, Thomas and Bajorski, Peter and Pelz, Jeff B.},
booktitle = {2010 Western New York Image Processing Workshop},
doi = {10.1109/WNYIPW.2010.5649742},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2010 Western New York Image Processing Workshop/2010/Kinsman, Bajorski, Pelz/Kinsman, Bajorski, Pelz - 2010 - Hierarchical image clustering for analyzing eye tracking videos.pdf:pdf},
isbn = {978-1-4244-9298-5},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = nov,
pages = {58--61},
publisher = {IEEE},
title = {{Hierarchical image clustering for analyzing eye tracking videos}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5649742 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5649742},
year = {2010}
}
@inproceedings{Vatavu2013,
abstract = {We show that large consensus exists among users in the way they articulate stroke gestures at various scales (i.e., small, medium, and large), and formulate a simple rule that estimates the user-intended scale of input gestures with 87\% accuracy. Our estimator can enhance current gestural interfaces by leveraging scale as a natural parameter for gesture input, reflective of user perception (i.e., no training required). Gesture scale can simplify gesture set design, improve gesture-to-function mappings, and reduce the need for users to learn and for recognizers to discriminate unnecessary symbols.},
address = {New York, New York, USA},
author = {Vatavu, Radu-daniel and Casiez, G\'{e}ry and Grisoni, Laurent},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470692},
isbn = {9781450318990},
pages = {277--284},
publisher = {ACM Press},
title = {{Small, medium, or large?: estimating the user-perceived scale of stroke gestures}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470692},
year = {2013}
}
@article{Wu2010,
abstract = {This study is to investigate the fundamental problems of, (1) facial feature detection and localization, especially eye features; and (2) eye dynamics, including tracking and blink detection. We first describe our contribution to eye localization. Following that, we discuss a simultaneous eye tracking and blink detection system. Facial feature detection is solved in a general object detection framework and its performance for eye localization is presented. A binary tree representation based on feature dependency partitions the object feature space in a coarse to fine manner. In each compact feature subspace, independent component analysis (ICA) is used to get the independent sources, whose probability density functions (PDFs) are modeled by Gaussian mixtures. When applying this representation for the task of eye detection, a subwindow is used to scan the entire image and each obtained image patch is examined using Bayesian criteria to determine the presence of an eye subject. After the eyes are automatically located with binary tree-based probability learning, interactive particle filters are used for simultaneously tracking the eyes and detecting the blinks. The particle filters use classification-based observation models, in which the posterior probabilities are evaluated by logistic regressions in tensor subspaces. Extensive experiments are used to evaluate the performance from two aspects, (1) blink detection rate and the accuracy of blink duration in terms of the frame numbers; (2) eye tracking accuracy. We also present an experimental setup for obtaining the benchmark data in tracking accuracy evaluation. The experimental evaluation demonstrates the capability of this approach.},
author = {Wu, Junwen and Trivedi, Mohan M.},
doi = {10.1145/1671962.1671964},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM Transactions on Multimedia Computing, Communications, and Applications/2010/Wu, Trivedi/Wu, Trivedi - 2010 - An eye localization, tracking and blink pattern recognition system.pdf:pdf},
issn = {15516857},
journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
keywords = {blink,gaze analysis},
mendeley-tags = {blink,gaze analysis},
month = mar,
number = {2},
pages = {1--23},
title = {{An eye localization, tracking and blink pattern recognition system}},
url = {http://dl.acm.org/citation.cfm?id=1671964 http://portal.acm.org/citation.cfm?doid=1671962.1671964},
volume = {6},
year = {2010}
}
@book{Russell2009,
author = {Russell, Stuart and Norvig, Peter},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2009/Russell, Norvig/Russell, Norvig - 2009 - Artificial Intelligence A Modern Approach (3rd Edition)(2).pdf:pdf},
isbn = {0136042597},
keywords = {artificial intelligence},
mendeley-tags = {artificial intelligence},
pages = {1152},
publisher = {Prentice Hall},
title = {{Artificial Intelligence: A Modern Approach (3rd Edition)}},
url = {http://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597},
year = {2009}
}
@inproceedings{Bichlmeier2007,
abstract = {The need to improve medical diagnosis and reduce invasive surgery is dependent upon seeing into a living human system. The use of diverse types of medical imaging and endoscopic instruments has provided significant breakthroughs, but not without limiting the surgeon's natural, intuitive and direct 3D perception into the human body. This paper presents a method for the use of augmented reality (AR) for the convergence of improved perception of 3D medical imaging data (mimesis) in context to the patient's own anatomy (in-situ) incorporating the physician's intuitive multi- sensory interaction and integrating direct manipulation with endoscopic instruments. Transparency of the video images recorded by the color cameras of a video see-through, stereoscopic head- mounted-display (HMD) is adjusted according to the position and line of sight of the observer, the shape of the patient's skin and the location of the instrument. The modified video image of the real scene is then blended with the previously rendered virtual anatomy. The effectiveness has been demonstrated in a series of experiments at the Chirurgische Klinik in Munich, Germany with cadaver and in-vivo studies. The results can be applied for designing medical AR training and educational applications.},
address = {Nara-JP},
author = {Bichlmeier, Christoph and Wimmer, Felix and Heining, Sandro Michael and Navab, Nassir},
booktitle = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2007.4538837},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Burns/Unknown/Bichlmeier et al/Bichlmeier et al. - Unknown - Medical Augmented Reality Contextual Anatomic Mimesis.pdf:pdf},
isbn = {978-1-4244-1749-0},
month = nov,
pages = {1--10},
publisher = {IEEE},
title = {{Contextual Anatomic Mimesis Hybrid In-Situ Visualization Method for Improving Multi-Sensory Depth Perception in Medical Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538837},
year = {2007}
}
@inproceedings{Ali2010,
abstract = {Human detection and tracking in high density crowds is an unsolved problem. Standard preprocessing techniques such as background modeling fail when most of the scene is in motion. Because of high levels of occlusion, dense features, and shadows, object detectors tend to produce large numbers of false detections. We introduce a new method based on 3D head plane estimation that reduces these false detections while preserving high detection rates. Our algorithm learns the head plane from observations of human heads, without any a priori extrinsic camera calibration information. In an experimental evaluation, we show that the head plane estimation technique dramatically improves the performance of a pedestrian tracker for dense crowds based on a Viola and Jones AdaBoost cascade classifier for head detection, a particle filter for tracking, and color histograms for appearance modeling.},
address = {Singapore},
author = {Ali, Irshad and Dailey, Matthew N.},
booktitle = {2010 11th International Conference on Control Automation Robotics \& Vision},
doi = {10.1109/ICARCV.2010.5707425},
isbn = {978-1-4244-7814-9},
keywords = {trajectory extraction},
mendeley-tags = {trajectory extraction},
month = dec,
pages = {2054--2059},
publisher = {IEEE},
title = {{Head plane estimation improves the accuracy of pedestrian tracking in dense crowds}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5707425},
year = {2010}
}
@article{Hirooka2006,
abstract = {In this paper, we propose a novel virtual display system for a real object surface by using a video projector, so that the viewer can feel as if digital images are printed on the real surface with arbitrary shape. This system consists of an uncalibrated camera and video projector connected to a same PC and creates a virtual object by rendering 2D contents preserved beforehand onto a white object in a real world via a projector. For geometry registration between the rendered image and the object surface correctly, we regard the object surface as a set of a number of small rectangular regions and perform geometry registration by calculating homographies between the projector image plane and the each divided regions. By using such a homography-based method, we can avoid calibration of a camera and a projector that is necessary in a conventional method. In this system, we perform following two processes. First of all, we acquire the status of the object surface from images which capture the scene that color-coded checker patterns are projected on it and generate image rendered on it without distortion by calculating homographies. After once the projection image is generated, the rendered image can be updated if the object surface moves, or refined when it is stationary by observing the object surface. By this second process, the system always offers more accurate display. In implementation, we demonstrate our system in various conditions. This system enables it to project them as if it is printed on a real paper surface of a book. By using this system, we expect the realization of a virtual museum or other industrial application.},
author = {Hirooka, S and Saito, H},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEICE transactions on information and systems/2006/Hirooka, Saito/Hirooka, Saito - 2006 - Calibration free virtual display system using video projector onto real object surface.pdf:pdf},
journal = {IEICE transactions on information and systems},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
number = {1},
pages = {88--97},
title = {{Calibration free virtual display system using video projector onto real object surface}},
url = {http://search.ieice.org/bin/summary.php?id=e89-d\_1\_88},
volume = {E89-D},
year = {2006}
}
@phdthesis{Kasprowski2004a,
annote = {- cited by: 19- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Kasprowski, P},
booktitle = {Faculty of Automatic Control, Electronics and \ldots},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Faculty of Automatic Control, Electronics and \ldots/2004/Kasprowski/Kasprowski - 2004 - Human identification using eye movements.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {111},
school = {Silesian University of Technology},
title = {{Human identification using eye movements}},
url = {http://www.kasprowski.pl/phd/PhD\_Kasprowski.pdf},
year = {2004}
}
@inproceedings{Mahapatra2008,
abstract = {The importance of motion in attracting attention is well known. While watching videos, where motion is prevalent, how do we quantify the regions that are motion salient? In this paper, we investigate the role of motion in attention and compare it with the influence of other low-level features like image orientation and intensity. We propose a framework for motion saliency. In particular, we integrate motion vector information with spatial and temporal coherency to generate a motion attention map. The results show that our model achieves good performance in identifying regions that are moving and salient. We also find motion to have greater influence on saliency than other low-level features when watching videos.},
author = {Mahapatra, Dwarikanath and Winkler, Stefan and Yen, Shih-Cheng},
booktitle = {Human Vision and Electronic Imaging},
doi = {10.1117/12.766243},
editor = {Rogowitz, Bernice E. and Pappas, Thrasyvoulos N.},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Human Vision and Electronic Imaging/2008/Mahapatra, Winkler, Yen/Mahapatra, Winkler, Yen - 2008 - Motion saliency outweighs other low-level features while watching videos.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = feb,
pages = {68060P--68060P--10},
title = {{Motion saliency outweighs other low-level features while watching videos}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=810996},
year = {2008}
}
@article{Hennessey2009,
abstract = {Binocular eye-gaze tracking can be used to estimate the point-of-gaze (POG) of a subject in real-world 3-D space using the vergence of the eyes. In this paper, a novel noncontact model-based technique for 3-D POG estimation is presented. The noncontact system allows people to select real-world objects in 3-D physical space using their eyes, without the need for head-mounted equipment. Remote 3-D POG estimation may be especially useful for persons with quadriplegia or Amyotrophic Lateral Sclerosis. It would also enable a user to select 3-D points in space generated by 3-D volumetric displays, with potential applications to medical imaging and telesurgery. Using a model-based POG estimation algorithm allows for free head motion and a single stage of calibration. It is shown that an average accuracy of 3.93 cm was achieved over a workspace volume of 30 x 23 x 25 cm (W x H x D) with a maximum latency of 1.5 s due to the digital filtering employed. The users were free to naturally move and reorient their heads while operating the system, within an allowable headspace of 3 cm x 9 cm x 14 cm.},
author = {Hennessey, Craig and Lawrence, Peter},
doi = {10.1109/TBME.2008.2005943},
issn = {1558-2531},
journal = {IEEE transactions on bio-medical engineering},
keywords = {Adult,Algorithms,Biological,Calibration,Equipment Design,Eye Movement Measurements,Eye Movements,Eye Movements: physiology,Female,Fixation,Gaze 3D,Head Movements,Head Movements: physiology,Humans,Male,Models,Ocular,Sensitivity and Specificity},
mendeley-tags = {Gaze 3D},
month = mar,
number = {3},
pages = {790--9},
pmid = {19272927},
title = {{Noncontact binocular eye-gaze tracking for point-of-gaze estimation in three dimensions.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4633699 http://www.ncbi.nlm.nih.gov/pubmed/19272927},
volume = {56},
year = {2009}
}
@article{Henderson1971,
author = {Henderson, L F},
doi = {10.1038/229381a0},
issn = {0028-0836},
journal = {Nature},
keywords = {crowd simulation,dynamics},
mendeley-tags = {crowd simulation,dynamics},
month = feb,
number = {5284},
pages = {381--3},
pmid = {16059256},
shorttitle = {Nature},
title = {{The statistics of crowd fluids.}},
url = {http://dx.doi.org/10.1038/229381a0},
volume = {229},
year = {1971}
}
@inproceedings{Lee2013,
abstract = {A video see-through head mounted display (HMD) has a different viewing point than does the real eye, resulting in visual displacement (VD). VD deteriorates visuomotor performance due to sensory conflict. Previous work has investigated this deterioration and human adaptation by comparing fixed VD and real eye conditions. In this study we go a step further to investigate whether any differences in visuomotor and adaptation trends exist across 16 distinct VD conditions. The performance tasks studied were of two types: foot placement and finger touch. In contrast to our initial prediction, the results showed equal task performance levels and adaptation within about 5 minutes regardless of VD conditions. We found that human adaptation covered a variety of VDs --- up to 55 mm in the X, Y direction; up to 125mm in the Z direction; and up to 140mm of interocular distance (IOD). In addition, we found that partial adaptation gave participants the interesting experience of a sense of body structure distortion for a few minutes.},
address = {New York, New York, USA},
author = {Lee, Joong Ho and Kim, Sei-young and Yoon, Hae Cheol and Huh, Bo Kyung and Park, Ji-Hyung},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470698},
isbn = {9781450318990},
pages = {309--312},
publisher = {ACM Press},
title = {{A preliminary investigation of human adaptations for various virtual eyes in video see-through HMDS}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470698},
year = {2013}
}
@inproceedings{Bane2004,
abstract = {This paper presents a set of interactive tools designed to give users virtual x-ray vision. These tools address a common problem in depicting occluded infrastructure: either too much information is displayed, confusing users, or too little information is displayed, depriving users of important depth cues. Four tools are presented: the tunnel tool and room selector tool directly augment the user's view of the environment, allowing them to explore the scene in direct, first person view. The room in miniature tool allows the user to select and interact with a room from a third person perspective, allowing users to view the contents of the room from points of view that would normally be difficult or impossible to achieve. The room slicer tool aids users in exploring volumetric data displayed within the room in miniature tool. Used together, the tools presented in this paper can be used to achieve the virtual x-ray vision effect. We test our prototype system in a far-field mobile augmented reality setup, visualizing the interiors of a small set of buildings on the UCSB campus.},
address = {Arlington},
author = {Bane, R and Hollerer, T},
booktitle = {Third IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2004.36},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Third IEEE and ACM International Symposium on Mixed and Augmented Reality/2004/Bane, Hollerer/Bane, Hollerer - 2004 - Interactive Tools for Virtual X-Ray Vision in Mobile Augmented Reality.pdf:pdf},
isbn = {0-7695-2191-6},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {231--239},
publisher = {IEEE},
title = {{Interactive Tools for Virtual X-Ray Vision in Mobile Augmented Reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1383060 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1383060},
year = {2004}
}
@article{Renshaw2004,
abstract = {We describe an experiment in which the eye movements of participants, carrying out tasks using two contrasting graph designs, were recorded by means of a remote eye tracking device. A variety of eye movement properties were measured and analysed both temporally and spatially. Both graph designs were based on specific psychological theories and established graph design guidelines. One incorporated attributes thought likely to enhance usability, the other included attributes likely to have the opposite effect. The results demonstrate that the design and location of a graph's legend and its spatial relationship to the data area are extremely important in determining a graph's usability. The incorporation of these and other design features may promote or detract from perceptual proximity and therefore influence a display's usability. The paper demonstrates that this influence is reflected in eye movement patterns, which can be readily monitored by means of a remote eye tracking system, and that a relatively simple temporal analysis of the results can give important insights as to how the usability of visual displays has been influenced.},
annote = {- cited by: 26
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Renshaw, J.A and Finlay, J.E and Tyfa, D and Ward, R.D},
doi = {10.1016/j.intcom.2004.03.001},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Interacting with Computers/2004/Renshaw et al/Renshaw et al. - 2004 - Understanding visual influence in graph design through temporal and spatial eye movement characteristics.pdf:pdf},
journal = {Interacting with Computers},
keywords = {Eye tracking,Graphs,Perception,Usability,gaze analysis},
mendeley-tags = {gaze analysis},
number = {3},
pages = {557--578},
title = {{Understanding visual influence in graph design through temporal and spatial eye movement characteristics}},
url = {http://www.sciencedirect.com/science/article/pii/S0953543804000360},
volume = {16},
year = {2004}
}
@inproceedings{Veneri2010,
abstract = {Eye movement is the most simple and repetitive movement that enable humans to interact with the environment. The common daily activities, such as watching television or reading a book, involve this natural activity which consists of rapidly shifting our gaze from one region to another. The identification of the main components of eye movement during visual exploration such as fixations and saccades, is the objective of the analysis of eye movements in various contexts ranging from basic neuro sciences and visual sciences to virtual reality interactions and robotics. However, many of the algorithms that detect fixations present a number of problems. In this article, we present a new fixation identification algorithm based on the analysis of variance and F-test. We present the new algorithm and we compare it with the common fixations algorithm based on dispersion. To demonstrate the performance of our approach we tested the algorithm in a group of healthy subjects.},
annote = {- cited by: 5
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Veneri, Giacomo and Piu, Pietro and Federighi, Pamela and Rosini, Francesca and Federico, Antonio and Rufa, Alessandra},
booktitle = {2010 2nd International Workshop on Cognitive Information Processing},
doi = {10.1109/CIP.2010.5604221},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2010 2nd International Workshop on Cognitive Information Processing/2010/Veneri et al/Veneri et al. - 2010 - Eye fixations identification based on statistical analysis - Case study.pdf:pdf},
isbn = {978-1-4244-6459-3},
keywords = {Eye Tracking,F-test,Segmentation,analysis of variance,eye fixations identification,eye movement},
mendeley-tags = {Eye Tracking,Segmentation},
month = jun,
pages = {446--451},
publisher = {IEEE},
title = {{Eye fixations identification based on statistical analysis - Case study}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5604221 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5604221},
year = {2010}
}
@inproceedings{Qiang,
abstract = {This paper addresses the vision-based self-localization of an indoor service robot worked with a single vehicular camera. Based on the principle of projective geometry and the extended virtual image plane, the transverse position and orientation of robot is calculated through considering the image plane and floor plane as a whole. The longitudinal position is acquired by using the monocular stereovision method based on distance constraint along feature points of nature or artificial landmark. After a simple off-line process about landmark, the service robot can work well in a building. The validity and feasibility of the approach have been demonstrated through experiments.},
author = {Qiang, Fang and Cunxi, Xie},
booktitle = {Proceedings 7th International Conference on Signal Processing, 2004. Proceedings. ICSP '04. 2004.},
doi = {10.1109/ICOSP.2004.1441468},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings 7th International Conference on Signal Processing, 2004. Proceedings. ICSP '04. 2004/Unknown/Qiang, Cunxi/Qiang, Cunxi - Unknown - The vision-based metric self-localization of indoor mobile robot using projective geometry.pdf:pdf},
isbn = {0-7803-8406-7},
pages = {914--917},
publisher = {IEEE},
title = {{The vision-based metric self-localization of indoor mobile robot using projective geometry}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=1441468\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=.QT.projective+geometry.QT.},
volume = {2}
}
@article{Lauckner2013,
address = {New York, New York, USA},
author = {Lauckner, Carolyn and Hsieh, Gary},
doi = {10.1145/2470654.2470702},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {333--342},
publisher = {ACM Press},
title = {{The presentation of health-related search results and its impact on negative emotional outcomes}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470702},
year = {2013}
}
@inproceedings{Byeon2004,
abstract = {UML cannot meet all the requirements offered in different software system for diverse application domain. GNSS (Global Navigation Satellite System) application domain is an especial environment that requires precise measurement and precision calculation of real-world geographical entities with the help of GPS (Global Position System) in both temporal and spatial factor. Therefore in the paper new extended iconic stereotypes for better modeling GNSS application in the UML diagram are proposed, and the implementation of a program called StereotypeCreator, which is able to create iconic stereotypes used in one of the most popular visual modeling tools for software development, Rational Rose, will be also proposed.},
author = {Byeon, WS and Wang, B and Jeong, SK},
booktitle = {2004 International Conference on Cyberworlds},
doi = {10.1109/CW.2004.32},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2004 International Conference on Cyberworlds/2004/Byeon, Wang, Jeong/Byeon, Wang, Jeong - 2004 - Extension and Implementation of Iconic Stereotype for GNSS Application in the UML Class Diagram.pdf:pdf},
isbn = {0-7695-2140-1},
pages = {162--169},
publisher = {IEEE},
title = {{Extension and Implementation of Iconic Stereotype for GNSS Application in the UML Class Diagram}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1366169 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1366169},
year = {2004}
}
@article{Jang2013,
abstract = {We propose a novel approach for the identification of human implicit visual search intention based on eye movement patterns and pupillary analysis, in general, as well as pupil size, gradient of pupil size variation, fixation length and fixation count corresponding to areas of interest, and fixation count corresponding to non-areas of interest, in particular. The proposed model identifies human implicit visual search intention as task-free visual browsing or task-oriented visual search. Task-oriented visual search is further identified as task-oriented visual search intent generation, task-oriented visual search intent maintenance, or task-oriented visual search intent disappearance. During a visual search, measurement of the pupillary response is greatly influenced by external factors such the intensity and size of the visual stimulus. To alleviate the effects of external factors, we propose a robust baseline model that can accurately measure the pupillary response. Graphical representation of the measured parameter values shows significant differences among the different intent conditions, which can then be used as features for identification. By using the eye movement patterns and pupillary analysis, we can detect the transitions between different implicit intentions—task-free visual browsing intent to task-oriented visual search intent and task-oriented visual search intent maintenance to task-oriented visual search intent disappearance—using a hierarchical support vector machine. In the proposed model, the hierarchical support vector machine is able to identify the transitions between different intent conditions with greater than 90 \% accuracy.},
annote = {- cited by: 0
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Jang, Young-Min and Mallipeddi, Rammohan and Lee, Minho},
doi = {10.1007/s11257-013-9142-7},
file = {:home/acmt/Dropbox/Documentos/Mendeley/User Modeling and User-Adapted Interaction/2013/Jang, Mallipeddi, Lee/Jang, Mallipeddi, Lee - 2013 - Identification of human implicit visual search intention based on eye movement and pupillary analysis.pdf:pdf},
issn = {0924-1868},
journal = {User Modeling and User-Adapted Interaction},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = apr,
title = {{Identification of human implicit visual search intention based on eye movement and pupillary analysis}},
url = {http://link.springer.com/article/10.1007/s11257-013-9142-7 http://link.springer.com/10.1007/s11257-013-9142-7},
year = {2013}
}
@phdthesis{Voglsam2013,
abstract = {In computer graphics, ray tracing is a well-known image generation algorithm which exists since the late 1970s. Ray tracing is typically known as an offline algorithm, which means that the image generation process takes several seconds to minutes or even hours or days. In this thesis I present a ray tracer which runs in real-time. Real-time in terms of computer graphics means that 60 or more images per second (frames per second, FPS) are created. To achieve such frame rates, the ray tracer runs completely on the graphics card (GPU). This is possible by making use of Nvidia’s CUDA-API. With CUDA, it is possible to program the processor of a graphics card similar to a processor of a CPU. This way, the computational power of a graphics card can be fully utilized. A crucial part of any ray tracer is the acceleration data structure (ADS) used. The ADS is needed to efficiently search in geometric space. In this thesis, two variants of so called kD-Trees have been implemented. A kD-Tree is a binary tree, which divides at each node a given geometric space into two halves using an axis aligned splitting plane. Furthermore, a CUDA library for the rendering engine Aardvark, which is the in-house rendering engine at the VRVis Research Center, was developed to access CUDA functionality from within Aardvark in an easy and convenient way. The ray tracer is part of a current software project called “HILITE” at the VRVis Research Center.},
author = {Voglsam, G\"{u}nther},
pages = {115},
school = {Vienna University of Technology},
title = {{Real-time Ray Tracing on the GPU}},
volume = {2013},
year = {2013}
}
@inproceedings{Pires2013,
abstract = {Wearable devices with gaze tracking can assist users in many daily-life tasks. When used for extended periods of time, it is desirable that such devices do not employ active illumination for safety reasons and to minimize interference from other light sources such as the sun. Most non active-illumination methods for gaze tracking attempt to locate the iris contour by fitting an ellipse. Although the camera projection causes the iris to appear as an ellipse in the eye image, it is actually a circle on the eye surface. Instead of searching for an ellipse in the eye image, the method proposed in this paper searches for a circle on the eye surface. To this end, the method calibrates a three-dimensional eye model based on the location of the corners of the eye. Using the 3D eye model, an input image is first transformed so that the eye's spherical surface is warped into a plane, thus \&\#x201C;unwrapping\&\#x201D; the eye. The iris circle is then detected on the unwrapped image by a three-step robust circle-fitting procedure. The location of the circle corresponds to the gaze orientation on the outside image. The method is fast to calibrate and runs in realtime. Extensive experimentation on embedded hardware and comparisons with alternative methods demonstrate the effectiveness of the proposed solution.},
author = {Pires, Bernardo Rodrigues and Devyver, Michael and Tsukada, Akihiro and Kanade, Takeo},
booktitle = {2013 IEEE Workshop on Applications of Computer Vision (WACV)},
doi = {10.1109/WACV.2013.6475042},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2013 IEEE Workshop on Applications of Computer Vision (WACV)/2013/Pires et al/Pires et al. - 2013 - Unwrapping the eye for visible-spectrum gaze tracking on wearable devices.pdf:pdf},
isbn = {978-1-4673-5054-9},
issn = {1550-5790},
keywords = {Calibration,Cameras,Feature extraction,Iris,Robustness,Solid modeling,Transforms,eye tracker,toshi},
language = {English},
mendeley-tags = {eye tracker,toshi},
month = jan,
pages = {369--376},
publisher = {IEEE},
title = {{Unwrapping the eye for visible-spectrum gaze tracking on wearable devices}},
url = {http://www.computer.org/csdl/proceedings/wacv/2013/5053/00/06475042-abs.html http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6475042},
year = {2013}
}
@article{Reitmayr2005,
author = {Reitmayr, G. and Eade, E. and Drummond, T.},
doi = {10.1109/ISMAR.2005.39},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)/2005/Reitmayr, Eade, Drummond/Reitmayr, Eade, Drummond - 2005 - Localisation and interaction for augmented maps.pdf:pdf},
isbn = {0-7695-2459-1},
journal = {Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)},
keywords = {a control,a user interacting with,augmented,device to pick up,figure 1,maps using a pda,optical tracking,plays,projection dis-,spatially augmented reality,tangible user interfaces},
pages = {120--129},
publisher = {Ieee},
title = {{Localisation and interaction for augmented maps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544673},
year = {2005}
}
@article{Uzor2013,
address = {New York, New York, USA},
author = {Uzor, Stephen and Baillie, Lynne},
doi = {10.1145/2470654.2466159},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1233},
publisher = {ACM Press},
title = {{Exploring \& designing tools to enhance falls rehabilitation in the home}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466159},
year = {2013}
}
@article{Dobler2002,
author = {Dobler, D and Haller, M and Stampfl, P},
journal = {ACM SIGGRAPH 2002 conference  \ldots},
keywords = {augmented reality,sound},
mendeley-tags = {augmented reality,sound},
title = {{ASR: augmented sound reality}},
url = {http://dl.acm.org/citation.cfm?id=1242161},
year = {2002}
}
@article{Lerner2010,
abstract = {Many times, even if a crowd simulation looks good in general, there could be some specific individual behaviors which do not seem correct. Spotting such problems manually can become tedious, but ignoring them may harm the simulation’s credibility. In this paper we present a data-driven approach for evaluating the behaviors of in- dividuals within a simulated crowd. Based on video-footage of a real crowd, a database of behavior examples is generated. Given a simulation of a crowd, an analog analysis is performed on it, defining a set of queries, which are matched by a similarity function to the database examples. The results offer a possible objective answer to the question of how similar are the simulated individual behaviors to real observed behaviors. Moreover, by chang- ing the video input one can change the context of evaluation. We show several examples of evaluating simulated crowds produced using different techniques and comprising of dense crowds, sparse crowds and flocks},
author = {Lerner, Alon and Chrysanthou, Yiorgos and Shamir, Ariel and Cohen-Or, Daniel},
doi = {10.1111/j.1467-8659.2010.01808.x},
issn = {01677055},
journal = {Computer Graphics Forum},
month = sep,
number = {7},
pages = {2197--2206},
title = {{Context-Dependent Crowd Evaluation}},
url = {http://doi.wiley.com/10.1111/j.1467-8659.2010.01808.x},
volume = {29},
year = {2010}
}
@book{Filho2008,
author = {Filho, Paulo Jos\'{e} Freitas},
edition = {2},
isbn = {9788575022283},
pages = {372},
publisher = {Visual Books},
title = {{Introdu\c{c}\~{a}o a Modelagem e Simula\c{c}\~{a}o de Sistemas com Aplica\c{c}\~{o}es Arena}},
year = {2008}
}
@article{Birkfellner2002,
abstract = {Computer-aided surgery (CAS), the intraoperative application of biomedical visualization techniques, appears to be one of the most promising fields of application for augmented reality (AR), the display of additional computer-generated graphics over a real-world scene. Typically a device such as a head-mounted display (HMD) is used for AR. However, considerable technical problems connected with AR have limited the intraoperative application of HMDs up to now. One of the difficulties in using HMDs is the requirement for a common optical focal plane for both the realworld scene and the computer-generated image, and acceptance of the HMD by the user in a surgical environment. In order to increase the clinical acceptance of AR, we have adapted the Varioscope (Life Optics, Vienna), a miniature, cost-effective head-mounted operating binocular, for AR. In this paper, we present the basic design of the modified HMD, and the method and results of an extensive laboratory study for photogrammetric calibration of the Varioscope's computer displays to a real-world scene. In a series of 16 calibrations with varying zoom factors and object distances, mean calibration error was found to be 1.24 +/- 0.38 pixels or 0.12 +/- 0.05 mm for a 640 x 480 display. Maximum error accounted for 3.33 +/- 1.04 pixels or 0.33 +/- 0.12 mm. The location of a position measurement probe of an optical tracking system was transformed to the display with an error of less than 1 mm in the real world in 56\% of all cases. For the remaining cases, error was below 2 mm. We conclude that the accuracy achieved in our experiments is sufficient for a wide range of CAS applications.},
author = {Birkfellner, Wolfgang and Figl, Michael and Huber, Klaus and Watzinger, Franz and Wanschitz, Felix and Hummel, Johann and Hanel, Rudolf and Greimel, Wolfgang and Homolka, Peter and Ewers, Rolf and Bergmann, Helmar},
doi = {10.1109/TMI.2002.803099},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE transactions on medical imaging/2002/Birkfellner et al/Birkfellner et al. - 2002 - A head-mounted operating binocular for augmented reality visualization in medicine--design and initial evalu.pdf:pdf},
issn = {0278-0062},
journal = {IEEE transactions on medical imaging},
keywords = {Calibration,Computer Graphics,Depth Perception,Equipment Design,Equipment Failure Analysis,Imaging, Three-Dimensional,Imaging, Three-Dimensional: instrumentation,Imaging, Three-Dimensional: methods,Microscopy, Video,Microscopy, Video: instrumentation,Microsurgery,Microsurgery: instrumentation,Reproducibility of Results,Sensitivity and Specificity,Subtraction Technique,Surgical Equipment,User-Computer Interface,Video Recording,Video Recording: instrumentation,Video Recording: methods},
month = aug,
number = {8},
pages = {991--7},
pmid = {12472271},
title = {{A head-mounted operating binocular for augmented reality visualization in medicine--design and initial evaluation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12472271},
volume = {21},
year = {2002}
}
@article{Schmidt2013,
address = {New York, New York, USA},
author = {Schmidt, Dominik and Sas, Corina and Gellersen, Hans},
doi = {10.1145/2470654.2466457},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Multi-touch surfaces,clipboards,copy-and-paste},
pages = {3335},
publisher = {ACM Press},
title = {{Personal clipboards for individual copy-and-paste on shared multi-user surfaces}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466457},
year = {2013}
}
@article{Gregory1997,
author = {Gregory, RL},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Eye and brain: The psychology of seeing .}},
url = {http://psycnet.apa.org/psycinfo/1998-07123-000},
year = {1997}
}
@inproceedings{II2001,
abstract = {The Virtual-Reality Peripheral Network (VRPN) system provides a device-independent and network-transparent interface to virtual-reality peripherals. VRPN's application of factoring by function and of layering in the context of devices produces an interface that is novel and powerful. VRPN also integrates a wide range of known advanced techniques into a publicly-available system. These techniques benefit both direct VRPN users and those who implement other applications that make use of VR peripherals.},
address = {New York, New York, USA},
author = {Taylor, Russell M. and Hudson, Thomas C. and Seeger, Adam and Weber, Hans and Juliano, Jeffrey and Helser, Aron T.},
booktitle = {Proceedings of the ACM symposium on Virtual reality software and technology - VRST '01},
doi = {10.1145/505008.505019},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the ACM symposium on Virtual reality software and technology - VRST '01/2001/Taylor et al/Taylor et al. - 2001 - VRPN a device-independent, network-transparent VR peripheral system.pdf:pdf},
isbn = {1581134274},
pages = {55},
publisher = {ACM Press},
title = {{VRPN: a device-independent, network-transparent VR peripheral system}},
url = {http://dl.acm.org/citation.cfm?id=505019 http://portal.acm.org/citation.cfm?doid=505008.505019},
year = {2001}
}
@article{Muczynski2013,
abstract = {The following paper provides information about eye-tracking techniques and methodology. It is focused on introducing eye movement metrics in human factor research in maritime domain, explaining basic methodo- logy and describing the types of data analysis, thus providing the background and guidelines for simple eye- tracking studies.},
author = {Muczyński, B and Gucma, M},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Scientific Journals Maritime \ldots/2013/Muczyński, Gucma/Muczyński, Gucma - 2013 - Application of eye-tracking techniques in human factor research in marine operations. Challenges and methodol.pdf:pdf},
journal = {Scientific Journals Maritime \ldots},
keywords = {data analy-,eye-tracking,gaze analysis,human factor research,maritime operations,sis,technique methodology},
mendeley-tags = {gaze analysis},
number = {1},
pages = {116--120},
title = {{Application of eye-tracking techniques in human factor research in marine operations. Challenges and methodology}},
url = {http://repository.am.szczecin.pl/handle/123456789/541},
volume = {36},
year = {2013}
}
@article{Sato1999,
abstract = {This paper describes a new method for superimposing virtual objects with correct shadings onto an image of a real scene. Unlike the previously proposed methods, our method can measure a radiance distribution of a real scene automatically and use it for superimposing virtual objects appropriately onto a real scene. First, a geometric model of the scene is constructed from a pair of omnidirectional images by using an omnidirectional stereo algorithm. Then, radiance of the scene is computed from a sequence of omnidirectional images taken with different shutter speeds and mapped onto the constructed geometric model. The radiance distribution mapped onto the geometric model is used for rendering virtual objects superimposed onto the scene image. As a result, even for a complex radiance distribution, our method can superimpose virtual objects with convincing shadings and shadows cast onto the real scene. We successfully tested the proposed method by using real images to show its effectiveness},
author = {Sato, I. and Sato, Yoichi and Ikeuchi, Katsushi},
doi = {10.1109/2945.764865},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {1},
pages = {1--12},
title = {{Acquiring a radiance distribution to superimpose virtual objects onto a real scene}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=764865},
volume = {5},
year = {1999}
}
@inproceedings{Thalmann2009,
abstract = {The purpose of this paper is to identify the problems to solve in order to simulate real-time crowds in a virtual environment. We try to classify these problems and study how they have been addressed until now by the research community and our lab in particular. We then discuss for each problem what are the future challenges and how to address them.},
address = {Bradford},
author = {Thalmann, Daniel and Grillon, Helena and Maim, Jonathan and Yersin, Barbara},
booktitle = {2009 International Conference on CyberWorlds},
doi = {10.1109/CW.2009.23},
isbn = {978-1-4244-4864-7},
keywords = {Crowd simulation,behavioral animation},
pages = {1--12},
publisher = {IEEE},
title = {{Challenges in Crowd Simulation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5279720},
year = {2009}
}
@inproceedings{Kang2011,
abstract = {Human visual attention is an area of research that has a strong effect on the field of human-robot-interaction. It has various applications for human-care robots and service robots. The ability of human visual attention to acquire knowledge on informative objects through interaction with the environment is an important research issue in the field of human-computer interaction and augmented cognition. Such knowledge can be acquired by automatically detecting visually attentive regions using the fixation extracted from eye-tracking data. Therefore, the optimal fixation must be selected from human eye-tracking data to detect the region of interest. In this paper, eye movement tracking was accurately used to capture human eye movements and to characterize the location and extent of a user's interest as an input mechanism to drive the interaction system. Furthermore, both top-down and bottom-up processes of the human visual system were at work in the process of selective attention. The correlation between human eye movement information and the bottom-up saliency map was calculated and compared with the correlation that was calculated by combining the top-down and bottom-up processes. The experiment results showed that the human visual attention system cannot be constructed with the bottom-up process alone and requires the combination of the top-down and bottom-up processes together.},
address = {Karon Beach, Phuket},
author = {Kang, Dooseok and Lee, Sukhan and Lee, Yu-Bu},
booktitle = {2011 IEEE International Conference on Robotics and Biomimetics},
doi = {10.1109/ROBIO.2011.6181594},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 IEEE International Conference on Robotics and Biomimetics/2011/Kang, Lee, Lee/Kang, Lee, Lee - 2011 - Human visual attention with context-specific top-down saliency.pdf:pdf},
isbn = {978-1-4577-2138-0},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = dec,
pages = {2055--2060},
publisher = {IEEE},
title = {{Human visual attention with context-specific top-down saliency}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6181594 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6181594},
year = {2011}
}
@incollection{Teichrieba,
abstract = {Evolution of man-machine interaction paradigms brought new challenges to HCI research. Along years HCI studies merged with en- gineers and IT professionals work bring a diversity of ways to interact. In this context, the natural interaction tendency explores human body movements to perform interaction, including users body, hands and face motion.},
address = {Bauru-SP},
author = {Teichrieb, Veronica and Figueiredo, Lucas Silva},
booktitle = {Intera\c{c}\~{a}o em Realidade Virtual e Aumentada},
chapter = {1},
edition = {1},
editor = {Kelner, Judith and Brega, Jos\'{e} Remo F.},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Intera\c{c}\~{a}o em Realidade Virtual e Aumentada/2010/Teichrieb, Figueiredo/Teichrieb, Figueiredo - 2010 - Intera\c{c}\~{a}o Natural.pdf:pdf},
pages = {9--22},
publisher = {Canal6},
title = {{Intera\c{c}\~{a}o Natural}},
year = {2010}
}
@article{Zhou2014,
abstract = {We propose a novel method for relighting the image of outdoor scenes viewed from a fixed camera based on a sparse set of images of the same scene under different illumination. Unlike previous methods which require capturing images under pre-designed lighting or employing the 3D model of the target objects, our method adopts the technique of basis image which encapsulates material and geometry information of the scene into one image. We present a new method to calculate the basis images of an outdoor scene based on the sampling images captured in different time of a day, the relighting images corresponding to new sunlight incidence directions with arbitrary intensity can then be generated with these basis images. Besides, the subordinate shadow effect adhering to the sun's movement is also simulated, producing a visually plausible relighting result. Experiments demonstrate the efficiency and validity of our approach.},
author = {Zhou, Xuehong and Xing, Guanyu and Ding, Zhipeng and Liu, Yanli and Xiong, Junjun and Peng, Qunsheng},
doi = {10.1016/j.cag.2013.10.030},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {Outdoor scenes,illumination estimation,image-based relighting},
month = feb,
pages = {230--238},
publisher = {Elsevier},
title = {{Image-based relighting from a sparse set of outdoor images}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849313001714},
volume = {38},
year = {2014}
}
@book{williams1999visual,
author = {Williams, AM M and Davids, K and Williams, JG G P},
isbn = {9780419248002},
publisher = {Taylor \& Francis},
title = {{Visual Perception and Action in Sport}},
url = {http://books.google.com.br/books?hl=en\&lr=\&id=uONBuqh6tQoC\&oi=fnd\&pg=PR2\&dq=Visual+perception+and+action+in+sport\&ots=jGnktEj5uv\&sig=ia6hnz6Z\_CoprcWmKUbzYSAFYr8 http://books.google.com.br/books?id=uONBuqh6tQoC},
year = {1999}
}
@article{Platonov2006,
author = {Platonov, Juri and Heibel, Hauke and Meier, Peter},
doi = {10.1109/ISMAR.2006.297800},
file = {:home/acmt/Dropbox/Documentos/Mendeley/of the 5th IEEE and ACM/2006/Platonov, Heibel, Meier/Platonov, Heibel, Meier - 2006 - A mobile markerless AR system for maintenance and repair.pdf:pdf},
isbn = {1-4244-0650-1},
journal = {of the 5th IEEE and ACM},
keywords = {17,a laptop,and the user,augmented reality,augmented video stream are,computer,e,g,has been inspired by,lessly transmitted between a,maintenance,markerless tracking,our markerless tracking algorithm,solution both,the raw and the,wire-},
month = oct,
pages = {105--108},
publisher = {Ieee},
title = {{A mobile markerless AR system for maintenance and repair}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4079262 http://portal.acm.org/citation.cfm?id=1514222},
year = {2006}
}
@article{Arai2010,
author = {Arai, K and Mardiyanto, R},
journal = {International Journal of Human  \ldots},
keywords = {blink,gaze analysis},
mendeley-tags = {blink,gaze analysis},
title = {{Real time blinking detection based on Gabor filter}},
url = {http://download.neurosky.com.cn/marketing/学术论文/人机交互论文/IJHCI-11.pdf},
year = {2010}
}
@inproceedings{Pessoa2010,
abstract = {This paper presents a solution for the photorealistic rendering of synthetic objects into dynamic real scenes, in Augmented Reality applications. In order to achieve this goal, an Image Based Lighting approach is used, where environment maps with different levels of glossiness are generated for each virtual object in the scene at every frame. Due to this, illumination effects, such as color bleeding and specular reflections, can be simulated for virtual objects in a consistent way. A unifying sampling method for the spherical harmonics transformation pass is also used. It is independent of map format and does not need to apply different weights for each sample. The developed technique is combined with an extended version of Lafortune Spatial BRDF, featuring Fresnel effect and an innovative tangent rotation parameterization. The solution is evaluated in various Augmented Reality case studies, where other features like shadowing and lens effects are also exploited.},
address = {Waltham, MA},
author = {Pessoa, Saulo and Moura, Guilherme and Lima, Joao and Teichrieb, Veronica and Kelner, Judith},
booktitle = {2010 IEEE Virtual Reality Conference (VR)},
doi = {10.1109/VR.2010.5444836},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2010 IEEE Virtual Reality Conference (VR)/2010/Pessoa et al/Pessoa et al. - 2010 - Photorealistic rendering for Augmented Reality A global illumination and BRDF solution.pdf:pdf},
isbn = {978-1-4244-6237-7},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = mar,
pages = {3--10},
publisher = {IEEE},
title = {{Photorealistic rendering for Augmented Reality: A global illumination and BRDF solution}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5444836 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5444836},
year = {2010}
}
@inproceedings{Prasad2006,
abstract = {Recent advances in single-view reconstruction (SVR) have been in modelling power (curved 2.5D surfaces) and automation (automatic photo pop-up). We extend SVR along both of these directions. We increase modelling power in several ways: (i) We represent general 3D surfaces, rather than 2.5D Monge patches; (ii) We describe a closed-form method to reconstruct a smooth surface from its image apparent contour, including multilocal singularities ("kidney-bean" self-occlusions); (iii) We show how to incorporate user-specified data such as surface normals, interpolation and approximation constraints; (iv) We show how this algorithm can be adapted to deal with surfaces of arbitrary genus. We also show how the modelling process can be automated for simple object shapes and views, using a-priori object class information. We demonstrate these advances on natural images drawn from a number of object classes.},
author = {Prasad, M and Fitzgibbon, A},
booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR'06)},
doi = {10.1109/CVPR.2006.281},
isbn = {0-7695-2597-0},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {1345--1354},
publisher = {IEEE},
title = {{Single View Reconstruction of Curved Surfaces}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1640914 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1640914},
volume = {2},
year = {2006}
}
@inproceedings{Pelechano2007,
abstract = {Simulating the motion of realistic, large, dense crowds of autonomous agents is still a challenge for the computer graphics community. Typical approaches either resemble particle simulations (where agents lack orientation controls) or are conservative in the range of human motion possible (agents lack psychological state and aren't allowed to 'push' each other). Our HiDAC system (for High-Density Autonomous Crowds) focuses on the problem of simulating the local motion and global wayfinding behaviors of crowds moving in a natural manner within dynamically changing virtual environments. By applying a combination of psychological and geometrical rules with a social and physical forces model, HiDAC exhibits a wide variety of emergent behaviors from agent line formation to pushing behavior and its consequences; relative to the current situation, personalities of the individuals and perceived social density.},
address = {Prague},
author = {Pelechano, N and Allbeck, JM and Badler, NI},
booktitle = {Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2007 ACM SIGGRAPHEurographics symposium on Computer animation/2007/Pelechano, Allbeck, Badler/Pelechano, Allbeck, Badler - 2007 - Controlling individual agents in high-density crowd simulation.pdf:pdf},
keywords = {crowd simulation},
mendeley-tags = {crowd simulation},
pages = {99--108},
publisher = {Eurographics},
title = {{Controlling individual agents in high-density crowd simulation}},
url = {http://dl.acm.org/citation.cfm?id=1272705},
year = {2007}
}
@article{Agarwal2005,
abstract = {The estimation of the homography between two views is a key step in many applications involving multiple view geometry. The homography exists between two views between projections of points on a 3D plane. A homography exists also between projections of all points if the cameras have purely rotational motion. A number of algorithms have been proposed for the estimation of the homography relation between two images of a planar scene. They use features or primitives ranging from simple points to a complex ones like non-parametric curves. Different algorithms make different assumptions on the imaging setup and what is known about them. This article surveys several homography estimation techniques from the literature. The essential theory behind each method is presented brieﬂy and compared with the others. Experiments aimed at providing a representative analysis and comparison of the methods discussed are also presented in the paper.},
author = {Agarwal, A},
journal = {\ldots Tech. Rep. IIIT/TR/2005/12},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
title = {{A survey of planar homography estimation techniques}},
url = {http://iic.cs.arizona.edu/static/resources/2010/07/01/planar\_homography.pdf},
year = {2005}
}
@article{Engbert2003,
abstract = {Fixational eye movements are subdivided into tremor, drift, and microsaccades. All three types of miniature eye movements generate small random displacements of the retinal image when viewing a stationary scene. Here we investigate the modulation of microsaccades by shifts of covert attention in a classical spatial cueing paradigm. First, we replicate the suppression of microsaccades with a minimum rate about 150 ms after cue onset. Second, as a new finding we observe microsaccadic enhancement with a maximum rate about 350 ms after presentation of the cue. Third, we find a modulation of the orientation towards the cue direction. These multiple influences of visual attention on microsaccades accentuate their role for visual information processing. Furthermore, our results suggest that microsaccades can be used to map the orientation of visual attention in psychophysical experiments.},
author = {Engbert, Ralf and Kliegl, Reinhold},
doi = {10.1016/S0042-6989(03)00084-1},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Vision Research/2003/Engbert, Kliegl/Engbert, Kliegl - 2003 - Microsaccades uncover the orientation of covert attention.pdf:pdf},
issn = {00426989},
journal = {Vision Research},
keywords = {covert attention,gaze analysis,microsaccades},
mendeley-tags = {covert attention,gaze analysis,microsaccades},
month = apr,
number = {9},
pages = {1035--1045},
title = {{Microsaccades uncover the orientation of covert attention}},
url = {http://www.sciencedirect.com/science/article/pii/S0042698903000841 http://linkinghub.elsevier.com/retrieve/pii/S0042698903000841},
volume = {43},
year = {2003}
}
@inproceedings{Moreno-Noguer2005,
address = {New York, New York, USA},
author = {Moreno-Noguer, Francesc and Nayar, Shree K. and Belhumeur, Peter N.},
booktitle = {ACM SIGGRAPH 2005 Sketches on - SIGGRAPH '05},
doi = {10.1145/1187112.1187202},
pages = {75},
publisher = {ACM Press},
title = {{Optimal illumination for image and video relighting}},
url = {http://portal.acm.org/citation.cfm?doid=1187112.1187202},
year = {2005}
}
@inproceedings{Kakuta2005,
abstract = {We propose a simple method to express shading and shadowing of virtual objects in Mixed Reality especially appropriate for static architecture models in outdoor scenes. We create the shadows of the virtual objects in a fast and efficient way using a set of pre-rendered basis images and shadowing planes. The proposed method is limited in interactivity but can operate in near real-time.},
author = {Kakuta, T and Oishi, T and Ikeuchi, K},
booktitle = {Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)},
doi = {10.1109/ISMAR.2005.51},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)/2005/Kakuta, Oishi, Ikeuchi/Kakuta, Oishi, Ikeuchi - 2005 - Shading and shadowing of architecture in mixed reality.pdf:pdf},
isbn = {0-7695-2459-1},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
pages = {200--201},
publisher = {IEEE},
title = {{Shading and shadowing of architecture in mixed reality}},
url = {http://dl.acm.org/citation.cfm?id=1105211 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544693},
year = {2005}
}
@article{Chen1996,
abstract = {An inverse eigenvalue problem, where a matrix is to be constructed from some or all of its eigenvalues, may not have a real-valued solution at all. An approximate solution in the sense of least squares is sometimes desirable. Two types of least squares problems are formulated and explored in this paper. In spite of their different appearance, the two problems are shown to be equivalent. Thus one new numerical method, modified from the conventional alternating projection method, is proposed. The method converges linearly and globally and can be used to generate good starting values for other faster but more expensive and locally convergent methods. The idea can be applied to multiplicative inverse eigenvalue problems for the purpose of preconditioning. Numerical examples are presented.},
annote = {        From Duplicate 2 (                   On the Least Squares Solution of Inverse Eigenvalue Problems                 - Chen, Xuzhou; Chu, Moody Ten-Chao )
                
        
        
      },
author = {Chen, Xuzhou and Chu, Moody Ten-Chao},
doi = {10.1137/S0036142994264742},
issn = {0036-1429},
journal = {SIAM Journal on Numerical Analysis},
keywords = {1,65f15,65h15,ams subject classifications,applications,are of great importance,as well as numerical,discussions on various aspects,iep,introduction,inverse eigenvalue problem,inverse eigenvalue problems,least squares,lift and projection,lift projection,of the existence theory,to many},
month = dec,
number = {6},
pages = {2417--2430},
title = {{On the Least Squares Solution of Inverse Eigenvalue Problems}},
url = {http://epubs.siam.org/doi/abs/10.1137/S0036142994264742 http://link.aip.org/link/SJNAAM/v33/i6/p2417/s1\&Agg=doi},
volume = {33},
year = {1996}
}
@article{Leao2011,
author = {Le\~{a}o, Crystian Wendel M. and Lima, Jo\~{a}o Paulo and Teichrieb, Veronica and Kelner, Judith and Albuquerque, Eduardo S.},
doi = {10.1109/SVR.2011.29},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 XIII Symposium on Virtual Reality/2011/Le\~{a}o et al/Le\~{a}o et al. - 2011 - Geometric Modifications Applied to Real Elements in Augmented Reality.pdf:pdf},
isbn = {978-1-4577-0661-5},
journal = {2011 XIII Symposium on Virtual Reality},
keywords = {-augmented reality,mixed reality,physically-based},
month = may,
pages = {96--101},
publisher = {Ieee},
title = {{Geometric Modifications Applied to Real Elements in Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951840},
year = {2011}
}
@article{Hutto2013,
address = {New York, New York, USA},
author = {Hutto, C.J. and Yardi, Sarita and Gilbert, Eric},
doi = {10.1145/2470654.2470771},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Social networks,computer mediated communication,social media},
pages = {821},
publisher = {ACM Press},
title = {{A longitudinal study of follow predictors on twitter}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470771},
year = {2013}
}
@article{McDonald2013,
address = {New York, New York, USA},
author = {McDonald, Sharon and Petrie, Helen},
doi = {10.1145/2470654.2481407},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2941},
publisher = {ACM Press},
title = {{The effect of global instructions on think-aloud testing}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481407},
year = {2013}
}
@inproceedings{Madsen2005,
abstract = {Knowledge about the illumination conditions in a real world scene has many applications among them Aug- mented Reality which aims at placing virtual objects in the real world. An important factor for convincing aug- mentations is to use the illumination of the real world when rendering the virtual objects so they are shaded consistently and cast consistent shadows. This paper proposes two approaches to continuously estimate the illumination conditions in a static outdoor scene based on images from a single viewpoint of that scene while using the scene itself as light probe. Thus, no additional calibration objects are required. Experi- mental results show that the proposed illumination esti- mation is sufficient for Augmented Reality applications.},
address = {Copenhagen, Denmark},
author = {Madsen, Claus B and Jensen, Tommy and Andersen, Mikkel S and Christensen, Morten F and Technology, Media},
booktitle = {Proceedings: 14th Danish Conference on Pattern Recognition and Image Analysis},
pages = {1--9},
title = {{Real-time Illumination Estimation from Image Sequences}},
year = {2005}
}
@inproceedings{Komogortsev2009,
abstract = {In this paper, we introduce and evaluate a new Instantaneous Saccade (IS) selection scheme for eye gaze driven interfaces where the speed of the target selection is of utmost importance. In the IS selection scheme, target selection occurs at the start (onset) of a saccade requiring only constant amount of time to be completed. The IS performance is compared to the conventional Dwell Time (DT) selection scheme where target selection is triggered when a user fixates on an object for a certain amount of time. The IS method is also compared to the Saccade Offset (SO) selection scheme where target selection occurs at the end of a saccade. All three schemes were evaluated in terms of task completion time and the throughput of input performance in horizontal target selection task by six subjects. Results show that the Instantaneous Saccade selection was 57\% faster than the DT selection to complete a task. In terms of throughput comparison, the throughput of the IS selection is 1.9 times greater than the throughput of DT selection. We hypothesize that Instantaneous Saccade selection will be beneficial in gaming environments that require fast very interaction speeds.},
address = {New York, New York, USA},
author = {Komogortsev, Oleg V and Ryu, Young Sam and Koh, Do Hyong and Gowda, Sandeep M.},
booktitle = {Proceedings of the International Conference on Advances in Computer Enterntainment Technology - ACE '09},
doi = {10.1145/1690388.1690412},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the International Conference on Advances in Computer Enterntainment Technology - ACE '09/2009/Komogortsev et al/Komogortsev et al. - 2009 - Instantaneous saccade driven eye gaze interaction.pdf:pdf},
isbn = {9781605588643},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {140},
publisher = {ACM Press},
title = {{Instantaneous saccade driven eye gaze interaction}},
url = {http://dl.acm.org/citation.cfm?id=1690412 http://portal.acm.org/citation.cfm?doid=1690388.1690412},
year = {2009}
}
@inproceedings{Klein2004,
abstract = {This paper presents a set of technologies which enable robust, accurate, high resolution augmentation of live video, delivered via a tablet PC to which a video camera has been attached. By combining several technologies, this is achieved without the use of contrived markers in the environment: An outside-in tracker observes the tablet to generate robust, low-accuracy pose estimates. An inside-out tracker running on the tablet observes the video feed from the tablet-mounted camera and provides high-accuracy pose estimates by tracking natural features in the environment. Information from both of these trackers is combined in an extended Kalman filter. Finally, to maximise the quality of the augmented imagery, boundaries where the real world occludes the virtual imagery are identified and another tracker is used to refine the boundaries between real and virtual imagery so that their synthesis is as convincing as possible.},
address = {Nara-JP},
author = {Klein, G and Drummond, T},
booktitle = {Third IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2004.54},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Third IEEE and ACM International Symposium on Mixed and Augmented Reality/2004/Klein, Drummond/Klein, Drummond - 2004 - Sensor Fusion and Occlusion Refinement for Tablet-Based AR.pdf:pdf},
isbn = {0-7695-2191-6},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {38--47},
publisher = {IEEE},
title = {{Sensor Fusion and Occlusion Refinement for Tablet-Based AR}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1383041 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1383041},
year = {2004}
}
@article{Rashbass1961,
author = {Rashbass, C},
file = {::},
journal = {The Journal of Physiology},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{The relationship between saccadic and smooth tracking eye movements}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359508/},
year = {1961}
}
@article{Chong2013,
address = {New York, New York, USA},
author = {Chong, Ming Ki and Gellersen, Hans W.},
doi = {10.1145/2470654.2466207},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Spontaneous interaction,device association,group,guessability study,input techniques,pairing,wireless},
pages = {1559},
publisher = {ACM Press},
title = {{How groups of users associate wireless devices}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466207},
year = {2013}
}
@article{Alexiadis2013,
abstract = {The problem of robust, realistic and especially fast 3-D reconstruction of objects, although extensively studied, is still a challenging research task. Most of the state-of-the-art approaches that target real-time applications, such as immersive reality, address mainly the problem of synthesizing intermediate views for given view-points, rather than generating a single complete 3-D surface. In this paper, we present a multiple-Kinect capturing system and a novel methodology for the creation of accurate, realistic, full 3-D reconstructions of moving foreground objects, e.g., humans, to be exploited in real-time applications. The proposed method generates multiple textured meshes from multiple RGB-Depth streams, applies a coarse-to-fine registration algorithm and finally merges the separate meshes into a single 3-D surface. Although the Kinect sensor has attracted the attention of many researchers and home enthusiasts and has already appeared in many applications over the Internet, none of the already presented works can produce full 3-D models of moving objects from multiple Kinect streams in real-time. We present the capturing setup, the methodology for its calibration and the details of the proposed algorithm for real-time fusion of multiple meshes. The presented experimental results verify the effectiveness of the approach with respect to the 3-D reconstruction quality, as well as the achieved frame rates.},
author = {Alexiadis, Dimitrios S. and Zarpalas, Dimitrios and Daras, Petros},
doi = {10.1109/TMM.2012.2229264},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Multimedia/2013/Alexiadis, Zarpalas, Daras/Alexiadis, Zarpalas, Daras - 2013 - Real-Time, Full 3-D Reconstruction of Moving Foreground Objects From Multiple Consumer Depth Cameras.pdf:pdf},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
month = feb,
number = {2},
pages = {339--358},
title = {{Real-Time, Full 3-D Reconstruction of Moving Foreground Objects From Multiple Consumer Depth Cameras}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6359953\&contentType=Journals+\&+Magazines\&searchField=Search\_All\&queryText=kinect+calibration},
volume = {15},
year = {2013}
}
@inproceedings{Lu2011,
abstract = {We are interested in the problem of automatic tracking and identification of players in broadcast sport videos shot with a moving camera from a medium distance. While there are many good tracking systems, there are fewer methods that can identify the tracked players. Player identification is challenging in such videos due to blurry facial features (due to fast camera motion and low-resolution) and rarely visible jersey numbers (which, when visible, are deformed due to player movements). We introduce a new system consisting of three components: a robust tracking system, a robust person identification system, and a conditional random field (CRF) model that can perform joint probabilistic inference about the player identities. The resulting system is able to achieve a player recognition accuracy up to 85\% on unlabeled NBA basketball clips.},
author = {Lu, Wei-Lwun and Ting, Jo-Anne and Murphy, Kevin P. and Little, James J.},
booktitle = {CVPR 2011},
doi = {10.1109/CVPR.2011.5995562},
isbn = {978-1-4577-0394-2},
issn = {1063-6919},
keywords = {Cameras,Detectors,Feature extraction,Image color analysis,Tracking,Videos,Visualization,automatic tracking,basketball,blurry facial features,conditional random field model,face recognition,fast camera motion,feature extraction,image resolution,joint probabilistic inference,low resolution images,moving camera,player identification,player recognition,random processes,robust person identification system,robust tracking system,sport,sport video broadcasting,tracking,unlabeled NBA basketball clips,video signal processing},
mendeley-tags = {basketball,tracking},
month = jun,
pages = {3249--3256},
publisher = {IEEE},
shorttitle = {Computer Vision and Pattern Recognition (CVPR), 20},
title = {{Identifying players in broadcast sports videos using conditional random fields}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995562},
year = {2011}
}
@article{Plaue2014,
author = {Plaue, M and B\"{a}rwolff, G and Schwandt, H},
journal = {Pedestrian and Evacuation Dynamics  \ldots},
title = {{On measuring pedestrian density and flow fields in dense as well as sparse crowds}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-02447-9\_34},
year = {2014}
}
@book{Langton1995,
author = {Langton, Christopher G},
booktitle = {Artificial Life},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Artificial Life/1995/Langton/Langton - 1995 - Artificial Life An Overview.pdf:pdf},
isbn = {0262121891},
pages = {344},
publisher = {Bradford Books},
title = {{Artificial Life An Overview}},
year = {1995}
}
@article{Sheng2010a,
abstract = {When projectors are used to display images on complex, non-planar surface geometry, indirect illumination between the surfaces will disrupt the final appearance of this imagery, generally increasing brightness, decreasing contrast, and washing out colors. In this paper we predict through global illumination simulation this unintentional indirect component and solve for the optimal compensated projection imagery that will minimize the difference between the desired imagery and the actual total illumination in the resulting physical scene. Our method makes use of quadratic programming to minimize this error within the constraints of the physical system, namely, that negative light is physically impossible. We demonstrate our compensation optimization in both computer simulation and physical validation within a table-top spatially augmented reality system. We present an application of these results for visualization of interior architectural illumination. To facilitate interactive modifications to the scene geometry and desired appearance, our system is accelerated with a CUDA implementation of the QP optimization method.},
author = {Sheng, Yu and Yapo, Theodore C. and Cutler, Barbara},
doi = {10.1111/j.1467-8659.2009.01608.x},
issn = {01677055},
journal = {Computer Graphics Forum},
month = may,
number = {2},
pages = {387--396},
title = {{Global Illumination Compensation for Spatially Augmented Reality}},
url = {http://doi.wiley.com/10.1111/j.1467-8659.2009.01608.x},
volume = {29},
year = {2010}
}
@misc{Wikipedia2012,
author = {Wikipedia},
title = {{Teoria de Sistemas}},
url = {http://pt.wikipedia.org/wiki/Teoria\_de\_sistemas},
urldate = {10/02/2012},
year = {2012}
}
@inproceedings{Hradis2012,
abstract = {This paper discusses estimation of active speaker in multi-party video-mediated communication from gaze data of one of the participants. In the explored settings, we predict voice activity of participants in one room based on gaze recordings of a single participant in another room. The two rooms were connected by high definition, low delay audio and video links and the participants engaged in different activities ranging from casual discussion to simple problem-solving games. We treat the task as a classification problem. We evaluate several types of features and parameter settings in the context of Support Vector Machine classification framework. The results show that using the proposed approach vocal activity of a speaker can be correctly predicted in 89 \% of the time for which the gaze data are available.},
address = {New York, New York, USA},
author = {Hradis, Michal and Eivazi, Shahram and Bednarik, Roman},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12},
doi = {10.1145/2168556.2168628},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12/2012/Hradis, Eivazi, Bednarik/Hradis, Eivazi, Bednarik - 2012 - Voice activity detection from gaze in video mediated communication.pdf:pdf},
isbn = {9781450312219},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {329},
publisher = {ACM Press},
title = {{Voice activity detection from gaze in video mediated communication}},
url = {http://dl.acm.org/citation.cfm?id=2168628 http://dl.acm.org/citation.cfm?doid=2168556.2168628},
year = {2012}
}
@inproceedings{Liu2011,
abstract = {Maneuvering target tracking is a big challenge to the performance of a visual tracker. The paper proposes a method to keep the tracker robust to target maneuvering by selecting discriminative features from a large feature space, and constructing a velocity motion model with adaptive noise variance. Furthermore, the feature selection procedure is embedded into the particle filtering process with the aid of calculating the Bhattacharyya distance. Top-ranked discriminative features are selected into the observation model and simultaneously invalid features are removed out to adjust the object representation adaptively. The adaptive motion model is computed via a first-order linear predictor using the previous particle configuration. Experimental results on tracking basketball in video sequences demonstrate the effectiveness and robustness of our algorithm.},
author = {Liu, Zongli and Cao, Jie and Yuan, Zhanting},
booktitle = {2011 3rd International Workshop on Intelligent Systems and Applications},
doi = {10.1109/ISA.2011.5873268},
isbn = {978-1-4244-9855-0},
keywords = {Adaptation model,Bhattacharyya distance,Color,Computational modeling,Noise,Particle filters,Target tracking,adaptive motion model,adaptive noise variance,basketball,discriminative feature,feature extraction,feature selection procedure,first-order linear predictor,image motion analysis,image sequences,particle filtering (numerical methods),particle filtering process,target tracking,target tracking maneuver,tracking,velocity motion model,video sequences,video signal processing,visual tracker},
mendeley-tags = {basketball,tracking},
month = may,
pages = {1--4},
publisher = {IEEE},
shorttitle = {Intelligent Systems and Applications (ISA), 2011 3},
title = {{Maneuvering Target Tracking Using Adaptive Models in a Particle Filter}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5873268},
year = {2011}
}
@article{Hu2011a,
author = {Hu, Min-Chun and Chang, Ming-Hsiu and Wu, Ja-Ling and Chi, Lin},
doi = {10.1109/TMM.2010.2100373},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {Broadcast basketball video,CamShift algorithm,CamShift based tracking method,basketball,basketball video,broadcast sport video,calibration,camera calibration,feature extraction,highlight extraction,object tracking,player tracking,robust camera calibration,sport,tracking,video browsing tool,video cameras,video retrieval,video signal processing},
language = {English},
mendeley-tags = {basketball,tracking},
month = apr,
number = {2},
pages = {266--279},
publisher = {IEEE},
title = {{Robust Camera Calibration and Player Tracking in Broadcast Basketball Video}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5671490},
volume = {13},
year = {2011}
}
@inproceedings{Munn2008,
abstract = {Video-based eye trackers produce an output video showing where a subject is looking, the subject's point-of-regard (POR), for each frame of a video of the scene. Fixation-identification algorithms simplify the long list of POR data into a more manageable set of data, especially for further analysis, by grouping PORs into fixations. Most current fixation-identification algorithms assume that the POR data are defined in static two-dimensional scene images and only use these raw POR data to identify fixations. The applicability of these algorithms to gaze data in dynamic scene videos is largely unexplored. We implemented a simple velocity-based, duration-sensitive fixation-identification algorithm and compared its performance to results obtained by three experienced users manually coding the eye tracking data displayed within the scene video such that these manual coders had knowledge of the scene motion. We performed this comparison for eye tracking data collected during two different tasks involving different types of scene motion. These two tasks included a subject walking around a building for about 100 seconds (Task 1) and a seated subject viewing a computer animation (approximately 90 seconds long, Task 2). It took our manual coders on average 75 minutes (stdev = 28) and 80 minutes (17) to code results from the first and second tasks, respectively. The automatic fixation-identification algorithm, implemented in MATLAB and run on an Apple 2.16 GHz MacBook, produced results in 0.26 seconds for Task 1 and 0.21 seconds for Task 2. For the first task (walking), the average percent difference among the three human manual coders was 9\% (3.5) and the average percent difference between the automatically generated results and the three coders was 11\% (2.0). For the second task (animation), the average percent difference among the three human coders was 4\% (0.75) and the average percent difference between the automatically generated results and the three coders was 5\% (0.9).},
address = {New York, New York, USA},
annote = {- cited by: 14
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Munn, SM Susan M. and Stefano, Leanne and Pelz, Jeff B. JB},
booktitle = {Proceedings of the 5th symposium on Applied perception in graphics and visualization - APGV '08},
doi = {10.1145/1394281.1394287},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 5th symposium on Applied perception in graphics and visualization - APGV '08/2008/Munn, Stefano, Pelz/Munn, Stefano, Pelz - 2008 - Fixation-identification in dynamic scenes Comparing an automated algorithm to manual coding.pdf:pdf},
isbn = {9781595939814},
keywords = {fixation,gaze analysis},
mendeley-tags = {fixation,gaze analysis},
pages = {33},
publisher = {ACM Press},
title = {{Fixation-identification in dynamic scenes: Comparing an automated algorithm to manual coding}},
url = {http://dl.acm.org/citation.cfm?id=1394287 http://portal.acm.org/citation.cfm?doid=1394281.1394287},
year = {2008}
}
@article{B¿ottcher2007,
author = {B¿ottcher, Guido and Allerkamp, Dennis and Wolter, Franz-Erich},
doi = {10.1109/CW.2007.29},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2007 International Conference on Cyberworlds (CW'07)/2007/B¿ottcher, Allerkamp, Wolter/B¿ottcher, Allerkamp, Wolter - 2007 - Virtual Reality Systems Modelling Haptic Two-Finger Contact with Deformable Physical Surfaces.pdf:pdf},
isbn = {0-7695-3005-2},
journal = {2007 International Conference on Cyberworlds (CW'07)},
month = oct,
pages = {292--299},
publisher = {Ieee},
title = {{Virtual Reality Systems Modelling Haptic Two-Finger Contact with Deformable Physical Surfaces}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4390932},
year = {2007}
}
@inproceedings{Kumar2007,
abstract = {We present a practical technique for pointing and selection using a combination of eye gaze and keyboard triggers. EyePoint uses a two-step progressive refinement process fluidly stitched together in a look-press-look-release action, which makes it possible to compensate for the accuracy limitations of the current state-of-the-art eye gaze trackers. While research in gaze-based pointing has traditionally focused on disabled users, EyePoint makes gaze-based pointing effective and simple enough for even able-bodied users to use for their everyday computing tasks. As the cost of eye gaze tracking devices decreases, it will become possible for such gaze-based techniques to be used as a viable alternative for users who choose not to use a mouse depending on their abilities, tasks and preferences.},
address = {New York, New York, USA},
author = {Kumar, Manu and Paepcke, Andreas and Winograd, Terry},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '07},
doi = {10.1145/1240624.1240692},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '07/2007/Kumar, Paepcke, Winograd/Kumar, Paepcke, Winograd - 2007 - EyePoint practical pointing and selection using gaze and keyboard.pdf:pdf},
isbn = {9781595935939},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {421},
publisher = {ACM Press},
title = {{EyePoint: practical pointing and selection using gaze and keyboard}},
url = {http://dl.acm.org/citation.cfm?id=1240692 http://portal.acm.org/citation.cfm?doid=1240624.1240692},
year = {2007}
}
@article{Hartley2000a,
author = {Hartley, R and Zisserman, A},
file = {::},
keywords = {computer vision,geometry,multiple view,perspective},
mendeley-tags = {computer vision,geometry,multiple view,perspective},
title = {{Multiple view geometry in computer vision}},
url = {http://journals.cambridge.org/production/action/cjoGetFulltext?fulltextid=289189},
year = {2000}
}
@article{DanielHerreraC2012,
abstract = {We present an algorithm that simultaneously calibrates two color cameras, a depth camera, and the relative pose between them. The method is designed to have three key features: accurate, practical, and applicable to a wide range of sensors. The method requires only a planar surface to be imaged from various poses. The calibration does not use depth discontinuities in the depth image, which makes it flexible and robust to noise. We apply this calibration to a Kinect device and present a new depth distortion model for the depth sensor. We perform experiments that show an improved accuracy with respect to the manufacturer's calibration.},
author = {{Daniel Herrera C} and {Juho Kannala} and {Janne Heikkil\"{a}}},
doi = {10.1109/TPAMI.2012.125},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE transactions on pattern analysis and machine intelligence/2012/Daniel Herrera C, Juho Kannala, Janne Heikkil\"{a}/Daniel Herrera C, Juho Kannala, Janne Heikkil\"{a} - 2012 - Joint depth and color camera calibration with distortion correction.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = oct,
number = {10},
pages = {2058--64},
pmid = {22641701},
title = {{Joint depth and color camera calibration with distortion correction.}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6205765\&contentType=Journals+\%26+Magazines\&searchField\%3DSearch\_All\%26queryText\%3Dkinect+calibration http://www.ncbi.nlm.nih.gov/pubmed/22641701},
volume = {34},
year = {2012}
}
@article{Wang2013,
address = {New York, New York, USA},
author = {Wang, Yiran and Echenique, Andy and Shelton, Martin and Mark, Gloria},
doi = {10.1145/2470654.2481375},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2717},
publisher = {ACM Press},
title = {{A comparative evaluation of multiple chat stream interfaces for information-intensive environments}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481375},
year = {2013}
}
@article{Komogortsev2013,
abstract = {Ternary eye movement classification, which separates fixations, saccades, and smooth pursuit from the raw eye positional data, is extremely challenging. This article develops new and modifies existing eye-tracking algorithms for the purpose of conducting meaningful ternary classification. To this end, a set of qualitative and quantitative behavior scores is introduced to facilitate the assessment of classification performance and to provide means for automated threshold selection. Experimental evaluation of the proposed methods is conducted using eye movement records obtained from 11 subjects at 1000 Hz in response to a step-ramp stimulus eliciting fixations, saccades, and smooth pursuits. Results indicate that a simple hybrid method that incorporates velocity and dispersion thresholding allows producing robust classification performance. It is concluded that behavior scores are able to aid automated threshold selection for the algorithms capable of successful classification.},
author = {Komogortsev, Oleg V and Karpov, Alex},
doi = {10.3758/s13428-012-0234-9},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Behavior research methods/2013/Komogortsev, Karpov/Komogortsev, Karpov - 2013 - Automated classification and scoring of smooth pursuit eye movements in the presence of fixations and sa(2).pdf:pdf},
issn = {1554-3528},
journal = {Behavior research methods},
keywords = {Adolescent,Adult,Algorithms,Automated,Automated: methods,Biological,Eye Movement Measurements,Fixation,Humans,Models,Ocular,Pattern Recognition,Pursuit,Reaction Time,Reference Values,Saccades,Smooth,Young Adult,gaze analysis,smooth pursuit},
mendeley-tags = {gaze analysis,smooth pursuit},
month = mar,
number = {1},
pages = {203--15},
pmid = {22806708},
title = {{Automated classification and scoring of smooth pursuit eye movements in the presence of fixations and saccades.}},
url = {http://link.springer.com/article/10.3758/s13428-012-0234-9 http://www.ncbi.nlm.nih.gov/pubmed/22806708},
volume = {45},
year = {2013}
}
@article{Goyal2013,
address = {New York, New York, USA},
author = {Goyal, Nitesh and Leshed, Gilly and Fussell, Susan R.},
doi = {10.1145/2470654.2481376},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2721},
publisher = {ACM Press},
title = {{Effects of visualization and note-taking on sensemaking and analysis}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481376},
year = {2013}
}
@inproceedings{Noronha2013,
address = {Cuiaba},
author = {Noronha, Guilherme and Batista, Guilherme and Soares, Luciano Pereira},
booktitle = {2013 XV Symposium on Virtual and Augmented Reality},
doi = {10.1109/SVR.2013.50},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2013 XV Symposium on Virtual and Augmented Reality/2013/Noronha, Batista, Soares/Noronha, Batista, Soares - 2013 - Crowd Simulation with Augmented Reality.pdf:pdf},
isbn = {978-0-7695-5001-5},
keywords = {crowd simulation},
mendeley-tags = {crowd simulation},
month = may,
pages = {228--231},
publisher = {IEEE},
title = {{Crowd Simulation with Augmented Reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6655786 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6655786},
year = {2013}
}
@article{Levy2006,
author = {Levy, Y and Ellis, TJ},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Informing Science Journal/2006/Levy, Ellis/Levy, Ellis - 2006 - A systems approach to conduct an effective literature review in support of information systems research.pdf:pdf},
journal = {Informing Science Journal},
keywords = {RBS},
mendeley-tags = {RBS},
pages = {181--212},
title = {{A systems approach to conduct an effective literature review in support of information systems research}},
url = {http://www.scs.ryerson.ca/aferworn/courses/CP8101/CLASSES/ConductingLiteratureReview.pdf},
volume = {9},
year = {2006}
}
@inproceedings{Gibson2003a,
abstract = {We propose a new algorithm that uses consumer-level graphics hardware to render shadows cast by synthetic objects and a real lighting environment. This has immediate benefit for interactive Augmented Reality applications, where synthetic objects must be accurately merged with real images. We show how soft shadows cast by direct and indirect illumination sources may be generated and composited into a background image at interactive rates. We describe how the sources of light (and hence shadow) affecting each point in an image can be efficiently encoded using a hierarchical shaft-based subdivision of line-space. This subdivision is then used to determine the sources of light that are occluded by synthetic objects, and we show how the contributions from these sources may be removed from a background image using facilities available on modern graphics hardware. A trade-off may be made at run-time between shadow accuracy and rendering cost, converging towards a result that is subjectively similar to that obtained using ray-tracing based differential rendering algorithms. Examples of the proposed technique are given for a variety of different lighting environments, and the visual fidelity of images generated by our algorithm is compared to both real photographs and synthetic images generated using non-real-time techniques.},
address = {Leuven, Belgium},
author = {Gibson, Simon and Cook, Jon and Howard, Toby and Hubbold, Roger},
booktitle = {Proceedings of the 14th Eurographics Workshop on Rendering},
pages = {219--229},
publisher = {Eurographics Association},
title = {{Rapid Shadow Generation in Real-World Lighting Environments}},
url = {http://dl.acm.org/citation.cfm?id=882404.882436},
year = {2003}
}
@inproceedings{Divjak2009,
address = {Tokyo, Japan},
author = {Divjak, M and Bischof, H},
booktitle = {IAPR Conference on Machine Vision Applications - MVA},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IAPR Conference on Machine Vision Applications - MVA/2009/Divjak, Bischof/Divjak, Bischof - 2009 - Eye Blink Based Fatigue Detection for Prevention of Computer Vision Syndrome.pdf:pdf},
keywords = {blink,gaze analysis},
mendeley-tags = {blink,gaze analysis},
pages = {350--353},
title = {{Eye Blink Based Fatigue Detection for Prevention of Computer Vision Syndrome.}},
url = {http://www.mva-org.jp/Proceedings/2009CD/papers/10-04.pdf},
year = {2009}
}
@article{Musthag2013,
address = {New York, New York, USA},
author = {Musthag, Mohamed and Ganesan, Deepak},
doi = {10.1145/2470654.2470745},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {641},
publisher = {ACM Press},
title = {{Labor dynamics in a mobile micro-task market}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470745},
year = {2013}
}
@article{Peterka2008,
abstract = {A solid-state dynamic parallax barrier autostereoscopic display mitigates some of the restrictions present in static barrier systems, such as fixed view-distance range, slow response to head movements, and fixed stereo operating mode. By dynamically varying barrier parameters in real time, viewers may move closer to the display and move faster laterally than with a static barrier system, and the display can switch between 3D and 2D modes by disabling the barrier on a per-pixel basis. Moreover, Dynallax can output four independent eye channels when two viewers are present, and both head-tracked viewers receive an independent pair of left-eye and right-eye perspective views based on their position in 3D space. The display device is constructed by using a dual-stacked LCD monitor where a dynamic barrier is rendered on the front display and a modulated virtual environment composed of two or four channels is rendered on the rear display. Dynallax was recently demonstrated in a small-scale head-tracked prototype system. This paper summarizes the concepts presented earlier, extends the discussion of various topics, and presents recent improvements to the system.},
author = {Peterka, Tom and Kooima, Robert L and Sandin, Daniel J and Johnson, Andrew and Leigh, Jason and DeFanti, Thomas A},
doi = {10.1109/TVCG.2007.70627},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE transactions on visualization and computer graphics/2008/Peterka et al/Peterka et al. - 2008 - Advances in the Dynallax solid-state dynamic parallax barrier autostereoscopic visualization display system.pdf:pdf},
issn = {1077-2626},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Algorithms,Biomedical,Computer Graphics,Computer-Assisted,Computer-Assisted: instrumen,Computer-Assisted: instrumentat,Computer-Assisted: methods,Data Display,Equipment Design,Equipment Failure Analysis,Image Enhancement,Image Enhancement: instrumentation,Image Enhancement: methods,Image Interpretation,Imaging,Numerical Analysis,Photogrammetry,Photogrammetry: instrumentation,Photogrammetry: methods,Signal Processing,Technology Assessment,Three-Dimensional,Three-Dimensional: instrumentation,Three-Dimensional: methods,User-Computer Interface},
month = jan,
number = {3},
pages = {487--99},
pmid = {18369259},
title = {{Advances in the Dynallax solid-state dynamic parallax barrier autostereoscopic visualization display system.}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4407697},
volume = {14},
year = {2008}
}
@article{Niemenlehto2009,
abstract = {The analysis of eye movements has proven to be valuable in both clinical work and research as well as in other fields besides medicine. The detection of saccadic eye movements and the extraction of related saccade parameters, such as maximum angular velocity, amplitude, and duration, are usually performed during the analysis of electro-oculographic (EOG) signals. This article considers a saccade detection method that is based on the constant false alarm rate technique, in which the detection sensitivity is continuously adjusted on the basis of the observed signal in order to keep the number of false alarms constant. The method is computationally efficient, it can operate autonomously without user intervention, and it is capable of detecting saccades in a sequential fashion. Therefore, the method finds potential use in applications that require automated analysis of electro-oculographic signals. Because of the constant false alarm rate property, the method can also perform in situations where ideal measurement conditions cannot be guaranteed and noise presents a considerable problem.},
author = {Niemenlehto, PH},
doi = {10.1016/j.cmpb.2009.04.011},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer methods and programs in biomedicine/2009/Niemenlehto/Niemenlehto - 2009 - Constant false alarm rate detection of saccadic eye movements in electro-oculography.pdf:pdf},
issn = {1872-7565},
journal = {Computer methods and programs in biomedicine},
keywords = {Algorithms,Artifacts,Automated,Automated: methods,Computer-Assisted,Computer-Assisted: methods,Diagnosis,Electrooculography,Electrooculography: methods,Humans,Pattern Recognition,Reproducibility of Results,Saccades,Saccades: physiology,Sensitivity and Specificity,Signal Processing,gaze analysis},
mendeley-tags = {gaze analysis},
month = nov,
number = {2},
pages = {158--71},
pmid = {19482371},
title = {{Constant false alarm rate detection of saccadic eye movements in electro-oculography.}},
url = {http://www.sciencedirect.com/science/article/pii/S0169260709001564 http://www.ncbi.nlm.nih.gov/pubmed/19482371},
volume = {96},
year = {2009}
}
@article{Azuma1995,
abstract = {This paper surveys the field of Augmented Reality, in which 3-D virtual objects are integrated into a 3-D real environment in real time. It describes the medical, manufacturing, visualization, path planning, entertainment and military applications that have been explored. This paper describes the characteristics of Augmented Reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective Augmented Reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using Augmented Reality.},
address = {Los Angeles},
author = {Azuma, Ronald T},
doi = {10.1.1.35.5387},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Presence Teleoperators and Virtual Environments/1995/Azuma/Azuma - 1995 - A Survey of Augmented Reality.pdf:pdf},
journal = {Presence: Teleoperators and Virtual Environments},
number = {4},
pages = {355--385},
publisher = {ACM},
title = {{A Survey of Augmented Reality}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.5387},
volume = {6},
year = {1995}
}
@article{Takahashi1983,
abstract = {The effects of a change in target speed (10°–100°/s) and amplitude (10°–80°) on smooth pursuit eye movements were analyzed in normal subjects by moving a target unidirectionally. The limit of pursuit speed adaptation changed according to changes in the target amplitude, being about 40°–50°/s at a target amplitude of 40°–80°. The minimum target amplitude needed to induce rhythmic pursuit eye movements markedly increased from 3.7° at 10°/s to 55.0° at 100°/s. The relationship between the gain (the ratio of eye speed to target speed) and the pursuit time suggested that pursuit eye speeds may depend on the pursuit time rather than absolute target speeds and that the gain might become unity even at fast target speeds, provided the critical pursuit time is given.},
author = {Takahashi, Masahiro and Uemura, Takuya and Fujishiro, Takehisa},
doi = {10.1007/BF00453933},
issn = {0302-9530},
journal = {Archives of Oto-Rhino-Laryngology},
keywords = {gaze analysis,smooth pursuit},
mendeley-tags = {gaze analysis,smooth pursuit},
month = oct,
number = {3},
pages = {225--232},
title = {{Quantitative analysis of pursuit eye movements by unidirectional target motion}},
url = {http://link.springer.com/10.1007/BF00453933},
volume = {238},
year = {1983}
}
@article{Klopfer2008,
author = {Klopfer, E and Squire, K},
journal = {Educational Technology Research and Development},
keywords = {augmented reality,simulation},
mendeley-tags = {augmented reality,simulation},
title = {{Environmental Detectives—the development of an augmented reality platform for environmental simulations}},
url = {http://link.springer.com/article/10.1007/s11423-007-9037-6},
year = {2008}
}
@article{Wallace2013a,
address = {New York, New York, USA},
author = {Wallace, Jayne and McCarthy, John and Wright, Peter C. and Olivier, Patrick},
doi = {10.1145/2470654.2466473},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3441},
publisher = {ACM Press},
title = {{Making design probes work}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466473},
year = {2013}
}
@techreport{Olsen2012,
abstract = {This document describes the general principles behind an I-VT fixation filter and they are implemented in the Tobii I-VT Fixation Filter.},
annote = {- cited by: 1
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Olsen, A},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2012/Olsen/Olsen - 2012 - The Tobii I-VT Fixation Filter.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {21},
title = {{The Tobii I-VT Fixation Filter}},
url = {http://www.tobii.com/Global/Analysis/Training/WhitePapers/Tobii\_WhitePaper\_TobiiIVTFixationFilter.pdf},
year = {2012}
}
@article{Chang2007,
abstract = {In recent years, the success of single-robot SLAM has led to more multi-robot SLAM (MR-SLAM) research. A team of robots with MR-SLAM can explore an environment more efficiently and reliably; however, MR-SLAM also raises many challenging problems, including map fusion, unknown robot poses and scalability issues. The first two problems can be considered as an optimization problem of finding a consistent joint map based on robots\&x2019; relative poses and sensory data. This optimization problem exhibits a similar property of a singlerobot topological/metric mapping. To exploit this property, we propose a multi-robot SLAM (MR-SLAM) algorithm, which builds a graph-like topological map with vertices representing local metric maps and edges describing relative positions of adjacent local maps. In this MR-SLAM algorithm, the map fusion between two robots can be naturally done by adding an edge that connects two topological maps, and the estimation of relative robot pose is simply performed by optimizing this edge. For the third scalable problem, the proposed algorithm is also scalable to the number of robots and the size of an environment. Computer simulations with a public data set and experimental work on Pioneer 3-DX robots have been conducted to validate the performance of the proposed MR-SLAM algorithm.},
author = {Chang, H Jacky and Lee, C S George and Hu, Y Charlie and {Yung-Hsiang Lu}, A Yung-Hsiang Lu},
journal = {2007 IEEERSJ International Conference on Intelligent Robots and Systems},
pages = {1467--1472},
publisher = {Ieee},
title = {{Multi-robot SLAM with topological/metric maps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4399142},
year = {2007}
}
@misc{Wiering2012,
author = {Wiering, Mike},
keywords = {SBGames},
mendeley-tags = {SBGames},
title = {{TileStudio}},
url = {http://tilestudio.sourceforge.net/},
urldate = {2013/07/26},
year = {2012}
}
@inproceedings{Ohmori2009,
abstract = {For designing and modeling complicated and sophisticated systems such as cyberworlds, their mathematical foundation is critical. To realize it, two important properties called the homotopy lifting property and homotopy extension property are applied for designing and modeling a system in a bottom-up way and a top-down way, respectively. Activities of Internet Company are described by pi-calculus processes and a Petri net which are derived from system requirements in a bottom-up way and a top-down way using the homotopy lifting property and the homotopy extension property. Entities in both properties are specified by the incrementally modular abstraction hierarchy by climbing down the abstraction hierarchy from the most abstract homotopy level to the most specific view level, while keeping invariants such as homotopy equivalence or topological equivalence.},
address = {Bradford},
author = {Ohmori, Kenji and Kunii, Tosiyasu L.},
booktitle = {2009 International Conference on CyberWorlds},
doi = {10.1109/CW.2009.20},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2009 International Conference on CyberWorlds/2009/Ohmori, Kunii/Ohmori, Kunii - 2009 - Mathematical Foundation for Designing and Modeling Cyberworlds.pdf:pdf},
isbn = {978-1-4244-4864-7},
pages = {80--87},
publisher = {IEEE},
title = {{Mathematical Foundation for Designing and Modeling Cyberworlds}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5279683 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5279683},
year = {2009}
}
@inproceedings{Spakov2012,
abstract = {We compared various real-time filters designed to denoise eye movements from low-sampling devices. Most of the filters found in literature were implemented and tested on data gathered in a previous study. An improvement was proposed for one of the filters. Parameters of each filter were adjusted to ensure their best performance. Four estimation parameters were proposed as criteria for comparison. The output from the filters was compared against two idealized signals (the signals denoised offline). The study revealed that FIR filters with triangular or Gaussian kernel (weighting) functions and parameters dependent on signal state show the best performance.},
address = {New York, New York, USA},
author = {\v{S}pakov, Oleg},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12},
doi = {10.1145/2168556.2168616},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12/2012/\v{S}pakov/\v{S}pakov - 2012 - Comparison of eye movement filters used in HCI(2).pdf:pdf},
isbn = {9781450312219},
keywords = {Gaze analysis},
mendeley-tags = {Gaze analysis},
pages = {281},
publisher = {ACM Press},
title = {{Comparison of eye movement filters used in HCI}},
url = {http://dl.acm.org/citation.cfm?id=2168616 http://dl.acm.org/citation.cfm?doid=2168556.2168616},
year = {2012}
}
@article{Tappen2005,
abstract = {Interpreting real-world images requires the ability distinguish the different characteristics of the scene that lead to its final appearance. Two of the most important of these characteristics are the shading and reflectance of each point in the scene. We present an algorithm that uses multiple cues to recover shading and reflectance intrinsic images from a single image. Using both color information and a classifier trained to recognize gray-scale patterns, given the lighting direction, each image derivative is classified as being caused by shading or a change in the surface's reflectance. The classifiers gather local evidence about the surface's form and color, which is then propagated using the Generalized Belief Propagation algorithm. The propagation step disambiguates areas of the image where the correct classification is not clear from local evidence. We use real-world images to demonstrate results and show how each component of the system affects the results.},
author = {Tappen, Marshall F and Freeman, William T and Adelson, Edward H},
doi = {10.1109/TPAMI.2005.185},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Colorimetry,Colorimetry: methods,Computer Graphics,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Numerical Analysis, Computer-Assisted,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Signal Processing, Computer-Assisted},
month = sep,
number = {9},
pages = {1459--72},
pmid = {16173188},
title = {{Recovering intrinsic images from a single image.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16173188},
volume = {27},
year = {2005}
}
@misc{Goran2010,
author = {Goran, C},
booktitle = {US Patent App. 12/848,201},
file = {:home/acmt/Dropbox/Documentos/Mendeley/US Patent App. 12848,201/2010/Goran/Goran - 2010 - Anamorphic projection device.pdf:pdf},
keywords = {anamorphism},
mendeley-tags = {anamorphism},
title = {{Anamorphic projection device}},
url = {http://www.google.com.br/patents?hl=pt-BR\&lr=\&vid=USPATAPP12848201\&id=oacCAgAAEBAJ\&oi=fnd\&dq=anamorphism+handheld+projector\&printsec=abstract\#v=onepage\&q=anamorphism handheld projector\&f=false},
year = {2010}
}
@article{Fuchs1967,
abstract = {Voluntary eye movements were measured in the chronic, unanaesthetized monkey. A training technique is described which conditions the animals to follow a large variety of target trajectories. The eye movements of the monkey are not qualitatively different from those of man. In response to random target motions the monkey also employs a combination of saccadic and smooth pursuit movements. Monkeys execute their saccades more rapidly than humans. Monkeys are capable of attaining smooth pursuit velocities which are twice as fast as those of man. Most of the critical experiments showing the separate nature of the saccadic and smooth pursuit modes in man have been performed on monkeys with similar results. Therefore, if one remains aware of the quantitative differences between the two primates, results of neurophysiological studies of the occulomotor system of the monkey can be expected to have considerable relevance when extrapolated to man.},
author = {Fuchs, AF},
file = {::},
journal = {The Journal of physiology},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Saccadic and smooth pursuit eye movements in the monkey}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1365495/},
year = {1967}
}
@inproceedings{Urbina2007,
author = {Urbina, MH and Huckauf, A},
booktitle = {Proceedings of the 3rd Conference on Communication by Gaze Interaction},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 3rd Conference on Communication by Gaze Interaction/2007/Urbina, Huckauf/Urbina, Huckauf - 2007 - Dwell time free eye typing approaches(2).pdf:pdf},
keywords = {dwell free,eye typing},
mendeley-tags = {dwell free,eye typing},
pages = {65--70},
title = {{Dwell time free eye typing approaches}},
url = {http://scholar.google.com.br/scholar?q='Dwell+time+free+eye+typing+approaches"\&btnG=\&hl=pt-BR\&as\_sdt=0,5\#0},
year = {2007}
}
@inproceedings{Wang2006,
address = {Montreal},
author = {Wang, Xiangyu and Dunston, P.S.},
booktitle = {Proceedings of Joint International Conference on Computing and Decision Making in Civil and Building Engineering. Montreal, Canad\'{a}:[sn]},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of Joint International Conference on Computing and Decision Making in Civil and Building Engineering. Montreal, Canad\'{a}sn/2006/Wang, Dunston/Wang, Dunston - 2006 - Groupware Concepts for Augmented Reality Mediated Human-To-Human Collaboration.pdf:pdf},
keywords = {augmented reality,computer-supported cooperative work,cscw,groupware,mobility},
pages = {1836--1842},
title = {{Groupware Concepts for Augmented Reality Mediated Human-To-Human Collaboration}},
url = {http://itc.scix.net/data/works/att/w78-2006-tf278.pdf},
year = {2006}
}
@article{Almansa2003,
abstract = {Even though vanishing points in digital images result from parallel lines in the 3D scene, most of the proposed detection algorithms are forced to rely heavily either on additional properties (like orthogonality or coplanarity and equal distance) of the underlying 3D lines, or on knowledge of the camera calibration parameters, in order to avoid spurious responses. In this work, we develop a new detection algorithm that relies on the Helmoltz principle recently proposed for computer vision by Desolneux et al (2001; 2003), both at the line detection and line grouping stages. This leads to a vanishing point detector with a low false alarms rate and a high precision level, which does not rely on any a priori information on the image or calibration parameters, and does not require any parameter tuning.},
annote = {        From Duplicate 1 (                           Vanishing point detection without any a priori information                         - Almansa, a.; Desolneux, a.; Vamech, S. )
                
        From Duplicate 2 (                           Vanishing point detection without any a priori information                         - Almansa, A; Desolneux, A; Vamech, S )
                
        
        
        
        
      },
author = {Almansa, a. and Desolneux, A. and Vamech, S.},
doi = {10.1109/TPAMI.2003.1190575},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Pattern Analysis and Machine Intelligence/2003/Almansa, Desolneux, Vamech/Almansa, Desolneux, Vamech - 2003 - Vanishing point detection without any a priori information.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = apr,
number = {4},
pages = {502--507},
title = {{Vanishing point detection without any a priori information}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1190575 http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1190575},
volume = {25},
year = {2003}
}
@inproceedings{Fischer2005b,
abstract = {The ultimate goal of augmented reality is to provide the user with a view of the surroundings enriched by virtual objects. Practically all augmented reality systems rely on standard real-time rendering methods for generating the images of virtual scene elements. Although such conventional computer graphics algorithms are fast, they often fail to produce sufficiently realistic renderings. The use of simple lighting and shading methods, as well as the lack of knowledge about actual lighting conditions in the real surroundings, cause virtual objects to appear artificial. In this paper, we propose an entirely novel approach for generating augmented reality images in video see-through systems. Our method is based on the idea of applying stylization techniques for reducing the visual realism of both the camera image and the virtual graphical objects. A special painterly image filter is applied to the camera video stream. The virtual scene elements are generated using a non-photorealistic rendering method. Since both the camera image and the virtual objects are stylized in a corresponding "cartoon-like" or "sketch-like" way, they appear very similar. As a result, the graphical objects seem to be an actual part of the real surroundings. We describe both the new painterly filter for the camera image and the non-photorealistic rendering method for virtual scene elements, which has been adapted for this purpose. Both are fast enough for generating augmented reality images in real-time and are highly customizable. The results obtained using our method, are very promising and show that it improves immersion in augmented reality.},
address = {Bonn},
author = {Fischer, J and Bartz, D and Strasser, W.},
booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
doi = {10.1109/VR.2005.1492774},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Proceedings. VR 2005. Virtual Reality, 2005/2005/Fischer, Bartz, Strasser/Fischer, Bartz, Strasser - 2005 - Stylized augmented reality for improved immersion.pdf:pdf;:home/acmt/Dropbox/Documentos/Mendeley/IEEE Proceedings. VR 2005. Virtual Reality, 2005/2005/Fischer, Bartz, Strasser/Fischer, Bartz, Strasser - 2005 - Stylized augmented reality for improved immersion(2).pdf:pdf},
isbn = {0-7803-8929-8},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
pages = {195--325},
publisher = {IEEE},
title = {{Stylized augmented reality for improved immersion}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1492774 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1492774},
year = {2005}
}
@article{Santos2011,
abstract = {Current GPU computational power enables the execution of complex and parallel algorithms, such as ray tracing techniques supported by kD-trees for 3D scene rendering in real time. This work describes in detail the study and implementation of eight different kD-tree traversal algorithms using the parallel framework NVIDIA Compute Unified Device Architecture, in order to point their pros and cons regarding performance, memory consumption, branch divergencies and scalability on multiple GPUs. In addition, two new algorithms are proposed by the authors based on this analysis, aiming to performance improvement. Both of them are capable of reaching speedup gains up to 3 × when compared to recent and optimized parallel traversal implementations. As a consequence, interactive frame rates are possible for scenes with 1,408 × 768 pixels of resolution and 3.6 million primitives.},
author = {Santos, Artur and Teixeira, Jo\~{a}o Marcelo and Farias, Thiago and Teichrieb, Veronica and Kelner, Judith},
doi = {10.1007/s10766-011-0186-1},
issn = {0885-7458},
journal = {International Journal of Parallel Programming},
keywords = {cuda,kd-tree,ray tracing,traversal},
month = sep,
number = {3},
pages = {331--352},
title = {{Understanding the Efficiency of kD-tree Ray-Traversal Techniques over a GPGPU Architecture}},
url = {http://link.springer.com/10.1007/s10766-011-0186-1},
volume = {40},
year = {2011}
}
@article{Ashdown2003,
abstract = {We present a robust calibration method for aligning a camera-projector system to multiple planar surfaces. Un- like prior work, we do not recover the 3D scene geometry, nor do we assume knowledge of projector or camera posi- tion. We recover the mapping between the projector and each surface in three stages. In the first stage, we recover pla- nar homographies between the projector and the camera through each surface using an uncalibrated variant of struc- tured light. In the second stage, we express the homogra- phies from the camera to each display surface as the com- position of a metric rectification and a similarity transform. Our metric rectification algorithm uses several images of a rectangular object. In the third stage, we obtain the homo- graphies between the projector and each surface by combin- ing the results of the previous two stages. Inconsistencies appear along the boundaries between adjacent surfaces; we eliminate them through a process of iterative refinement. Standard techniques for recovering homographies from line correspondences and performing metric rectification are very sensitive to image processing outliers. We present robust algorithms for both tasks, and confirm that accuracy is maintained in the presence of outliers, both in simulation and on our interactive application that spans a table and ad- jacent wall. Our calibration method enables users to quickly set up multi-planar displays as they are needed, using any avail- able projector and camera. These displays could be applied to visualization tasks in medical imaging, architecture and geographic information systems.},
author = {Ashdown, M and Sukthankar, R},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Laboratories Tech Report HPL-2003-24/2003/Ashdown, Sukthankar/Ashdown, Sukthankar - 2003 - Robust calibration of camera-projector system for multi-planar displays.pdf:pdf},
journal = {Laboratories Tech Report HPL-2003-24},
keywords = {anamorphism},
mendeley-tags = {anamorphism},
title = {{Robust calibration of camera-projector system for multi-planar displays}},
url = {http://www.cse.iitb.ac.in/graphics/~rocky/website/mtp/papers/Ashdown-HPL-2003-24.pdf},
year = {2003}
}
@article{Noh2010,
abstract = {The most challenging task in developing Augmented Reality (AR) applications is to make virtual objects mixed harmoniously with the real scene. To achieve photorealistic AR environment, three key issues must be emphasized namely consistency of geometry, illumination and time. Shadow is an essential element to improve visual perception and realism. Without shadow, virtual objects will appear like it is floating and thus will make the environment look unrealistic. However, many shadow algorithms still have drawbacks such as producing sharp and hard-edged outlines, which make the shadow’s appearance unrealistic. Thus, this paper will focus on generating soft shadow in AR scene, rendered base on real light sources position. The reflective sphere is used to create environment map image that can estimate the light source from the real scene and generate the soft shadows.},
author = {Noh, Z and Sunar, MS},
file = {::},
journal = {Advances in Multimedia-An International Journal},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
number = {2},
pages = {26--36},
title = {{Soft shadow rendering based on real light source estimation in augmented reality}},
url = {http://www.cscjournals.org/csc/manuscript/Journals/AMIJ/volume1/Issue2/AMIJ-10.pdf},
volume = {1},
year = {2010}
}
@inproceedings{Behzadan2008,
author = {Behzadan, Amir H. and Kamat, Vineet R},
booktitle = {2008 Winter Simulation Conference},
doi = {10.1109/WSC.2008.4736353},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2008 Winter Simulation Conference/2008/Behzadan, Kamat/Behzadan, Kamat - 2008 - Simulation and visualization of traffic operations in Augmented Reality for improved planning and design of roa.pdf:pdf},
isbn = {978-1-4244-2707-9},
month = dec,
pages = {2447--2454},
publisher = {IEEE},
title = {{Simulation and visualization of traffic operations in Augmented Reality for improved planning and design of road construction projects}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4736353},
year = {2008}
}
@article{Myers2005,
abstract = {The ability to objectively compare scanpaths enables researchers to study the adoption and evolution of visual search strategies within complex task environments. Current eye tracking technology is enhancing the researcher's ability to collect scanpaths. However, a drawback of using eye tracking technology is the difficulty in objectively analyzing portions of the obtained data. For example, the ability to compare two complete scanpaths, two subsections within the same scanpath, or two subsections of two different scanpaths has been elusive. Recent research has applied objective measures to scanpaths (see Goldberg \& Kotval, 1999; Ponsoda, Scott, \& Findlay, 1995). However, such measures fail to utilize the temporal flow of dwells composing scanpaths (e.g., scanpath length or convex hull area). I present a novel method to objectively determine scanpath similarity that includes the temporal flow of dwells. A sequence alignment algorithm adopted from bioinformatics is used to determine whether areas of interest in a complex task environment are foveated in the same temporal order over repeated trials and between participants. The algorithm determines the minimum number of edits necessary to change one scanpath into another - the smaller the number of edits the greater the similarity between the compared scanpaths. To demonstrate the usefulness of scanpath comparisons, all scanpaths from two conditions of an empirical study were compared for two participants (one per condition). Results demonstrate that prototypical scanpaths are determinable through sequence alignment, and represent search strategies. Results also demonstrate that such strategies are attained and settled on early in the task, and that two or more strategies likely compete over the course of the task. Furthermore, prototypical search strategies for the two subjects were similar, but differed systematically, suggesting that minute changes in task environments subtly change visual search strategies.},
author = {Myers, C. W.},
doi = {10.1167/5.8.693},
issn = {1534-7362},
journal = {Journal of Vision},
keywords = {scanpath,similarity},
mendeley-tags = {scanpath,similarity},
month = mar,
number = {8},
pages = {693--693},
title = {{Toward a method of objectively determining scanpath similarity}},
url = {http://www-mtl.journalofvision.org/content/5/8/693.short http://www.journalofvision.org/lookup/doi/10.1167/5.8.693},
volume = {5},
year = {2010}
}
@article{Grasset2006,
author = {Grasset, R. and Gascuel, J.-D.},
doi = {10.1109/ISMAR.2003.1240731},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings/2006/Grasset, Gascuel/Grasset, Gascuel - 2006 - Interactive mediated reality.pdf:pdf},
isbn = {0-7695-2006-5},
journal = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
pages = {302--303},
publisher = {IEEE Comput. Soc},
title = {{Interactive mediated reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240731},
year = {2006}
}
@misc{Accattato,
author = {Accattato, Dominick},
title = {{Red5}},
url = {http://www.red5.org},
urldate = {17/01/2010}
}
@article{Zhang2000,
author = {Zhang, Z},
doi = {10.1109/34.888718},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Pattern Analysis and Machine Intelligence/2000/Zhang/Zhang - 2000 - A flexible new technique for camera calibration.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1330--1334},
title = {{A flexible new technique for camera calibration}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=888718 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=888718},
volume = {22},
year = {2000}
}
@inproceedings{Wloka1995,
abstract = {Current state-of-the-art augmented reality systems simply overlay computer-generated visuals on the real-world imagery, for example via video or optical see-through displays. However, overlays are not effective when displaying data in three dimensions, since occlusion between the real and computer-generated objects is not addressed.We present a video see-through augmented reality system capable of resolving occlusion between real and computer-generated objects. The heart of our system is a new algorithm that assigns depth values to each pixel in a pair of stereo video images in near-real-time. The algorithm belongs to the class of stereo matching algorithms and thus works in fully dynamic environments. We describe our system in general and the stereo matching algorithm in particular.},
address = {New York, New York, USA},
author = {Wloka, Matthias M. and Anderson, Brian G.},
booktitle = {Proceedings of the 1995 symposium on Interactive 3D graphics - SI3D '95},
doi = {10.1145/199404.199405},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 1995 symposium on Interactive 3D graphics - SI3D '95/1995/Wloka, Anderson/Wloka, Anderson - 1995 - Resolving occlusion in augmented reality.pdf:pdf},
isbn = {0897917367},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {5--12},
publisher = {ACM Press},
title = {{Resolving occlusion in augmented reality}},
url = {http://dl.acm.org/citation.cfm?id=199405 http://portal.acm.org/citation.cfm?doid=199404.199405},
year = {1995}
}
@article{Bederson1995,
author = {Bederson, BB},
journal = {Conference companion on Human factors in computing \ldots},
keywords = {augmented reality,sound},
mendeley-tags = {augmented reality,sound},
title = {{Audio augmented reality: a prototype automated tour guide}},
url = {http://dl.acm.org/citation.cfm?id=223526},
year = {1995}
}
@article{Zhan2008,
abstract = {In the year 1999 the world population reached 6 billion, doubling the previous census estimate of 1960. Recently, the United States Census Bureau issued a revised forecast for world population showing a projected growth to 9.4 billion by 2050 (US Census Bureau, http://www.census.gov/ipc/www/worldpop.html). Different research disci- plines have studied the crowd phenomenon and its dynamics from a social, psychological and computational standpoint respectively. This paper presents a survey on crowd analysis methods employed in computer vision research and discusses perspectives from other research disciplines and how they can contribute to the computer vision approach.},
author = {Zhan, Beibei and Monekosso, Dorothy N. and Remagnino, Paolo and Velastin, Sergio a. and Xu, Li-Qun},
doi = {10.1007/s00138-008-0132-4},
issn = {0932-8092},
journal = {Machine Vision and Applications},
keywords = {computer vision,crowd dynamics,crowd simulations,crowd studies,ingly more frquent and,large concerts,on,popular events such as,public demonstrations and so,socio-dynamics,sport matches,to avoid},
month = apr,
number = {5-6},
pages = {345--357},
title = {{Crowd analysis: a survey}},
url = {http://link.springer.com/10.1007/s00138-008-0132-4},
volume = {19},
year = {2008}
}
@article{Veas2012,
abstract = {In this paper, we explore techniques that aim to improve site understanding for outdoor Augmented Reality (AR) applications. While the first person perspective in AR is a direct way of filtering and zooming on a portion of the data set, it severely narrows overview of the situation, particularly over large areas. We present two interactive techniques to overcome this problem: multi-view AR and variable perspective view. We describe in details the conceptual, visualization and interaction aspects of these techniques and their evaluation through a comparative user study. The results we have obtained strengthen the validity of our approach and the applicability of our methods to a large range of application domains.},
author = {Veas, Eduardo and Grasset, Rapha\"{e}l and Kruijff, Ernst and Schmalstieg, Dieter},
doi = {10.1109/TVCG.2012.44},
issn = {1941-0506},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Adult,Computer Graphics,Environment,Female,Humans,Male,User-Computer Interface},
month = apr,
number = {4},
pages = {565--72},
pmid = {22402683},
title = {{Extended overview techniques for outdoor augmented reality.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22402683},
volume = {18},
year = {2012}
}
@article{Treuille2006,
abstract = {We present a real-time crowd model based on continuum dynamics. In our model, a dynamic potential field simultaneously integrates global navigation with moving obstacles such as other people, efficiently solving for the motion of large crowds without the need for explicit collision avoidance. Simulations created with our system run at interactive rates, demonstrate smooth flow under a variety of conditions, and naturally exhibit emergent phenomena that have been observed in real crowds.},
author = {Treuille, Adrien and Cooper, Seth and Popovi\'{c}, Zoran},
doi = {10.1145/1141911.1142008},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {crowds,human simulation,motion planning},
month = jul,
number = {3},
pages = {1160},
title = {{Continuum crowds}},
url = {http://portal.acm.org/citation.cfm?doid=1141911.1142008},
volume = {25},
year = {2006}
}
@inproceedings{Jung2014,
abstract = {This paper introduces a mobile augmented reality system using orientation sensor to estimate environment lights. The proposed system extracts the lighting direction and intensity from an input image by a front-side camera of mobile device. Experiment results show that this method generates more natural image with multiple smooth shadows in realtime.},
address = {Las Vegas, NV},
author = {Jung, Yeongseok and Choi, Euna and Hong, Hyunki},
booktitle = {Consumer Electronics (ICCE), 2014 IEEE International Conference on},
doi = {10.1109/ICCE.2014.6775904},
isbn = {9781479912919},
pages = {53--54},
publisher = {IEEE Computer Society},
title = {{Using orientation sensor of smartphone to reconstruct environment lights in augmented reality}},
year = {2014}
}
@article{Ullrich2007,
abstract = {In this article, we present two algorithms for precise collision detection between two potentially colliding objects. The first one uses axis-aligned bounding boxes (AABB) and is a typical representative of a computational geometry algorithm. The second one uses spherical distance fields originating in image processing. Both approaches addresses the following challenges of collision detection algorithms: just in time, little resources, inclusive etc. Thus both approaches are scalable in the information they give in collision determination and the analysis up to a fixed refinement level, the collision time depends on the granularity of the bounding volumes and it is also possible to estimate the time bounds for the collision test tightly},
author = {Ullrich, Torsten and Funfzig, Christoph and Fellner, Dieter},
doi = {10.1109/MP.2007.343037},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Potentials/2007/Ullrich, Funfzig, Fellner/Ullrich, Funfzig, Fellner - 2007 - Two different views on collision detection.pdf:pdf},
issn = {0278-6648},
journal = {IEEE Potentials},
month = jan,
number = {1},
pages = {26--30},
title = {{Two different views on collision detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4147707},
volume = {26},
year = {2007}
}
@inproceedings{Tsuruta2007,
author = {Tsuruta, Seiya and Kawauchi, Yamato and Choi, Woong and Hachimura, Kozaburo},
booktitle = {17th International Conference on Artificial Reality and Telexistence (ICAT 2007)},
doi = {10.1109/ICAT.2007.37},
file = {:home/acmt/Dropbox/Documentos/Mendeley/17th International Conference on Artificial Reality and Telexistence (ICAT 2007)/2007/Tsuruta et al/Tsuruta et al. - 2007 - Real-Time Recognition of Body Motion for Virtual Dance Collaboration System.pdf:pdf},
isbn = {0-7695-3056-7},
month = nov,
pages = {23--30},
publisher = {IEEE},
title = {{Real-Time Recognition of Body Motion for Virtual Dance Collaboration System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4414612},
year = {2007}
}
@article{Heibeck2013,
address = {New York, New York, USA},
author = {Heibeck, Felix},
doi = {10.1145/2468356.2479578},
isbn = {9781450319522},
journal = {CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13},
keywords = {game design,physical computing,tangible play},
pages = {4503},
publisher = {ACM Press},
title = {{Cuboino. Extending Physical Games. An Example}},
url = {http://dl.acm.org/citation.cfm?doid=2468356.2479578},
year = {2013}
}
@inproceedings{Debevec2008a,
abstract = {We present a method of recovering high dynamic range radiance maps from photographs taken with conventional imaging equipment. In our method, multiple photographs of the scene are taken with different amounts of exposure. Our algorithm uses these differently exposed photographs to recover the response function of the imaging process, up to factor of scale, using the assumption of reciprocity. With the known response function, the algorithm can fuse the multiple photographs into a single, high dynamic range radiance map whose pixel values are proportional to the true radiance values in the scene. We demonstrate our method on images acquired with both photochemical and digital imaging processes. We discuss how this work is applicable in many areas of computer graphics involving digitized photographs, including image-based modeling, image compositing, and image processing. Lastly, we demonstrate a few applications of having high dynamic range radiance maps, such as synthesizing realistic motion blur and simulating the response of the human visual system.},
address = {New York, New York, USA},
author = {Debevec, Paul E. and Malik, Jitendra},
booktitle = {ACM SIGGRAPH 2008 classes on - SIGGRAPH '08},
doi = {10.1145/1401132.1401174},
pages = {1},
publisher = {ACM Press},
title = {{Recovering high dynamic range radiance maps from photographs}},
url = {http://portal.acm.org/citation.cfm?doid=1401132.1401174},
year = {2008}
}
@article{Meyer2006,
abstract = {Many eye-tracking systems either require the user to keep their head still or involve cameras or other equipment mounted on the user’s head. While acceptable for research applications, these limitations make the systems unsatisfactory for prolonged use in interactive applications. Since the goal of our work is to use eye trackers for improved visual communication through gaze guidance [1,2] and for Augmentative and Alternative Communication (AAC) [3], we are interested in less invasive eye tracking techniques.},
author = {Meyer, A and B\"{o}hme, M and Martinetz, T and Barth, E},
doi = {10.1007/11768029\_25},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Perception and Interactive Technologies/2006/Meyer et al/Meyer et al. - 2006 - A single-camera remote eye tracker.pdf:pdf},
journal = {Perception and Interactive Technologies},
keywords = {eye tracker},
mendeley-tags = {eye tracker},
pages = {208--211},
title = {{A single-camera remote eye tracker}},
url = {http://link.springer.com/chapter/10.1007/11768029\_25},
volume = {4021},
year = {2006}
}
@article{Barakonyi2006,
author = {Barakonyi, I and Schmalstieg, D},
file = {::},
journal = {Mixed and Augmented Reality,  \ldots},
keywords = {augmented reality,simulation},
mendeley-tags = {augmented reality,simulation},
title = {{Ubiquitous animated agents for augmented reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4079268},
year = {2006}
}
@article{Bowman2012,
abstract = {Electronic games are starting to incorporate in-game telemetry that collects data about player, team, and community performance on a massive scale, and as data begins to accumulate, so does the demand for effectively analyzing this data. In this paper, we use examples from both old and new games of different genres to explore the theory and design space of visualization for games. Drawing on these examples, we define a design space for this novel research topic and use it to formulate design patterns for how to best apply visualization technology to games. We then discuss the implications that this new framework will potentially have on the design and development of game and visualization technology in the future.},
author = {Bowman, B. and Elmqvist, N. and Jankun-Kelly, T. J.},
doi = {10.1109/TVCG.2012.77},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Communities,Computer games,Data visualization,Games,Real time systems,SBGames,Telemetry,Three dimensional displays,community performance,data analysis,data visualisation,design patterns,design space,electronic games,entertainment,game analytics,in-game telemetry,interactive entertainment,video games,visualization,visualization technology},
language = {English},
mendeley-tags = {SBGames},
month = nov,
number = {11},
pages = {1956--1968},
publisher = {IEEE},
title = {{Toward Visualization for Games: Theory, Design Space, and Patterns}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6165280},
volume = {18},
year = {2012}
}
@book{brannan2011geometry,
author = {Brannan, D A and Esplen, M F and Gray, J J},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2011/Brannan, Esplen, Gray/Brannan, Esplen, Gray - 2011 - Geometry.pdf:pdf},
isbn = {9781107647831},
publisher = {Cambridge University Press},
title = {{Geometry}},
url = {http://books.google.com.br/books?id=UlrmKjIjrzQC},
year = {2011}
}
@inproceedings{Kirner2006,
address = {Bel\'{e}m},
author = {Kirner, Claudio and Tori, Romero},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {22--38},
publisher = {SBC},
title = {{Fundamentos de Realidade Aumentada}},
year = {2006}
}
@article{Erkelens1995,
abstract = {We studied the trajectories of self-paced saccades in two experimental conditions. Saccades were made between two visual targets in one condition and between the same two, not visible, positions in the other condition. Target pairs were presented which required oblique saccades of 20 or 40 deg. At least 200 saccades were made between each pair of targets. Horizontal and vertical eye movements were measured of the right eye with a scleral coil technique. We computed the angle between starting and end point of each primary saccade (effective direction). We also computed the angle between starting point and eye position when the saccade had covered a distance of 2.5 deg (initial direction). We found that variability in initial directions was two to seven times larger than variability in the effective directions. This effect was found in both experimental directions for saccades made in all tested directions. We conclude that curvedness of saccades is the result of a purposeful control strategy. The saccadic trajectories show that, initially, the eye is accelerated roughly in the direction of the target and subsequently is guided to the target. This behavior cannot be described by present models of saccade generation. We suggest that the coupling between saccadic pulse and step signals is not as tight as generally is accepted in the literature.},
author = {Erkelens, CJ and Vogels, IMLC},
doi = {10.1016/S0926-907X(05)80012-1},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Studies in Visual Information Processing/1995/Erkelens, Vogels/Erkelens, Vogels - 1995 - The initial direction and landing position of saccades.pdf:pdf},
journal = {Studies in Visual Information Processing},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {133--144},
title = {{The initial direction and landing position of saccades}},
url = {http://www.sciencedirect.com/science/article/pii/S0926907X05800121},
volume = {6},
year = {1995}
}
@phdthesis{Architecture2008,
abstract = {Simultaneous localisation and mapping (SLAM) has been the focus of intensive research in the last decade due to the potential benefits it offers to the field of autonomous mobile robotics. SLAM is concerned with the ability of an autonomous vehicle to navigate through an unex- plored environment and incrementally construct a map of the environment and localise itself within this map. This thesis describes an entirely vision-based, large-area, 6DoF SLAM sys- tem that was developed specifically for real-time deployment on an autonomous underwater vehicle (AUV) equipped with a calibrated stereo system. This SLAM system is based on the extended Kalman filter (EKF) and incorporates a novel approach to landmark description and data association in which landmarks are essentially local submaps that consist of a cloud of 3D points and their associated SIFT or SURF descriptors. Furthermore, landmarks are sparsely distributed in the constructed map which greatly simplifies and accelerates data association and map updates. In addition to performing localisation based on landmark observations the system also performs visual odometry and predicts vehicle motion using a constant-velocity model. For a simulated 87m long 3D loop trajectory the mean squared localisation error of the system was 3.16 and the maximum absolute error in roll, pitch and yaw angles was 11.6 o , 24.3 o and 24.4 o respectively when the stereo and landmark correspondences contained Gaussian noise with a standard deviation of 0.1 pixels and 10\% of correspondences were outliers. This thesis represents an important contribution to entirely vision-based 6DoF SLAM as very few implementations currently exist, and the approach utilised in this thesis achieves comparable results and has the potential to operate in real-time.},
author = {Thomas, SJ},
booktitle = {\ldots thesis, Heriot-Watt University, Universitat de \ldots},
title = {{Real-time stereo visual slam}},
type = {MSc},
url = {https://igvcbyu.googlecode.com/files/S Thomas.pdf},
year = {2008}
}
@inproceedings{Nillius2001,
abstract = {We present a fully automatic algorithm for estimating the projected light source direction from a single image. The requirement is that there exists a segment of an occluding contour of an object with locally Lambertian surface reflectance in the image. The algorithm consists of three stages. First a heuristic algorithm picks out potential occluding contours using color and edge information. Secondly, for each contour the light source direction is estimated using a shading model. In the final stage the results from the estimations are fused together in a Bayesian network to arrive at the most likely light source direction. The probabilistic model takes into account that the contours from the first stage might not be occluding contours. Using the same framework the contours are also classified as occluding or not. Experiments test the second stage, estimating the light source direction from an occluding contour, as well as the full algorithm.},
author = {Nillius, Peter and Eklundh, J.O.},
booktitle = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
doi = {10.1109/CVPR.2001.990650},
isbn = {0-7695-1272-0},
pages = {I--1076--I--1083},
publisher = {IEEE Comput. Soc},
title = {{Automatic estimation of the projected light source direction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=990650},
volume = {1},
year = {2001}
}
@misc{Komogortsev2013a,
author = {Komogortsev, Oleg V},
title = {{Eye movement Classification [Software]}},
url = {http://cs.txstate.edu/~ok11/emd\_offline.html},
urldate = {2013-12-14},
year = {2013}
}
@article{perucia2005desenvolvimento,
author = {Perucia, Alexandre Souza and de Berth\^{e}m, Ant\^{o}nio C\'{o}rdova and Bertschinger, Guilherme Lage and Menezes, Roberto Ribeiro Castro},
journal = {S\~{a}o Paulo: Novatec},
keywords = {SBGames},
mendeley-tags = {SBGames},
title = {{Desenvolvimento de jogos eletr\^{o}nicos}},
year = {2005}
}
@inproceedings{rbi,
abstract = {We are in the midst of an explosion of emerging human-computer interaction techniques that redefine our understanding of both computers and interaction. We propose the notion of Reality-Based Interaction (RBI) as a unifying concept that ties together a large subset of these emerging interaction styles. Based on this concept of RBI, we provide a framework that can be used to understand, compare, and relate current paths of recent HCI research as well as to analyze specific interaction designs. We believe that viewing interaction through the lens of RBI provides insights for design and uncovers gaps or opportunities for future research.},
address = {New York, New York, USA},
author = {Jacob, Robert J.K. and Girouard, Audrey and Hirshfield, Leanne M. and Horn, Michael S. and Shaer, Orit and Solovey, Erin Treacy and Zigelbaum, Jamie},
booktitle = {Proceeding of the twenty-sixth annual CHI conference on Human factors in computing systems - CHI '08},
doi = {10.1145/1357054.1357089},
isbn = {9781605580111},
keywords = {SBGames},
mendeley-tags = {SBGames},
organization = {ACM},
pages = {201},
publisher = {ACM Press},
title = {{Reality-based interaction: : a framework for post-WIMP interfaces}},
url = {http://dl.acm.org/citation.cfm?id=1357089 http://portal.acm.org/citation.cfm?doid=1357054.1357089},
year = {2008}
}
@inproceedings{Fosh2013,
abstract = {We apply the HCI concept of trajectories to the design of a sculpture trail. We crafted a trajectory through each sculpture, combining textual and audio instructions to drive directed viewing, movement and touching while listening to accompanying music. We designed key transitions along the way to oscillate between moments of social interaction and isolated personal engagement, and to deliver official interpretation only after visitors had been given the opportunity to make their own. We describe how visitors generally followed our trajectory, engaging with sculptures and making interpretations that sometimes challenged the received interpretation. We relate our findings to discussions of sense-making and design for multiple interpretations, concluding that curators and designers may benefit from considering, trajectories of interpretation‟.},
address = {New York, New York, USA},
author = {Fosh, Lesley and Benford, Steve and Reeves, Stuart and Koleva, Boriana and Brundell, Patrick},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470675},
isbn = {9781450318990},
pages = {149--158},
publisher = {ACM Press},
title = {{‘See Me, Feel Me, Touch Me, Hear Me’: Trajectories and Interpretation in a Sculpture Garden}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470675},
year = {2013}
}
@inproceedings{Feldman2007,
author = {Feldman, Adam and Adams, Summer and Hybinette, Maria and Balch, Tucker},
booktitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2007.363956},
isbn = {1-4244-0602-1},
issn = {1050-4729},
keywords = {4 SICK LMS291 laser scanners,Clustering algorithms,Computer vision,Humans,Infrared sensors,Laser modes,Object detection,Robotics and automation,Sensor systems,Target tracking,Trajectory,basketball,clustering-based algorithm,discrete target detections,multiple dynamic target tracker,multiple sensors,pattern clustering,sensors,target tracking,tracking},
language = {English},
mendeley-tags = {basketball,tracking},
month = apr,
pages = {3140--3141},
publisher = {IEEE},
title = {{A Tracker for Multiple Dynamic Targets Using Multiple Sensors}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4209574},
year = {2007}
}
@inproceedings{Karsch2011,
abstract = {We propose a method to realistically insert synthetic objects into existing photographs without requiring access to the scene or any additional scene measurements. With a single image and a small amount of annotation, our method creates a physical model of the scene that is suitable for realistically rendering synthetic objects with diffuse, specular, and even glowing materials while accounting for lighting interactions between the objects and the scene. We demonstrate in a user study that synthetic images produced by our method are confusable with real scenes, even for people who believe they are good at telling the difference. Further, our study shows that our method is competitive with other insertion methods while requiring less scene information. We also collected new illumination and reflectance datasets; renderings produced by our system compare well to ground truth. Our system has applications in the movie and gaming industry, as well as home decorating and user content creation, among others.},
address = {New York, New York, USA},
author = {Karsch, Kevin and Hedau, Varsha and Forsyth, David and Hoiem, Derek},
booktitle = {Proceedings of the 2011 SIGGRAPH Asia Conference on - SA '11},
doi = {10.1145/2024156.2024191},
isbn = {9781450308076},
keywords = {computational photography,image-based rendering,light estimation,photo editing},
number = {6},
pages = {1},
publisher = {ACM Press},
title = {{Rendering synthetic objects into legacy photographs}},
url = {http://dl.acm.org/citation.cfm?doid=2024156.2024191},
volume = {30},
year = {2011}
}
@article{Besl1992,
author = {Besl, PJ and McKay, ND},
file = {::},
journal = {Robotics-DL tentative},
title = {{Method for registration of 3-D shapes}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=981454},
year = {1992}
}
@article{Gibson2003,
author = {Gibson, S and Chalmers, A},
file = {:home/acmt/Dropbox/Documentos/Mendeley/\ldots Augmented Reality- \ldots/2003/Gibson, Chalmers/Gibson, Chalmers - 2003 - Photorealistic augmented reality.pdf:pdf},
journal = {\ldots Augmented Reality- \ldots},
keywords = {augmented reality,photorealism},
mendeley-tags = {augmented reality,photorealism},
title = {{Photorealistic augmented reality}},
url = {https://diglib.eg.org/EG/DL/Conf/EG2003/tut/tut4.pdf},
year = {2003}
}
@inproceedings{Hougen1993,
abstract = {The authors deal with estimation of the light source distribution and its use in surface shape estimation by the methods of stereo and shading. There are many advantages to integrating stereo and shading information. However, shape from shading algorithms are limited in their applicability by the assumption of overly simplistic models of the light source distribution. A more complete representation is described in which the lighting model makes use of multiple fixed point sources located at infinity. Methods of estimating the model parameters are developed, and a method of estimating the surface shape given the albedo and source distribution is presented. The shading algorithm is combined with a stereo algorithm in an integrated approach that is designed to handle albedo variations through the use of color images},
address = {Berlin, Heidelberg},
author = {Hougen, D.R. and Ahuja, N.},
booktitle = {1993 (4th) International Conference on Computer Vision},
doi = {10.1109/ICCV.1993.378225},
isbn = {0-8186-3870-2},
pages = {148--155},
publisher = {IEEE Computer Society Press},
title = {{Estimation of the light source distribution and its use in integrated shape recovery from stereo and shading}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=378225},
year = {1993}
}
@article{Feild2013,
address = {New York, New York, USA},
author = {Feild, Henry and White, Ryen W. and Fu, Xin},
doi = {10.1145/2470654.2481416},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2999},
publisher = {ACM Press},
title = {{Supporting orientation during search result examination}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481416},
year = {2013}
}
@article{Kanatani2000,
abstract = {We describe a theoretically optimal algorithm for computing the homography between two images. First, we derive a theoretical accuracy bound based on a mathematical model of image noise and do simulation to confirm that our renormalization technique effectively attains that bound. Then, we apply our technique to mosaicing of images with small overlaps. By using real images, we show how our algorithm reduces the instability of the image mapping.},
author = {Kanatani, K and KANAZAWA, Y},
file = {::},
journal = {IEICE TRANSACTIONS on Information and Systems},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
number = {7},
pages = {1369--1374},
title = {{Optimal homography computation with a reliability measure}},
url = {http://search.ieice.org/bin/summary.php?id=e83-d\_7\_1369},
volume = {E83-D},
year = {2000}
}
@inproceedings{Higa2007,
abstract = {There have been many implementations of virtual reality, using audio and visual senses. However, implementations of mixed reality (MR) have thus far only dealt with the visual sense. We have developed an MR system that merges real and virtual worlds in both the audio and visual senses, wherein the geometric consistency of the audio sense was fully coordinated with the visual sense. We tried two approaches for merging real and virtual worlds in the audio sense, using open-air and closed-air headphones.},
author = {Higa, Kyota and Nishiura, Takanobu and Kimura, Asako and Shibata, Fumihisa and Tamura, Hideyuki},
booktitle = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2007.4538847},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality/2007/Higa et al/Higa et al. - 2007 - A Two-by-Two Mixed Reality System That Merges Real and Virtual Worlds in Both Audio and Visual Senses.pdf:pdf},
isbn = {9781424417490},
keywords = {audio and visual senses,audio visual senses,closed air headphones,closed-air headphones,consistency,geometric,geometric consistency,mixed reality,open air headphones,open-air headphones},
month = nov,
pages = {1--4},
publisher = {Ieee},
title = {{A Two-by-Two Mixed Reality System That Merges Real and Virtual Worlds in Both Audio and Visual Senses}},
url = {http://dl.acm.org/citation.cfm?id=1514358 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538847},
year = {2007}
}
@book{Tiller2001,
abstract = {The first book on Modelica, a modeling language that can be used to simulate both continuous and discrete behavior, Introduction to Physical Modeling with Modelica provides the necessary background to develop Modelica models of almost any physical system. The author starts with basic differential equations from several engineering domains and describes how these equations can be used to create reusable component models. Next, he describes techniques for modeling complex non-linear behavior, exploiting the powerful array handling features and mixing continuous and discrete behavior. The second part of the book focuses on effective use of all the language features provided by the Modelica modeling language. This includes, among other things, discussions on maximizing the reusability of component models being developed, managing the model development process, and making models as computationally efficient as possible. Introduction to Physical Modeling with Modelica includes online access to supplementary material containing the Modelica source code for all examples as well as an evaluation copy of Dymola. Using Dymola, readers can immediately begin to explore the dynamics of the models included with the book or to develop their own models. Nearly 100 examples of mechanical, electrical, biological, chemical, thermal and hydraulic models are included. Introduction to Physical Modeling with Modelica will be of interest to all professional engineers and university researchers developing physical models. Students studying control system development or modeling of physical systems will also find it useful.},
author = {Tiller, Michael},
edition = {1},
pages = {368},
publisher = {Springer},
title = {{Introduction to Physical Modeling with Modelica}},
url = {http://www.amazon.com/Introduction-Physical-Modeling-International- Engineering/dp/0792373677},
year = {2001}
}
@phdthesis{Fell2003,
abstract = {The following paper considers the field of Crowd Dynamics, and also the modeling thereof. This discipline has been becoming increasingly important in recent years and concerns itself with the propagation of pedestrian traffic in dense quarters. By considering such traffic flow a number of unique features and patterns can be discerned. Studied will be the current ‘state of the art’, and a modeling project based on one of the cited papers will be constructed and considered in further depth},
author = {Fell, Andrew},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2003/Fell/Fell - 2003 - A Study of Modeling Crowd Dynamics.pdf:pdf},
pages = {33},
school = {Carleton University},
title = {{A Study of Modeling Crowd Dynamics}},
year = {2003}
}
@book{hacking,
author = {Kramer, Jeff and Parker, Matt and Herrera, Daniel and Burrus, Nicolas and Echtler, Florian},
publisher = {Apress},
title = {{Hacking the Kinect}},
year = {2012}
}
@article{Anabuki2000,
author = {Anabuki, M and Kakuta, H},
file = {::},
journal = {CHI'00 extended abstracts \ldots},
keywords = {augmented reality,simulation},
mendeley-tags = {augmented reality,simulation},
title = {{Welbo: an embodied conversational agent living in mixed reality space}},
url = {http://dl.acm.org/citation.cfm?id=633299},
year = {2000}
}
@inproceedings{Heuel,
abstract = {We present a geometric method for (i) matching 2D line segments from multiple oriented images, (ii) optimally reconstructing 3D line segments and (iii) grouping 3D line segments to corners. The proposed algorithm uses two developments in combining projective geometry and statistics, which are described in this article: (i) the geometric entities points, lines and planes in 2D and 3D and their uncertainty are represented in homogeneous coordinates and new entities may be constructed including their propagated uncertainty. The construction can be performed directly or as an estimation. (ii) relations such as incidence, equality, parallelism and orthogonality between points, lines and planes can be tested statistically based on a given significance level. Using these tools, the resulting algorithm is straightforward and gives reasonable results. It is only based on geometric information and does not use any image intensities, though it can be extended to use other information. The matching of 3D lines does not need any thresholds other than a significance value for the hypotheses tests.},
author = {Heuel, S. and Forstner, W.},
booktitle = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
doi = {10.1109/CVPR.2001.991006},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001/Unknown/Heuel, Forstner/Heuel, Forstner - Unknown - Matching, reconstructing and grouping 3D lines from multiple views using uncertain projective geometry.pdf:pdf},
isbn = {0-7695-1272-0},
pages = {II--517--II--524},
publisher = {IEEE Comput. Soc},
title = {{Matching, reconstructing and grouping 3D lines from multiple views using uncertain projective geometry}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=991006\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=.QT.projective+geometry.QT.},
volume = {2}
}
@article{Lin2006,
author = {Lin, CS and Ho, C and Chen, W and Chiu, C and Yeh, M},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Optica Applicata/2006/Lin et al/Lin et al. - 2006 - Powered wheelchair controlled by eye-tracking system.pdf:pdf},
journal = {Optica Applicata},
keywords = {eye tracker},
mendeley-tags = {eye tracker},
title = {{Powered wheelchair controlled by eye-tracking system}},
url = {http://www.if.pwr.wroc.pl/~optappl/pdf/2006/no23/optappl\_3623p401.pdf},
year = {2006}
}
@article{Massung2013,
address = {New York, New York, USA},
author = {Massung, Elaine and Coyle, David and Cater, Kirsten F. and Jay, Marc and Preist, Chris},
doi = {10.1145/2470654.2470708},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {371},
publisher = {ACM Press},
title = {{Using crowdsourcing to support pro-environmental community activism}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470708},
year = {2013}
}
@article{Davis2013,
address = {New York, New York, USA},
author = {Davis, Nicholas and Zook, Alexander and O'Neill, Brian and Headrick, Brandon and Riedl, Mark and Grosz, Ashton and Nitsche, Michael},
doi = {10.1145/2470654.2470747},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {651},
publisher = {ACM Press},
title = {{Creativity support for novice digital filmmaking}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470747},
year = {2013}
}
@article{Komogortsev2009a,
abstract = {Our work addresses one of the core issues related to Human Computer Interaction (HCI) systems that use eye gaze as an input. This issue is the sensor, transmission and other delays that exist in any eye tracker-based system, reducing its performance. A delay effect can be compensated by an accurate prediction of the eye movement trajectories. This paper introduces a mathematical model of the human eye that uses anatomical properties of the Human Visual System to predict eye movement trajectories. The eye mathematical model is transformed into a Kalman filter form to provide continuous eye position signal prediction during all eye movement types. The model presented in this paper uses brainstem control properties employed during transitions between fast (saccade) and slow (fixations, pursuit) eye movements. Results presented in this paper indicate that the proposed eye model in a Kalman filter form improves the accuracy of eye movement prediction and is capable of a real-time performance. In addition to the HCI systems with the direct eye gaze input, the proposed eye model can be immediately applied for a bit-rate/computational reduction in real-time gaze-contingent systems.},
author = {Komogortsev, Oleg V and Khan, Javed I.},
doi = {10.1007/s11768-009-7218-z},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of Control Theory and Applications/2009/Komogortsev, Khan/Komogortsev, Khan - 2009 - Eye movement prediction by oculomotor plant Kalman filter with brainstem control.pdf:pdf},
issn = {1672-6340},
journal = {Journal of Control Theory and Applications},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = feb,
number = {1},
pages = {14--22},
title = {{Eye movement prediction by oculomotor plant Kalman filter with brainstem control}},
url = {http://link.springer.com/article/10.1007/s11768-009-7218-z http://link.springer.com/10.1007/s11768-009-7218-z},
volume = {7},
year = {2009}
}
@article{Wagner2013,
address = {New York, New York, USA},
author = {Wagner, Julie and Nancel, Mathieu and Gustafson, Sean G. and Huot, Stephane and Mackay, Wendy E.},
doi = {10.1145/2470654.2466170},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1299},
publisher = {ACM Press},
title = {{Body-centric design space for multi-surface interaction}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466170},
year = {2013}
}
@inproceedings{Gu2011,
abstract = {This research demo describes the implementation of a mobile AR-supported educational course application, AR Circuit, which is designed to promote the effectiveness of remote collaborative learning for physics. The application employs the TCP/IP protocol enabling multiplayer functionality in a mobile AR environment. One phone acts as the server and the other acts as the client. The server phone will capture the video frames, process the video frame, and send the current frame and the markers transformation matrices to the client phone.},
address = {Singapore},
author = {Gu, Jian and Li, Nai and Duh, Henry Been-Lirn},
booktitle = {2011 IEEE Virtual Reality Conference},
doi = {10.1109/VR.2011.5759496},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 IEEE Virtual Reality Conference/2011/Gu, Li, Duh/Gu, Li, Duh - 2011 - A remote mobile collaborative AR system for learning in physics.pdf:pdf},
isbn = {978-1-4577-0039-2},
keywords = {Collaboration,Collaborative work,Games,Mobile communication,Mobile handsets,Physics,Servers},
mendeley-tags = {Collaboration,Collaborative work,Games,Mobile communication,Mobile handsets,Physics,Servers},
month = mar,
pages = {257--258},
publisher = {IEEE},
title = {{A remote mobile collaborative AR system for learning in physics}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5759496 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5759496},
year = {2011}
}
@inproceedings{Reynolds1987,
abstract = {The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle system, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the "animator." The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.},
address = {Anahein, CA},
author = {Reynolds, Craig W.},
booktitle = {ACM SIGGRAPH 1987 Conference Proceedings},
keywords = {Applications,Artificial Intelligence,Computational Geometry and Object Modeling,Computer Graphics,Simulation and Modeling,Three Dimensional Graphics and Realism-Animation,Vision and Scene Understanding,actor,aggregate motion,behavioral animation,bird,constraints,fish,flight,flock,herd,particle system,path planning.,school},
pages = {25--34},
publisher = {ACM},
title = {{Flocks, Herds, and Schools: A Distributed Behavioral Model}},
year = {1987}
}
@book{fitts1979human,
author = {Fitts, P M and Posner, M I},
publisher = {Greenwood Press},
series = {Basic Concepts in Psychology Series},
title = {{Human performance}},
url = {http://books.google.com.br/books?id=6msbAQAAMAAJ},
year = {1979}
}
@inproceedings{Fuks2003,
address = {Salvador-BA},
author = {Fuks, Hugo and Raposo, Alberto Barbosa and Gerosa, Marco Aur\'{e}lio},
booktitle = {IX Simp\'{o}sio Brasileiro de Sistemas Multim\'{\i}dia e Web - WebMidia},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IX Simp\'{o}sio Brasileiro de Sistemas Multim\'{\i}dia e Web - WebMidia/2003/Fuks, Raposo, Gerosa/Fuks, Raposo, Gerosa - 2003 - Do Modelo de Colabora\c{c}\~{a}o 3C \`{a} Engenharia de Groupware.pdf:pdf},
pages = {445--452},
title = {{Do Modelo de Colabora\c{c}\~{a}o 3C \`{a} Engenharia de Groupware}},
volume = {2003},
year = {2003}
}
@article{Matsumoto2000a,
author = {Matsumoto, Y and Zelinsky, A},
file = {::},
journal = {Automatic Face and Gesture  \ldots},
keywords = {3d gaze},
mendeley-tags = {3d gaze},
title = {{An algorithm for real-time stereo vision implementation of head pose and gaze direction measurement}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=840680},
year = {2000}
}
@inproceedings{Kruijff2010,
abstract = {This paper provides a classification of perceptual issues in augmented reality, created with a visual processing and interpretation pipeline in mind. We organize issues into ones related to the environment, capturing, augmentation, display, and individual user differences. We also illuminate issues associated with more recent platforms such as handhelds or projector-camera systems. Throughout, we describe current approaches to addressing these problems, and suggest directions for future research.},
author = {Kruijff, Ernst and Swan, J. Edward and Feiner, Steven},
booktitle = {2010 IEEE International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2010.5643530},
file = {::},
isbn = {978-1-4244-9343-2},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = oct,
pages = {3--12},
publisher = {IEEE},
title = {{Perceptual issues in augmented reality revisited}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5643530 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5643530},
year = {2010}
}
@article{Saxena2009,
abstract = {We consider the problem of estimating detailed 3D structure from a single still image of an unstructured environment. Our goal is to create 3D models that are both quantitatively accurate as well as visually pleasing. For each small homogeneous patch in the image, we use a Markov Random Field (MRF) to infer a set of "plane parameters" that capture both the 3D location and 3D orientation of the patch. The MRF, trained via supervised learning, models both image depth cues as well as the relationships between different parts of the image. Other than assuming that the environment is made up of a number of small planes, our model makes no explicit assumptions about the structure of the scene; this enables the algorithm to capture much more detailed 3D structure than does prior art and also give a much richer experience in the 3D flythroughs created using image-based rendering, even for scenes with significant nonvertical structure. Using this approach, we have created qualitatively correct 3D models for 64.9 percent of 588 images downloaded from the Internet. We have also extended our model to produce large-scale 3D models from a few images.},
author = {Saxena, Ashutosh and Sun, Min and Ng, Andrew Y},
doi = {10.1109/TPAMI.2008.132},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Pattern Analysis and Machine Intelligence/2009/Saxena, Sun, Ng/Saxena, Sun, Ng - 2009 - Make3D learning 3D scene structure from a single still image.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Computer-Assisted,Computer-Assisted: methods,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Information Storage and Retrieval,Information Storage and Retrieval: methods,Pattern Recognition,Photography,Photography: methods,Reproducibility of Results,Sensitivity and Specificity,Three-Dimensional,Three-Dimensional: methods,perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = may,
number = {5},
pages = {824--40},
pmid = {19299858},
title = {{Make3D: learning 3D scene structure from a single still image.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4531745 http://www.ncbi.nlm.nih.gov/pubmed/19299858},
volume = {31},
year = {2009}
}
@article{Galvao2004,
abstract = {A revis\~{a}o sistem\'{a}tica \'{e} um recurso importante da pr\'{a}tica baseada em evid\^{e}ncias, que consiste em uma forma de s\'{\i}ntese dos resultados de pesquisas relacionados com um problema espec\'{\i}fico. O presente artigo tem como objetivo oferecer subs\'{\i}dios que proporcionem reflex\~{o}es para a constru\c{c}\~{a}o e/ou aplica\c{c}\~{a}o de revis\~{o}es sistem\'{a}ticas no cen\'{a}rio da enfermagem. Fundamentados na literatura, apresentamos as fases que comp\~{o}em uma revis\~{a}o sistem\'{a}tica e aspectos relevantes a serem considerados para a utiliza\c{c}\~{a}o desse recurso.},
author = {Galv\~{a}o, CM and Sawada, NO and Trevizan, MA},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Revista Latino-Americana de enfermagem/2004/Galv\~{a}o, Sawada, Trevizan/Galv\~{a}o, Sawada, Trevizan - 2004 - Revis\~{a}o Sistem\'{a}tica Recurso que Proporciona a Incorpora\c{c}\~{a}o das Evid\^{e}ncias na Pr\'{a}tica da Enferma.pdf:pdf},
journal = {Revista Latino-Americana de enfermagem},
keywords = {RBS,enfermagem,metan\'{a}lise,pesquisa},
mendeley-tags = {RBS},
number = {3},
pages = {549--556},
title = {{Revis\~{a}o Sistem\'{a}tica: Recurso que Proporciona a Incorpora\c{c}\~{a}o das Evid\^{e}ncias na Pr\'{a}tica da Enfermagem}},
url = {http://www.scielo.br/pdf/rlae/v12n3/v12n3a14.pdf},
volume = {12},
year = {2004}
}
@article{Steinicke2005,
abstract = {In this paper we propose the concepts of virtual reflections, lights and shadows to enhance immersion in mixed reality (MR) environments, which focus on merging the real and the virtual world seamlessly. To improve immersion, we augment the virtual objects with real world information regarding the virtual reality (VR) system environment, e.g., CAVE, workbench etc. Real-world objects such as input devices or light sources as well as the position and posture of the user are used to simulate global illumination phenomena, e.g., users can see their own reflections and shadows on virtual objects. Besides the concepts and the implementation of this approach, we describe the system setup and an example application for this kind of advanced MR system environment.},
author = {Steinicke, Frank and Hinrichs, Klaus and Ropinski, Timo},
doi = {10.1007/11555261\_94},
journal = {INTERACT},
keywords = {Artificial Intelligence,Computers and Education,Information Systems Applications,User Interfaces and Human Computer Interaction},
pages = {1018--1021},
title = {{Virtual Reflections and Virtual Shadows in Mixed Reality Environments}},
volume = {3585},
year = {2005}
}
@article{Musse2001,
abstract = {This paper describes a model for simulating crowds of humans in real time. We deal with a hierarchy composed of virtual crowds, groups, and individuals. The groups are the most complex structure that can be controlled in different degrees of autonomy. This autonomy refers to the extent to which the virtual agents are independent of user intervention and also the amount of information needed to simulate crowds. Thus, depending on the complexity of the simulation, simple behaviors can be sufficient to simulate crowds. Otherwise, more complicated behavioral rules can be necessary and, in this case, it can be included in the simulation data in order to improve the realism of the animation. We present three different ways for controlling crowd behaviors: 1) by using innate and scripted behaviors, 2) by defining behavioral rules, using events and reactions, and 3) by providing an external control to guide crowd behaviors in real time. The two main contributions of our approach are: the possibility of increasing the complexity of group/agent behaviors according to the problem to be simulated and the hierarchical structure based on groups to compose a crowd.},
author = {Musse, Soraia Raupp and Thalmann, Daniel},
doi = {10.1109/2945.928167},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {2},
pages = {152--164},
title = {{Hierarchical model for real time simulation of virtual human crowds}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=928167},
volume = {7},
year = {2001}
}
@article{Mathot2012,
author = {Math\^{o}t, S and Cristino, F and Gilchrist, ID and Theeuwes, J},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of Eye Movement \ldots/2012/Math\^{o}t et al/Math\^{o}t et al. - 2012 - A simple way to estimate similarity between pairs of eye movement sequences.pdf:pdf},
journal = {Journal of Eye Movement  \ldots},
keywords = {eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
title = {{A simple way to estimate similarity between pairs of eye movement sequences}},
url = {http://www.cs.vu.nl/~cogsci/cogpsy/theeuwes/JoER\_1.pdf},
year = {2012}
}
@article{Anderson2013,
address = {New York, New York, USA},
author = {Anderson, Fraser and Bischof, Walter F.},
doi = {10.1145/2470654.2466143},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1109},
publisher = {ACM Press},
title = {{Learning and performance with gesture guides}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466143},
year = {2013}
}
@inproceedings{Jacobs2013,
abstract = {A study of an interactive artwork shows how artists engaged the public with scientific climate change data. The artwork visualised live environmental data collected from remote trees, alongside both historical and forecast global CO2 data. Visitors also took part in a mobile sensing experience in a nearby forest. Our study draws on the perspectives of the artists, visitors and a climate scientist to reveal how the work was designed and experienced. We show that the artists adopted a distinct approach that fostered an emotional engagement with data rather than an informative or persuasive one. We chart the performative strategies they used to achieve this including sensory engagement with data, a temporal structure that balanced liveness with slowness, and the juxtaposition of different treatments of the data to enable interpretation and dialogue.},
address = {New York, New York, USA},
author = {Jacobs, Rachel and Benford, Steve and Selby, Mark and Golembewski, Michael and Price, Dominic and Giannachi, Gabriella},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470673},
isbn = {9781450318990},
pages = {129--138},
publisher = {ACM Press},
title = {{A conversation between trees: what data feels like in the forest}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470673},
year = {2013}
}
@inproceedings{Levin2008,
abstract = {Modern geospatial data acquisition systems deliver vast amounts of multi-domain remotely sensed data such as multi/hyper spectral imagery and LIDAR point-clouds. Unfortunately geospatial products automatically derived from source geospatial data are burdened with residual errors and artifacts which should be manually inspected, cleaned and corrected. These tasks become critical in many large-scale projects that require real-time processing of immense amounts of visual information and usually require manual post-processing or visual inspection of the source and/or derived data. The process of visual inspection could be divided in two general phases: perception and reaction. Scene perception comprises several steps such as visual search, feature selection and identification. Reaction reflects a decision made by an operator and usually involves other types of modalities (e.g. physical action such as mouse movements or typing). Human analysts perceive visual data through intensive movements of eyes which subconsciously select the most distinctive features in an image in order to reduce our overall ambiguity about the observed scene. A sequence of eye movements may then be understood within a framework of sequential accumulation of information. Whether or not all informative points are detected depends on both the observer’s current knowledge of the stimulus and the particular task. Dynamics of this information accrual process can be documented and quantified using analysis of eye movements during the process of natural, active visual perception. This paper presents theoretical and practical investigations that have been made to illustrate the feasibility of 3D gaze tracking as a disambiguation tool in the process of visual search of a target in high resolution imagery.},
address = {Portland},
annote = {- cited by: 5
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Levin, E and Helton, W and Liimakka, R and Gienko, G},
booktitle = {Proceedings of ASPRS Annual Conference},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of ASPRS Annual Conference/2008/Levin et al/Levin et al. - 2008 - Eye movement analysis in visual inspection of geospatial data.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
title = {{Eye movement analysis in visual inspection of geospatial data}},
url = {http://www.asprs.org/a/publications/proceedings/portland08/0050.pdf},
year = {2008}
}
@inproceedings{Ziegler2003,
abstract = {In this paper we present a novel algorithm for reconstructing 3D scenes from a set of images. The user defines a set of polygonal regions with corresponding labels in each image using familiar 2D photo-editing tools. Our reconstruction algorithm computes the 3D model with maximum volume that is consistent with the set of regions in the input images. The algorithm is fast, uses only 2D intersection operations, and directly computes a polygonal model. We implemented a user-assisted system for 3D scene reconstruction and show results on scenes that are difficult or impossible to reconstruct with other methods.},
author = {Ziegler, R and Matusik, W and Pfister, H and McMillan, L},
booktitle = {Proceedings of the 2003 Eurographics/ACM SIGGRAPH symposium on Geometry processing},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2003 EurographicsACM SIGGRAPH symposium on Geometry processing/2003/Ziegler et al/Ziegler et al. - 2003 - 3D reconstruction using labeled image regions.pdf:pdf},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {248--259},
publisher = {ACM Press},
title = {{3D reconstruction using labeled image regions}},
url = {http://dl.acm.org/citation.cfm?id=882403},
year = {2003}
}
@inproceedings{Radinsky2004,
abstract = {An improved algorithm for classification of nystagmus was designed allowing the sorting of response segments even in severely non-linear patients and subjects with abnormally large phase shifts. The algorithm employs a model-based approach that was developed by Rey and Galiana. The improved classification algorithm consists of two essential stages. In the first stage the eye velocity response is classified to obtain initial estimates of the slow phase eye velocity intervals. In the second stage, the slow phase estimates are used to identify a response phase shift and nonlinearity, and compensate for their effects. Multiple tests on simulated data and experimental data obtained from clinical subjects are presented. The results of the tests demonstrate that the algorithm is able to analyze the patient data with a high accuracy even in the presence of noise, eye-blinks and other artifacts.},
address = {San Francisco, CA},
author = {Radinsky, Iliya and Galiana, Henrietta L},
booktitle = {Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Conference},
doi = {10.1109/IEMBS.2004.1403212},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Conference proceedings ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engine. Conference/2004/Radinsky, Galiana/Radinsky, Galiana - 2004 - Improved algorithm for classification of ocular nystagmus.pdf:pdf},
issn = {1557-170X},
keywords = {gaze analysis,nystagmus},
mendeley-tags = {gaze analysis,nystagmus},
month = jan,
pages = {534--537},
pmid = {17271731},
title = {{Improved algorithm for classification of ocular nystagmus.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1403212 http://www.ncbi.nlm.nih.gov/pubmed/17271731},
volume = {1},
year = {2004}
}
@inproceedings{Azuma1999,
abstract = {Almost all previous Augmented Reality (AR) systems work indoors. Outdoor AR systems offer the potential for new application areas. However, building an outdoor AR system is difficult due to portability constraints, the inability to modify the environment, and the greater range of operating conditions. We demonstrate a hybrid tracker that stabilizes an outdoor AR display with respect to user motion, achieving more accurate registration than previously shown in an outdoor AR system. The hybrid tracker combines rate gyros with a compass and tilt orientation sensor in a near real-time system. Sensor distortions and delays required compensation to achieve good results. The measurements from the two sensors are fused together to compensate for each other's limitations. From static locations with moderate head rotation rates, peak registration errors are \~{}2 degrees, with typical errors under 1 degree, although errors can become larger over long time periods due to compass drift. Without our stabilization, even small motions make the display nearly unreadable},
address = {Houston, TX},
author = {Azuma, R and Hoff, B and Neely, H. and Sarfaty, R.},
booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
doi = {10.1109/VR.1999.756959},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)/1999/Azuma et al/Azuma et al. - 1999 - A motion-stabilized outdoor augmented reality system.pdf:pdf},
isbn = {0-7695-0093-5},
keywords = {augmented reality,outdoors},
mendeley-tags = {augmented reality,outdoors},
pages = {252--259},
publisher = {IEEE Comput. Soc},
title = {{A motion-stabilized outdoor augmented reality system}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=756959 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=756959},
year = {1999}
}
@article{Thalmann2007,
author = {Thalmann, Daniel},
keywords = {crowd simulation},
mendeley-tags = {crowd simulation},
title = {{Crowd simulation}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/9780470050118.ecse676/full},
year = {2007}
}
@book{Ponce2011,
abstract = {Computer Vision: A Modern Approach, 2e, is appropriate for upper-division undergraduate- and graduate-level courses in computer vision found in departments of Computer Science, Computer Engineering and Electrical Engineering. This textbook provides the most complete treatment of modern computer vision methods by two of the leading authorities in the field. This accessible presentation gives both a general view of the entire computer vision enterprise and also offers sufficient detail for students to be able to build useful applications. Students will learn techniques that have proven to be useful by first-hand experience and a wide range of mathematical methods},
author = {Ponce, Jean},
edition = {2},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2011/Ponce/Ponce - 2011 - Computer Vision A Modern Approach.pdf:pdf},
isbn = {013608592X},
pages = {761},
publisher = {Pearson Education, Limited},
title = {{Computer Vision: A Modern Approach}},
url = {http://books.google.com/books?id=gM63QQAACAAJ\&pgis=1},
year = {2011}
}
@unpublished{Chesnet2011,
abstract = {The decomposition of the movement of the eye into different categories is critical to their study. Ac- cording to the algorithm used, some movements may significantly be over or under estimated. The two most important movements (saccade, swift movement and fixation, when the eye remains on the same position to capture information) are well characterized and usually properly identified by most algorithms, however in the context of writing, some other movements cannot be characterized as fixations or saccades. These movements may either be considered as microsaccades or slow movements which are actually close to saccades of fixations respectively but without completely sharing their characteristics. None of the algorithms available for eye movements classification can handle all the four movements. Some of them tend to first identify fixations by the barycenter method while others first identify saccades by the mean of speed. The approach used here is different as movements are first decomposed into elementary move- ments according to the acceleration scheme before they are merged. This new approach allows a better identification of each of the 4 kinds of movements.},
annote = {- cited by: 0
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000
        
- Dia lido: N\~{a}o lido},
author = {Alamargot, Denis and Caporossi, Gilles and Chesnet, David},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2011/Alamargot, Caporossi, Chesnet/Alamargot, Caporossi, Chesnet - 2011 - An Algorithm for Qualifying Eye Movements During Handwriting.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {12},
title = {{An Algorithm for Qualifying Eye Movements During Handwriting}},
url = {http://www.gerad.ca/fichiers/cahiers/G-2011-41.pdf},
year = {2011}
}
@inproceedings{Quintella2010,
address = {Natal},
author = {Quintella, Felipe and Vicente, R M S and Soares, Luciano and Raposo, Alberto},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2010/Quintella et al/Quintella et al. - 2010 - DWeb3D Um toolkit para facilitar a cri\c{c}\~{a}o e manipula\c{c}\~{a}o de cenas 3D em X3D.pdf:pdf},
pages = {1--4},
publisher = {SBC},
title = {{DWeb3D: Um toolkit para facilitar a cri\c{c}\~{a}o e manipula\c{c}\~{a}o de cenas 3D em X3D}},
year = {2010}
}
@article{Ruttle,
abstract = {3D reconstruction from multiple view images requires that camera parameters are very accurately known and standard camera calibration techniques [1] often fail to provide the required level of accuracy for the extrinsic camera parameters. Using the Kinect depth camera, we propose to estimate camera parameters by minimising the cross correlation between density functions modelled for each recorded depth images. We illustrate experimentally how this improves the modelling for estimating 3D shape from Depths.},
author = {Ruttle, J. and Arellano, C. and Dahyot, R.},
title = {{Extrinsic camera parameters estimation for shape-from-depths}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6334154\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=kinect+calibration}
}
@article{Khaled2013,
address = {New York, New York, USA},
author = {Khaled, Rilla and Nelson, Mark J. and Barr, Pippin},
doi = {10.1145/2470654.2466201},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {your keywords},
pages = {1509},
publisher = {ACM Press},
title = {{Design metaphors for procedural content generation in games}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466201},
year = {2013}
}
@article{Agapie2013,
address = {New York, New York, USA},
author = {Agapie, Elena and Golovchinsky, Gene and Qvarfordt, Pernilla},
doi = {10.1145/2470654.2481418},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {3,4,although users,documents in exploratory search,effective at retrieving useful,information needs,keyword queries are more,longer,queries are a familiar,research literature shows that,way of representing},
pages = {3019},
publisher = {ACM Press},
title = {{Leading people to longer queries}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481418},
year = {2013}
}
@misc{Rizzoli2009,
author = {Rizzoli, A E},
title = {{A Collection of Modelling and Simulation Resources on the Internet}},
url = {http://www.idsia.ch/~andrea/simtools.html},
year = {2009}
}
@article{Camerini1988,
author = {Camerini, PM and Galbiati, G and Maffioli, F},
journal = {Annals of Operations Research},
keywords = {MST,graph,tree},
mendeley-tags = {MST,graph,tree},
title = {{Algorithms for finding optimum trees: Description, use and evaluation}},
url = {http://link.springer.com/article/10.1007/BF02288325},
year = {1988}
}
@article{Thimbleby2013,
address = {New York, New York, USA},
author = {Thimbleby, Harold},
doi = {10.1145/2470654.2466190},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13/2013/Thimbleby/Thimbleby - 2013 - Reasons to question seven segment displays.pdf:pdf},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1431},
publisher = {ACM Press},
title = {{Reasons to question seven segment displays}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466190},
year = {2013}
}
@phdthesis{Funcke2012,
abstract = {There is a great deal of literature available on the topic of crowd simulation, but far less focusing on model comparisons. This paper takes the most prominent models, compares them to one another, and then makes generalisations based on the results. Models discussed include Cellular Automata models, the Social Forces Model, Rule-Based models and Hybrid models. The primary goal of this paper is to determine what model types are best suited to what applications. It was found that cellular automata are best suited to teaching, social forces models generally give the most accurate results, rule based models are most suited to applications with a heavy focus on graphics and hybrid models (both commercial and non-commercial) tend to generally do well for all applications. Additionally direct realationships between model cost, complexity and simulation accuracy were found.},
author = {Funcke, Matthew and Wells, George},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2012/Funcke, Wells/Funcke, Wells - 2012 - A Comparison of Crowd Simulation Techniques.pdf:pdf;:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2012/Funcke, Wells/Funcke, Wells - 2012 - A Comparison of Crowd Simulation Techniques.pptx:pptx},
pages = {61},
school = {Rhodes University},
title = {{A Comparison of Crowd Simulation Techniques}},
year = {2012}
}
@article{Schmalstieg2007e,
author = {Schmalstieg, Dieter and Wagner, Daniel},
isbn = {9781424417506},
journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
keywords = {augmented reality games,cultural heritage,mobile augmented reality,wearable computing},
month = nov,
pages = {1--13},
publisher = {IEEE Computer Society Press},
title = {{Experiences with Handheld Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538819},
year = {2007}
}
@inproceedings{worldkit,
abstract = {Instant access to computing, when and where we need it, has long been one of the aims of research areas such as ubiquitous computing. In this paper, we describe the WorldKit system, which makes use of a paired depth camera and projector to make ordinary surfaces instantly interactive. Using this system, touch-based interactivity can, without prior calibration, be placed on nearly any unmodified surface literally with a wave of the hand, as can other new forms of sensed interaction. From a user perspective, such interfaces are easy enough to instantiate that they could, if desired, be recreated or modified "each time we sat down" by "painting" them next to us. From the programmer's perspective, our system encapsulates these capabilities in a simple set of abstractions that make the creation of interfaces quick and easy. Further, it is extensible to new, custom interactors in a way that closely mimics conventional 2D graphical user interfaces, hiding much of the complexity of working in this new domain. We detail the hardware and software implementation of our system, and several example applications built using the library.},
address = {New York, New York, USA},
author = {Xiao, Robert and Harrison, Chris and Hudson, Scott E},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2466113},
isbn = {9781450318990},
organization = {ACM},
pages = {879--888},
publisher = {ACM Press},
title = {{WorldKit: rapid and easy creation of ad-hoc interactive applications on everyday surfaces}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466113},
year = {2013}
}
@misc{TileMap,
author = {TileMap},
keywords = {SBGames},
mendeley-tags = {SBGames},
title = {{Mappy}},
url = {http://www.tilemap.co.uk/mappy.php},
urldate = {2013/07/26}
}
@article{Santosa2013,
address = {New York, New York, USA},
author = {Santosa, Stephanie and Chevalier, Fanny and Balakrishnan, Ravin and Singh, Karan},
doi = {10.1145/2470654.2466148},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1149},
publisher = {ACM Press},
title = {{Direct space-time trajectory control for visual media editing}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466148},
year = {2013}
}
@article{Kumar2009,
author = {Kumar, N},
journal = {Systems, Man and  \ldots},
title = {{A novel approach to video-based pupil tracking}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5345909},
year = {2009}
}
@article{Tweed1990,
author = {Tweed, D and Vilis, T},
journal = {Vision research},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Geometric relations of eye position and velocity vectors during saccades}},
url = {http://www.sciencedirect.com/science/article/pii/0042698990901314},
year = {1990}
}
@article{Kasprowski2004,
author = {Kasprowski, P and Ober, J},
journal = {Biometric Authentication},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Eye movements in biometrics}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-25976-3\_23},
year = {2004}
}
@inproceedings{Xing2011,
abstract = {In augmented reality, consistent illumination plays an important role when integrating a virtual object into a video of real scene. In this paper, we propose a novel image based framework to estimate-on-line the dynamic ally changing illumination parameters of outdoor video sequences captured by a fixed camera. Unlike previous approaches which either request to know the scene geometry or involve huge storage to preserve time dependent basis images or statistic parameters, our approach requires very simple interaction at the initialization stage by a few brushes to select areas with specified surface normal, which are used to calculate the sunlight parameters. An optimization procedure is also applied, ensuring the robustness and precision of our estimation. Experimental results demonstrate the effectiveness and flexibility of the proposed approach.},
address = {Jinan},
author = {Xing, Guanyu and Liu, Yanli and Qin, Xueying and Peng, Qunsheng},
booktitle = {2011 12th International Conference on Computer-Aided Design and Computer Graphics},
doi = {10.1109/CAD/Graphics.2011.51},
isbn = {978-1-4577-1079-7},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = sep,
pages = {439--442},
publisher = {IEEE},
title = {{On-line Illumination Estimation of Outdoor Scenes Based on Area Selection for Augmented Reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6062825 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6062825},
year = {2011}
}
@inproceedings{Henrysson2005,
author = {Henrysson, Anders and Billinghurst, Mark and Ollila, Mark},
booktitle = {Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)},
doi = {10.1109/ISMAR.2005.32},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)/2005/Henrysson, Billinghurst, Ollila/Henrysson, Billinghurst, Ollila - 2005 - Face to face collaborative AR on mobile phones.pdf:pdf},
isbn = {0-7695-2459-1},
pages = {80--89},
publisher = {IEEE},
title = {{Face to face collaborative AR on mobile phones}},
url = {http://portal.acm.org/citation.cfm?id=1105185 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544667},
year = {2005}
}
@article{Duchowski2007,
author = {Duchowski, AT},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Eye tracking methodology: Theory and practice}},
url = {http://books.google.com/books?hl=pt-BR\&lr=\&id=WtvVdNESRyIC\&oi=fnd\&pg=PR15\&dq=Eye+Tracking:+A+comprehensive+guide+to+methods+and+measures\&ots=8kze4tzVbx\&sig=SxR4NlyF0UdIqTZ7F5-KeWVqV0M},
year = {2007}
}
@article{Zhang2013,
abstract = {In augmented reality, it is essential that the rendered virtual objects are embedded harmonically into the view of the background scenes and their appearance should reflect the changing lighting condition of the real scene to ensure illumination consistency. In this paper, we propose a novel method to solve for the sunlight and skylight basis images of static outdoor scenes from a time-lapse image sequence. It is proved that the resulted basis images encapsulate the geometry and material reflectivity of the scene, correspond to the global illumination effects of the outdoor scene under a unit intensity of the sunlight and skylight. Our method is fully automatic. Unlike previous methods, it gets rid of the constraints that the reflectance of all objects in scenes should be ideal diffuse, or the weather condition should be overcast or sunshine. During decomposition, we first detect shadowed pixels by analyzing the time-lapse curve of each pixel through k-means clustering, the basis images of sunlight and skylight are then solved by an iterative procedure with the decomposition equation. The basis images are further optimized by exploiting their constraints and priors. Experimental results demonstrate the effectiveness and flexibility of the proposed method. Our method can also be applied in image understanding and compressing.},
author = {Zhang, Rui and Zhong, Fan and Lin, Lili and Xing, Guanyu and Peng, Qunsheng and Qin, Xueying},
doi = {10.1007/s00371-013-0776-6},
issn = {0178-2789},
journal = {The Visual Computer},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = jan,
number = {11},
pages = {1197--1210},
title = {{Basis image decomposition of outdoor time-lapse videos}},
url = {http://link.springer.com/article/10.1007/s00371-013-0776-6 http://link.springer.com/10.1007/s00371-013-0776-6},
volume = {29},
year = {2013}
}
@article{Malacria2013,
address = {New York, New York, USA},
author = {Malacria, Sylvain and Bailly, Gilles and Harrison, Joel and Cockburn, Andy and Gutwin, Carl},
doi = {10.1145/2470654.2470735},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Hotkeys, keyboard shortcuts, rehearsal, menus, com},
pages = {573},
publisher = {ACM Press},
title = {{Promoting Hotkey use through rehearsal with ExposeHK}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470735},
year = {2013}
}
@article{Liu2009,
abstract = {Illumination consistency is important for photorealistic rendering of mixed reality. However, it is usually difficult to acquire illumination conditions of natural environments. In this paper, we propose a novel method for evaluating the light conditions of a static outdoor scene without knowing its geometry, material, or texture. In our method, we separate respectively the shading effects of the scene due to sunlight and skylight through learning a set of sample images which are captured with the same sun position. A fixed illumination map of the scene under sunlight or skylight is then derived reflecting the scene geometry, surface material properties and shadowing effects. These maps, one for sunlight and the other for skylight, are therefore referred to as basis images of the scene related to the specified sun position. We show that the illumination of the same scene under different weather conditions can be approximated as a linear combination of the two basis images. We further extend this model to estimate the lighting condition of scene images under deviated sun positions, enabling virtual objects to be seamlessly integrated into images of the scene at any time. Our approach can be applied for online video process and deal with both cloudy and sun shine situations. Experiment results successfully verify the effectiveness of our approach.},
author = {Liu, Yanli and Qin, Xueying and Xu, Songhua and Nakamae, Eihachiro and Peng, Qunsheng},
doi = {10.1007/s00371-009-0342-4},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Visual Computer/2009/Liu et al/Liu et al. - 2009 - Light source estimation of outdoor scenes for mixed reality.pdf:pdf},
issn = {0178-2789},
journal = {The Visual Computer},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = mar,
number = {5-7},
pages = {637--646},
title = {{Light source estimation of outdoor scenes for mixed reality}},
url = {http://link.springer.com/article/10.1007/s00371-009-0342-4 http://link.springer.com/10.1007/s00371-009-0342-4},
volume = {25},
year = {2009}
}
@article{Peffers2007,
author = {Peffers, K and Tuunanen, T},
file = {::},
journal = {\ldots  information systems},
title = {{A design science research methodology for information systems research}},
url = {http://mesharpe.metapress.com/index/276818W6PN4T5483.pdf},
year = {2007}
}
@article{Lee2000,
author = {Lee, Edward A and Neuendorffer, Steve},
doi = {10.1.1.3.2696},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Language/2000/Lee, Neuendorffer/Lee, Neuendorffer - 2000 - MoML - A Modeling Markup Language in XML - Version 0 . 4.pdf:pdf},
journal = {Language},
pages = {1--14},
title = {{MoML - A Modeling Markup Language in XML - Version 0 . 4}},
year = {2000}
}
@article{Rother2002b,
abstract = {A man-made environment is characterized by many parallel lines and orthogonal edges. In this article, a new method for detecting the three mutually orthogonal directions of such an environment is presented. Since real-time performance is not necessary for architectural applications, such as building reconstruction, a computationally intensive approach was chosen. However, this enables us to avoid one fundamental error of most other existing techniques. Compared to theirs, our approach is furthermore more rigorous, since all conditions given by three mutually orthogonal directions are identified and utilized. We assume a partly calibrated camera with unknown focal length and unknown principal point. By examining these camera parameters, which can be determined from orthogonal directions, falsely detected vanishing points may be rejected.},
author = {Rother, Carsten},
doi = {10.1016/S0262-8856(02)00054-9},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Image and Vision Computing/2002/Rother/Rother - 2002 - A new approach to vanishing point detection in architectural environments.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = aug,
number = {9-10},
pages = {647--655},
title = {{A new approach to vanishing point detection in architectural environments}},
url = {http://www.sciencedirect.com/science/article/pii/S0262885602000549 http://linkinghub.elsevier.com/retrieve/pii/S0262885602000549},
volume = {20},
year = {2002}
}
@article{Jabri2009,
abstract = {In this paper a new strategy for observing and analyzing a basketball match using video processing techniques to identify the game systems of a team is described. The system tracks players' positions during the match. At the outset, three video streams from three fixed cameras are available, each processed separately to deliver measures of the players' positions from different available views. Each treated view includes foreground detection and a bounding-box tracker designed to contain the pixels representing each player. During the multi-view process, measurements from different views are synchronized to enable identification of the same player when the player is visible simultaneously on several cameras. These measurements are combined in order to obtain the players' positions as well as their updated positions through the images. The position thus obtained is exploited in a database containing the representative points (coordinates) of all the players, who form a polygon. The analysis of a game system is thus simply the analysis of the deformation and movement of this polygon during the match. Comparative indicators of the two teams are defined, along with an indicator which represents the opinion of an expert in the sport (action code). Statistical tools are exploited with the objective on the one hand of identifying correlations and relationships between different indicators, and on the other hand of identifying the game system adopted by comparing the expert opinion and the results of the established heuristic models.},
author = {Jabri, Imed and Battikh, Tahar and Hammouda, Nizar},
doi = {10.1109/CJECE.2009.5443855},
issn = {0840-8688},
journal = {Canadian Journal of Electrical and Computer Engineering},
keywords = {Cameras,Games,Image databases,Position measurement,Streaming media,basket-ball,basketball,basketball match,bounding box tracker,camera,cameras,computer vision,foreground detection,game systems,game-system identification,identification de systeme de jeu,image motion analysis,image recognition,player identification,player tracking,polygon,poursuite de joueurs,sport,statistical analysis,tracking,tracking players,traitement video / basketball,video processing,video processing techniques,video signal processing},
mendeley-tags = {basketball,tracking},
number = {3},
pages = {87--93},
shorttitle = {Electrical and Computer Engineering, Canadian Jour},
title = {{Player tracking and identification of game systems in basketball using three cameras}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5443855},
volume = {34},
year = {2009}
}
@inproceedings{Kumar2007a,
abstract = {The GUIDe (Gaze-enhanced User Interface Design) project in the HCI Group at Stanford University explores how gaze information can be effectively used as an augmented input in addition to keyboard and mouse. We present three practical applications of gaze as an augmented input for pointing and selection, application switching, and scrolling. Our gaze-based interaction techniques do not overload the visual channel and present a natural, universally-accessible and general purpose use of gaze information to facilitate interaction with everyday computing devices.},
address = {New York, New York, USA},
author = {Kumar, Manu and Winograd, Terry},
booktitle = {CHI '07 extended abstracts on Human factors in computing systems - CHI '07},
doi = {10.1145/1240866.1240935},
file = {:home/acmt/Dropbox/Documentos/Mendeley/CHI '07 extended abstracts on Human factors in computing systems - CHI '07/2007/Kumar, Winograd/Kumar, Winograd - 2007 - GUIDe gaze-enhanced UI design.pdf:pdf},
isbn = {9781595936424},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {1977},
publisher = {ACM Press},
title = {{GUIDe: gaze-enhanced UI design}},
url = {http://dl.acm.org/citation.cfm?id=1240935 http://portal.acm.org/citation.cfm?doid=1240866.1240935},
year = {2007}
}
@inproceedings{Lv2002,
abstract = {Analysis of human activity from a video camera is simplified by the knowledge of the camera's intrinsic and extrinsic parameters. We describe a technique to estimate such parameters from image observations without requiring measurements of scene objects. We first develop a general technique for calibration using vanishing points and vanishing line. We then describe a method for estimating the needed points and line by observing the motion of a human in the scene. Experimental results, including error estimates, are presented.},
author = {Nevatia, R},
booktitle = {Object recognition supported by user interaction for service robots},
doi = {10.1109/ICPR.2002.1044793},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Object recognition supported by user interaction for service robots/2002/Nevatia/Nevatia - 2002 - Self-calibration of a camera from video of a walking human.pdf:pdf},
isbn = {0-7695-1695-X},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {562--567},
publisher = {IEEE Comput. Soc},
title = {{Self-calibration of a camera from video of a walking human}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1044793 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1044793},
volume = {1},
year = {2002}
}
@article{Cossairt2008,
abstract = {We present a novel image-based method for compositing real and synthetic objects in the same scene with a high degree of visual realism. Ours is the first technique to allow global illumination and near-field lighting effects between both real and synthetic objects at interactive rates, without needing a geometric and material model of the real scene. We achieve this by using a light field interface between real and synthetic components---thus, indirect illumination can be simulated using only two 4D light fields, one captured from and one projected onto the real scene. Multiple bounces of interreflections are obtained simply by iterating this approach. The interactivity of our technique enables its use with time-varying scenes, including dynamic objects. This is in sharp contrast to the alternative approach of using 6D or 8D light transport functions of real objects, which are very expensive in terms of acquisition and storage and hence not suitable for real-time applications. In our method, 4D radiance fields are simultaneously captured and projected by using a lens array, video camera, and digital projector. The method supports full global illumination with restricted object placement, and accommodates moderately specular materials. We implement a complete system and show several example scene compositions that demonstrate global illumination effects between dynamic real and synthetic objects. Our implementation requires a single point light source and dark background.},
author = {Cossairt, Oliver and Nayar, Shree and Ramamoorthi, Ravi},
doi = {10.1145/1360612.1360656},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM Transactions on Graphics/2008/Cossairt, Nayar, Ramamoorthi/Cossairt, Nayar, Ramamoorthi - 2008 - Light field transfer Global Illumination Between Real and Synthetic Objects.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = aug,
number = {3},
pages = {1},
title = {{Light field transfer: Global Illumination Between Real and Synthetic Objects}},
url = {http://dl.acm.org/citation.cfm?id=1360656 http://portal.acm.org/citation.cfm?doid=1360612.1360656},
volume = {27},
year = {2008}
}
@inproceedings{Noonan2011,
abstract = {Robust motion correction in medical imaging requires accurate and reliable motion tracking. Current systems use devices such as the Polaris Vicra position sensor to monitor the position and orientation of a tracking tool which is fixed to the subject. Although in principle these methods offer high positional accuracy this is lost if the tool slips. Markerless motion tracking aims to track the object directly without the use of markers or a tracking tool. To date these methods have either been unsuccessful or too expensive to have been widely implemented. The Microsoft Kinect is a low cost RGB+D (colour plus depth) video camera. We have used the Kinect to perform motion tracking of a head phantom using a CT of the head as a high resolution template. We present initial results that show the Kinect can track rigid body motion to within <;2 mm of that measured by a Polaris system. We use the PointCloudLibrary open project [1] algorithm implementations to register the CT template to the Kinect frames.},
author = {Noonan, Philip J. and Cootes, Tim F. and Hallett, William A. and Hinz, Rainer},
booktitle = {2011 IEEE Nuclear Science Symposium Conference Record},
doi = {10.1109/NSSMIC.2011.6153680},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 IEEE Nuclear Science Symposium Conference Record/2011/Noonan et al/Noonan et al. - 2011 - The design and initial calibration of an optical tracking system using the Microsoft Kinect.pdf:pdf},
isbn = {978-1-4673-0120-6},
month = oct,
pages = {3614--3617},
publisher = {IEEE},
title = {{The design and initial calibration of an optical tracking system using the Microsoft Kinect}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6153680\&contentType=Conference+Publications\&searchField\%3DSearch\_All\%26queryText\%3Dkinect+calibration http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6153680},
year = {2011}
}
@article{Wu2011,
author = {Wu, Qiong and Boulanger, Pierre},
doi = {10.1109/SVR.2011.35},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 XIII Symposium on Virtual Reality/2011/Wu, Boulanger/Wu, Boulanger - 2011 - Real-Time Estimation of Missing Markers for Reconstruction of Human Motion.pdf:pdf},
isbn = {978-1-4577-0661-5},
journal = {2011 XIII Symposium on Virtual Reality},
month = may,
pages = {161--168},
publisher = {Ieee},
title = {{Real-Time Estimation of Missing Markers for Reconstruction of Human Motion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951848},
year = {2011}
}
@article{Pessoa2012,
abstract = {This paper presents a review of the Photorealistic Augmented Reality field and proposes a solution for interactively rendering virtual objects into dynamic real scenes in a photorealistic way. This solution features a rendering pipeline that comprises techniques regarding illumination, reflectance model, shadowing, composition, and camera effects. The techniques are chained in a flexible way, allowing the user to choose which techniques are to be enabled. An environment map generation procedure was developed and allows virtual objects to exhibit coherent effects such as color bleeding and specular reflection, even when the real objects are moved. The range of materials that can be rendered was widened by extending Lafortune's Spatial BRDF. The implemented infrastructure is offered as an authoring toolkit that consists of an API and a material editor tool. The aim of this authoring toolkit is to increase development productivity of Photorealistic Augmented Reality applications. The proposed solution was evaluated by taking into account visual and performance metrics. It allowed consistent rendering of dynamic scenes and photorealistic materials. The frame rate obtained was suitable to Augmented Reality applications when there were few virtual objects in the scene.},
author = {Pessoa, Saulo A. and Moura, Guilherme de S. and Lima, Joao Paulo S. do M. and Teichrieb, Veronica and Kelner, Judith},
doi = {10.1016/j.cag.2011.12.003},
file = {::},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = apr,
number = {2},
pages = {50--69},
title = {{RPR-SORS: Real-time photorealistic rendering of synthetic objects into real scenes}},
url = {http://www.sciencedirect.com/science/article/pii/S0097849311001701 http://linkinghub.elsevier.com/retrieve/pii/S0097849311001701},
volume = {36},
year = {2012}
}
@article{Barwolff2013,
author = {B\"{a}rwolff, G and Ahnert, T and Chen, M and Huth, F},
journal = {\ldots  Science and Its  \ldots},
title = {{Modeling and Numerical Simulation of Multi-destination Pedestrian Crowds}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-39640-3\_7},
year = {2013}
}
@inproceedings{Steimle2013,
abstract = {Flexpad is an interactive system that combines a depth camera and a projector to transform sheets of plain paper or foam into flexible, highly deformable, and spatially aware handheld displays. We present a novel approach for tracking deformed surfaces from depth images in real time. It captures deformations in high detail, is very robust to occlusions created by the user's hands and fingers, and does not require any kind of markers or visible texture. As a result, the display is considerably more deformable than in previous work on flexible handheld displays, enabling novel applications that leverage the high expressiveness of detailed deformation. We illustrate these unique capabilities through three application examples: curved cross-cuts in volumetric images, deforming virtual paper characters, and slicing through time in videos. Results from two user studies show that our system is capable of detecting complex deformations and that users are able to perform them quickly and precisely.},
address = {New York, New York, USA},
author = {Steimle, J\"{u}rgen and Jordt, Andreas and Maes, Pattie},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470688},
isbn = {9781450318990},
keywords = {Flexible display,bending,deformation,depth camera,handheld display,projection,tracking,volumetric data},
pages = {237--246},
publisher = {ACM Press},
title = {{Flexpad: highly flexible bending interactions for projected handheld displays}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470688},
year = {2013}
}
@inproceedings{Neto2006,
abstract = {There are several approaches for the treatment of phobias. The most effective, known as cognitive-behavior therapy, it bases on the gradual disensibility of the phobical individual through the exhibition to the stimulus cause of the fear. The idea of applying Interactive Systems in the phobias treatment it is promising for allowing the patient to access environments, objects or situations phobia causes with larger easiness, safety and low cost in relation to the traditional treatments. Taking as example phobias as airplane, height or driving, where the access and control of the environment are difficult and expensive, it can be elucidated the advantages of this tool. It is known that don't seek help about 60\% to 85\% of the people that suffer of these upset. The main reason is the enormous fear that they have to confront the object or the phobical situation. In spite of the traditional therapy to get good results. it is shown necessary the development of new tools that it encourage these people to seek the treatment, as, for example, interactive systems with virtual environment, that possess features that can exceed the results obtained by the conventional treatments.},
address = {Bel\'{e}m-PA},
author = {Neto, Ant\^{o}nio Val\'{e}rio},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {343--352},
publisher = {SBC},
title = {{Realidade Aumentada Aplicada ao Tratamento de Fobias}},
year = {2006}
}
@article{hcibook,
author = {Dix, Alan J and Finlay, Janet E and Abowd, Gregory D and Beale, Russell},
isbn = {9780130461094},
keywords = {SBGames},
mendeley-tags = {SBGames},
pages = {834},
publisher = {Pearson Education Limited},
title = {{Human-Computer Interaction}},
year = {2003}
}
@article{Oikawa2012,
author = {Oikawa, Marina Atsumi and Taketomi, Takafumi and Yamamoto, Goshiro and Fujisawa, Makoto and Amano, Toshiyuki and Miyazaki, Jun and Kato, Hirokazu},
doi = {10.1109/SVR.2012.3},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Oikawa et al/Oikawa et al. - 2012 - Local Quadrics Surface Approximation for Real-Time Tracking of Textureless 3D Rigid Curved Objects.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {-model-based tracking,a,a coarse representation of,a sparse mesh is,b,camera pose estimation,figure 1,in,object resulting,quadrics,red lines,rendered on the target,rigid curved objects,the object contour},
month = may,
pages = {246--253},
publisher = {Ieee},
title = {{Local Quadrics Surface Approximation for Real-Time Tracking of Textureless 3D Rigid Curved Objects}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297536},
year = {2012}
}
@article{Noh2010a,
abstract = {The most challenging task in developing Augmented Reality (AR) applications is to make virtual objects mixed harmoniously with the real scene. To achieve photorealistic AR environment, three key issues must be emphasized namely consistency of geometry, illumination and speed. Shadow is an essential element to improve visual perception and realism. Without shadow, virtual objects will appear like it is floating and thus will make the environment look unrealistic. However, many shadow algorithms still have drawbacks such as producing sharp and hard-edged outlines, which make the shadow’s appearance unrealistic. Thus, this paper will focus on generating soft shadow in AR scene render based on real light sources position, where reflective sphere is used to create environment map image to estimate the light source from the real scene and render the soft shadows.},
author = {Noh, Zakiah and Sunar, Mohd Shahrizal},
journal = {Advances in Multimedia - An International Journal (AMIJ)},
keywords = {Augmented Reality,augmented reality,illumination,reflective sphere,shadow,soft shadow},
mendeley-tags = {augmented reality,illumination},
number = {2},
pages = {26--48},
title = {{Soft Shadow Rendering based on Real Light Source Estimation in Augmented Reality Full text}},
url = {http://www.cscjournals.org/csc/manuscriptinfo.php?ManuscriptCode=64.76.72.73.44.48.47.99},
volume = {1},
year = {2010}
}
@article{Birnbaum2013,
address = {New York, New York, USA},
author = {Birnbaum, Benjamin and Borriello, Gaetano and Flaxman, Abraham D. and DeRenzi, Brian and Karlin, Anna R.},
doi = {10.1145/2470654.2481404},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2911},
publisher = {ACM Press},
title = {{Using behavioral data to identify interviewer fabrication in surveys}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481404},
year = {2013}
}
@article{Piekarski2007,
author = {Piekarski, W and Thomas, BH},
journal = {Space Time Play},
title = {{Outdoor Augmented Reality}},
url = {http://www.springerlink.com/index/g68028464u25131j.pdf},
year = {2007}
}
@inproceedings{Draelos2012,
abstract = {With proper calibration of its color and depth cameras, the Kinect can capture detailed color point clouds at up to 30 frames per second. This capability positions the Kinect for use in robotics as a low-cost navigation sensor. Thus, techniques for efficiently calibrating the Kinect depth camera and altering its optical system to improve suitability for imaging short-range obstacles are presented. To perform depth calibration, a calibration rig and software were developed to automatically map raw depth values to object depths. The calibration rig consisted of a traditional chessboard calibration target with easily locatable features in depth at its exterior corners that facilitated software extraction of corresponding object depths and raw depth values. To modify the Kinect's optics for improved short-range imaging, Nyko's Zoom adapter was used due to its simplicity and low cost. Although effective at reducing the Kinect's minimum range, these optics introduced pronounced distortion in depth. A method based on capturing depth images of planar objects at various depths produced an empirical depth distortion model for correcting such distortion in software. Together, the modified optics and the empirical depth undistortion procedure demonstrated the ability to improve the Kinect's resolution and decrease its minimum range by approximately 30\%.},
author = {Draelos, Mark and Deshpande, Nikhil and Grant, Edward},
booktitle = {2012 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)},
doi = {10.1109/MFI.2012.6343067},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)/2012/Draelos, Deshpande, Grant/Draelos, Deshpande, Grant - 2012 - The Kinect up close Adaptations for short-range imaging.pdf:pdf},
isbn = {978-1-4673-2512-7},
month = sep,
pages = {251--256},
publisher = {IEEE},
title = {{The Kinect up close: Adaptations for short-range imaging}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6343067\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=kinect+calibration},
year = {2012}
}
@inproceedings{Eichner,
abstract = {Salvucci \& Goldberg (2000) described dispersion algorithms (I-DT) as a robust way for fixation detection in low-speed-eytrackers. Never the less Shic, Chawarska and Scassellati (2008) found that changing settings for thresholds in the used I-DT altered the relation between independent and dependent variables significantly. Furthermore they concluded that it could be more important to find the relationships between measured variables and threshold settings than finding the “right” settings for thresholds. This work used eye- tracking data from reading and motivational research (N=280; 50 Hz) to find answers to this question. The findings show that dispersion thresholds are not crucial for relationship changes in variables, but temporal thresholds are. The theoretical approach separates physiological from psychological temporal thresholds. Findings indicate that psychological thresholds can be applied to fixation data outside the I-DT parameters to find meaningful relationships.},
annote = {- cited by: 0
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000
        
Eichner diz que n\~{a}o existe o melhor threshold para os algoritmos de dispers\~{a}o, e que \'{e} mais importante descobrir as rela\c{c}\~{o}es entre os as medi\c{c}\~{o}es e os par\^{a}metros do threshold. Ele descobre que thresholds espaciais n\~{a}o contribuem muito com a varia\c{c}\~{a}o dessa rela\c{c}\~{a}o, todavia os par\^{a}metros temporais contribuem.
        
Lido - 2013-12-10 12:47:00 },
author = {Eichner, M},
booktitle = {EyeTrackBehavior Conference},
file = {:home/acmt/Dropbox/Documentos/Mendeley/EyeTrackBehavior Conference/2011/Eichner/Eichner - 2011 - Do thresholds in dispersion-algorithms matter in applied eye movement studies.pdf:pdf},
keywords = {Eye movements,dispersion algorithm,fixation detection,gaze analysis,reading,threshold},
mendeley-tags = {gaze analysis},
pages = {1--2},
title = {{Do thresholds in dispersion-algorithms matter in applied eye movement studies?}},
url = {http://www.uni-giessen.de/~g61314/eyetracking/EichnerM\_Dispersion\_Algs\_Tobii\_EyeTrackingBehavior2011\_rev1.pdf},
year = {2011}
}
@article{Ichino2013,
address = {New York, New York, USA},
author = {Ichino, Junko and Isoda, Kazuo and Hanai, Ayako and Ueda, Tetsuya},
doi = {10.1145/2470654.2481413},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2979},
publisher = {ACM Press},
title = {{Effects of the display angle in museums on user's cognition, behavior, and subjective responses}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481413},
year = {2013}
}
@article{Liu2010a,
abstract = {Illumination consistency plays an important role in realistic rendering of virtual characters which are integrated into a live video of real scene. This paper proposes a novel method for estimating the illumination conditions of outdoor videos captured by a fixed viewpoint. We first derive an analytical model which relates the statistics of an image to the lighting parameters of the scene adhering to the basic illumination model. Exploiting this model, we then develop a framework to estimate the lighting conditions of live videos. In order to apply the above approach to scenes containing dynamic objects such as intrusive pedestrians and swaying trees, we enforce two constraints, namely spatial and temporal illumination coherence, to refine the solution. Our approach requires no geometric information of the scenes and is sufficient for real-time performance. Experiments show that with the lighting parameters recovered by our method, virtual characters can be seamlessly integrated into the live video.},
author = {Liu, Yanli and Qin, Xueying and Xing, Guanyu and Peng, Qunsheng},
doi = {10.1002/cav.357},
issn = {15464261},
journal = {Computer Animation and Virtual Worlds},
keywords = {augmented reality,illumination estimation,image statistics,outdoor scenes},
number = {3-4},
pages = {321--330},
title = {{A new approach to outdoor illumination estimation based on statistical analysis for augmented reality}},
url = {http://doi.wiley.com/10.1002/cav.357},
volume = {21},
year = {2010}
}
@book{duda2012pattern,
author = {Duda, Richard O and Hart, Peter E and Stork, David G},
publisher = {Wiley-interscience},
title = {{Pattern classification}},
year = {2012}
}
@article{Knecht2012,
abstract = {In this paper we present a novel plausible rendering method for mixed reality systems, which is useful for many real-life application scenarios, like architecture, product visualization or edutainment. To allow virtual objects to seamlessly blend into the real environment, the real lighting conditions and the mutual illumination effects between real and virtual objects must be considered, while maintaining interactive frame rates. The most important such effects are indirect illumination and shadows cast between real and virtual objects.},
author = {Knecht, Martin and Traxler, Christoph and Mattausch, Oliver and Wimmer, Michael},
doi = {10.1016/j.cag.2012.04.013},
file = {::},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = nov,
number = {7},
pages = {846--856},
title = {{Reciprocal shading for mixed reality}},
url = {http://www.sciencedirect.com/science/article/pii/S0097849312001148 http://linkinghub.elsevier.com/retrieve/pii/S0097849312001148},
volume = {36},
year = {2012}
}
@article{Ruiz2010,
author = {Ruiz, Marc and Szirmay-Kalos, L\'{a}zl\'{o} and Umenhoffer, Tam\'{a}s and Boada, Imma and Feixas, Miquel and Sbert, Mateu},
doi = {10.1007/s00371-010-0497-z},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Visual Computer/2010/Ruiz et al/Ruiz et al. - 2010 - Volumetric ambient occlusion for volumetric models.pdf:pdf},
issn = {0178-2789},
journal = {The Visual Computer},
keywords = {gpu,importance sampling,obscurances,real-time ambient occlusion},
month = apr,
number = {6-8},
pages = {687--695},
title = {{Volumetric ambient occlusion for volumetric models}},
url = {http://www.springerlink.com/index/10.1007/s00371-010-0497-z},
volume = {26},
year = {2010}
}
@inproceedings{Munn2008b,
abstract = {To study an observer's eye movements during realistic tasks, the observer should be free to move naturally throughout our three-dimensional world. Therefore, a technique to determine an observer's point-of-regard (POR) as well as his/her motion throughout a scene in three dimensions with minor user input is proposed. This requires robust feature tracking and calibration of the scene camera in order to determine the 3D location and orientation of the scene camera in the world. With this information, calibrated 2D PORs can be triangulated to 3D positions in the world; the scale of the world coordinate system can be obtained via input of the distance between two known points in the scene. Information about scene camera movement and tracked features can also be used to obtain observer position and head orientation for all video frames. The final observer motion -- including the observer's positions and head orientations -- and PORs are expressed in 3D world coordinates. The result is knowledge of not only eye movements but head movements as well allowing for the evaluation of how an observer combines head and eye movements to perform a visual task. Additionally, knowledge of 3D information opens the door for many more options for visualization of eye-tracking results.},
address = {New York, New York, USA},
author = {Munn, Susan M. and Pelz, Jeff B.},
booktitle = {Proceedings of the 2008 symposium on Eye tracking research \& applications - ETRA '08},
doi = {10.1145/1344471.1344517},
file = {::},
isbn = {9781595939821},
keywords = {Gaze 3D},
mendeley-tags = {Gaze 3D},
pages = {181},
publisher = {ACM Press},
title = {{3D point-of-regard, position and head orientation from a portable monocular video-based eye tracker}},
url = {http://dl.acm.org/citation.cfm?id=1344517 http://portal.acm.org/citation.cfm?doid=1344471.1344517},
year = {2008}
}
@article{Hashiba1995,
abstract = {Abnormalities of smooth pursuit eye movement (SPEM) have been estimated, mainly using the wave form on an electro-oculogram, in a qualitative way. Many methods for quantitative analysis of SPEM have been designed, though most are still uncommon in present clinical use. Using a personal computer, we developed a method of automatic quantitative analysis of ocular tracking eye movement recorded by electro-oculography (EOG). The design concept of this method is based on the observation that eye movement during ocular tracking consists of two different kinds of eye movements, one is SPEM and the other is saccade. The combination of SPEM and saccade (composite eye movement: CEM) commonly appears during ocular tracking. These two kinds of eye movement are essentially different not only in behavior but also about involved neural pathway in the central nervous system. From this point of view, we believe that the two kinds of eye movements involved in ocular tracking should be evaluated separately. The analysis method is outlined as follows. A horizontal sinusoidally moving visual target was employed to elicit ocular tracking eye movements. The test frequencies were set at 0.1, 0.2, 0.4 and 0.8Hz, and the amplitude of target motion was 15 deg at each frequency. The 20 seconds of eye movement data measured by EOG were fed into the computer through a digital-analog converter for further analysis. Using our original saccade detection algorithm, based on the physiological behavior of saccades, the saccadic components were detected and removed from the eye movement wave. The remaining parts, fragments of SPEM, were connected by means of interpolating defective parts. The reconstructed wave was a slow cumulative eye position curve (SCEP). Sinusoidal target motion, CEM and SCEP were processed by the FFT (Fast Fourier Transformation) method. Bode plots were applied to summarize the gain and phase of responses to SCEP and the target motion wave. These processes enable us to estimate abnormalities of SPEM such as low gain, abnormal phase shift and large trends in tested duration. We conclude that the method described here is useful for quantitative estimation of SPEM in clinical neuro-otological examinations.},
author = {Hashiba, M and Yasui, K and Watabe, H and Matsuoka, T and Baba, S},
issn = {0030-6622},
journal = {Nihon Jibiinkoka Gakkai kaiho},
keywords = {Adult,Algorithms,Computers,Electrooculography,Humans,Pursuit,Smooth,Smooth: physiology,gaze analysis,smooth pursuit},
mendeley-tags = {gaze analysis,smooth pursuit},
month = apr,
number = {4},
pages = {681--96},
pmid = {7782976},
title = {{[Quantitative analysis of smooth pursuit eye movement].}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7782976},
volume = {98},
year = {1995}
}
@article{Hara2013,
address = {New York, New York, USA},
author = {Hara, Kotaro and Le, Vicki and Froehlich, Jon},
doi = {10.1145/2470654.2470744},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {631},
publisher = {ACM Press},
title = {{Combining crowdsourcing and google street view to identify street-level accessibility problems}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470744},
year = {2013}
}
@inproceedings{Hen2009,
abstract = {Vision-based techniques to estimate 3D human pose from single camera are important in many applications like entertainment and games industries, sports science, gait analysis and video surveillance. While de-facto optical-based motion capture system is highly accurate in estimating 3D human pose, vision-based techniques offer better alternative because the latter are far cheaper and non-intrusive. In this paper, we review recent significant advances made in single-camera 3D human pose estimation techniques. We discuss about challenges faced, typical image observations used, ways to address the challenges and highlighting limitations in state-of-the-art. We conclude this paper with speculation of future research directions as well as open research problems.},
author = {Hen, Yap Wooi and Paramesran, Raveendran},
booktitle = {2009 International Conference for Technical Postgraduates (TECHPOS)},
doi = {10.1109/TECHPOS.2009.5412094},
isbn = {978-1-4244-5223-1},
keywords = {3D human pose estimation,Application software,Biological system modeling,Cameras,Humans,Image reconstruction,Joints,Motion estimation,Skeleton,Toy industry,Video surveillance,pose estimation,reconstruction,single camera,vision-based techniques},
mendeley-tags = {reconstruction},
month = dec,
pages = {1--8},
publisher = {IEEE},
shorttitle = {Technical Postgraduates (TECHPOS), 2009 Internatio},
title = {{Single camera 3D human pose estimation: A Review of current techniques}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5412094},
year = {2009}
}
@incollection{Boltes2010,
abstract = {To understand and model pedestrian dynamics, reliable empirical data of pedestrian movement are necessary for analysis and verification, but the existing database is small, inaccurate and highly contradictory. For collecting trajectories from extensive experimental series with a large number of persons we are developing a software named PeTrack which automatically extracts these trajectories from normal video recordings with high accuracy in space and time.},
address = {Berlin, Heidelberg},
author = {Boltes, M and Seyfried, A},
booktitle = {Pedestrian and Evacuation Dynamics},
doi = {10.1007/978-3-642-04504-2\_3},
isbn = {9783642045042},
keywords = {trajectory extraction},
mendeley-tags = {trajectory extraction},
pages = {43--54},
publisher = {Springer Berlin Heidelberg},
title = {{Automatic extraction of pedestrian trajectories from video recordings}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-04504-2\_3},
year = {2008}
}
@article{Urruty2007,
abstract = {Eye movements are certainly the most natural and repetitive movement of a human being. The most mundane activity, such as watching television or reading a newspaper, involves this automatic activity which consists of shifting our gaze from one point to another. Identification of the components of eye movements (fixations and saccades) is an essential part in the analysis of visual behavior because these types of movements provide the basic elements used by further investigations of human vision. However, many of the algorithms that detect fixations present a number of problems. In this article, we present a new fixation identification technique that is based on clustering of eye positions, using projections and projection aggregation applied to static pictures. We also present a new method that computes dispersion of eye fixations in videos considering a multiuser environment. To demonstrate the performance and usefulness of our approach we discuss our experimental work with two different applications: on fixed image and video.},
annote = {- cited by: 13
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000                
- keyword coletado
        
      },
author = {Urruty, Thierry and Lew, Stanislas and Ihadaddene, Nacim and Simovici, Dan A.},
doi = {10.1145/1314303.1314308},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM Transactions on Multimedia Computing, Communications, and Applications/2007/Urruty et al/Urruty et al. - 2007 - Detecting eye fixations by projection clustering.pdf:pdf},
issn = {15516857},
journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
keywords = {Eye Tracking,Segmentation,gaze analysis},
mendeley-tags = {Eye Tracking,Segmentation,gaze analysis},
month = dec,
number = {4},
pages = {1--20},
title = {{Detecting eye fixations by projection clustering}},
url = {http://dl.acm.org/citation.cfm?id=1314308 http://portal.acm.org/citation.cfm?doid=1314303.1314308},
volume = {3},
year = {2007}
}
@article{Yetim2013,
address = {New York, New York, USA},
author = {Yetim, Fahri},
doi = {10.1145/2470654.2466454},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3327},
publisher = {ACM Press},
title = {{Critical perspective on persuasive technology reconsidered}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466454},
year = {2013}
}
@incollection{Dehghan2014,
abstract = {Manual analysis of pedestrians and crowds is often impractical for massive datasets of surveillance videos. Automatic tracking of humans is one of the essential abilities for computerized analysis of such videos. In this keynote paper, we present two state of the art methods for automatic pedestrian tracking in videos with low and high crowd density. For videos with low density, first we detect each person using a part-based human detector. Then, we employ a global data association method based on Generalized Graphs for tracking each individual in the whole video. In videos with high crowd-density, we track individuals using a scene structured force model and crowd flow modeling. Additionally, we present an alternative approach which utilizes contextual information without the need to learn the structure of the scene. Performed evaluations show the presented methods outperform the currently available algorithms on several benchmarks.},
address = {Cham},
author = {Dehghan, Afshin and Idrees, Haroon and Zamir, Amir Roshan and Shah, Mubarak},
booktitle = {Pedestrian and Evacuation Dynamics},
doi = {10.1007/978-3-319-02447-9},
editor = {Weidmann, Ulrich and Kirsch, Uwe and Schreckenberg, Michael},
isbn = {978-3-319-02446-2},
keywords = {automatic surveillance,crowd analysis,crowd density,data association,human detection,tracking,trajectory extraction},
mendeley-tags = {trajectory extraction},
pages = {3--19},
publisher = {Springer International Publishing},
title = {{Automatic Detection and Tracking of Pedestrians in Videos with Various Crowd Densities}},
url = {http://link.springer.com/10.1007/978-3-319-02447-9},
year = {2014}
}
@inproceedings{Chakraborty2012,
abstract = {In this paper, an algorithm to detect the position of a basketball in a real time outdoor video is proposed. The problem of ball detection in sports video arises due to the occlusion of the ball with the players, the presence of many moving objects like spectators, flags, twigs and branches of the tree etc. The ball image is also getting distorted due to the high speed of the ball. Thus the conventional direct detection and tracking algorithm fails in various occasions. It is much advantageous to study the trajectory information of a ball and estimate the ball location using the trajectory information. In this trajectory based algorithm, firstly the ball like objects are detected and a set of ball candidates are generated for each frame. Then the set of candidate trajectories are plotted. From these potential trajectories, the actual ball trajectory is estimated by studying the trajectory information. The missing ball location in any frame can also be predicted by trajectory interpolation technique. In this paper, we also propose an efficient background detection method using the approximate median value of the background pixels, thus achieving a high degree of segmentations of the moving objects in the foreground. This algorithm provides excellent result under a high degree of occlusion and works satisfactorily in the presence of many moving objects along with the ball in the scene.},
author = {Chakraborty, Bodhisattwa and Meher, Sukadev},
booktitle = {2012 IEEE International Conference on Signal Processing, Computing and Control},
doi = {10.1109/ISPCC.2012.6224370},
isbn = {978-1-4673-1318-6},
keywords = {2D trajectory plotting,Ball detection and tracking,Cameras,Image edge detection,Interpolation,Shape,Streaming media,Trajectory,Video sequences,approximation theory,ball detection,ball image,basketball,basketball position,direct detection,estimation theory,interpolation,median value approximation,object tracking,occlusion,outdoor video,real-time position estimation,real-time position tracking,real-time systems,sport,sports video,tracking,tracking algorithm,trajectory information,trajectory interpolation,trajectory interpolation technique,trajectory-based,video signal processing},
mendeley-tags = {basketball,tracking},
month = mar,
pages = {1--6},
publisher = {IEEE},
shorttitle = {Signal Processing, Computing and Control (ISPCC),},
title = {{Real-time position estimation and tracking of a basketball}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6224370},
year = {2012}
}
@inproceedings{Kan2009,
address = {New York, New York, USA},
author = {Kan, Tai-Wei and Teng, Chin-Hung and Chou, Wen-Shou},
booktitle = {Proceedings of the 8th International Conference on Virtual Reality Continuum and its Applications in Industry - VRCAI '09},
doi = {10.1145/1670252.1670305},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 8th International Conference on Virtual Reality Continuum and its Applications in Industry - VRCAI '09/2009/Kan, Teng, Chou/Kan, Teng, Chou - 2009 - Applying QR code in augmented reality applications.pdf:pdf},
isbn = {9781605589121},
keywords = {2d barcode,artoolkit,augmented reality,be decoded by a,cell phone or a,codes that need to,mainly used in,personal com-,puter with built-in camera,qr code,qr code can,small program in a,so far,specific scanner,the qr codes are},
number = {212},
pages = {253},
publisher = {ACM Press},
title = {{Applying QR code in augmented reality applications}},
url = {http://portal.acm.org/citation.cfm?id=1670305 http://portal.acm.org/citation.cfm?doid=1670252.1670305},
volume = {1},
year = {2009}
}
@article{Morris2002,
abstract = {This work is motivated by our goal of providing non-contact head and eye based control of computer systems for people with motor difficulties. The system described here uses spatio-temporal filtering and variance maps to locate the head and find the eye-feature points, respectively. These feature points are accurately tracked in the succeeding frames by using a modified version of the Lucas–Kanade tracking algorithm with pyramidal implementation. Accurate head and eye tracking results are obtained at a processing rate of more than 30 frames per second (fps) in more than 90\% cases with a low false positive blink detection rate of 0.01\%. This is achieved under varying lighting conditions for people of different ethnicity, with and without wearing glasses.},
author = {Morris, T and Blenkhorn, P and Zaidi, Farhan},
doi = {10.1006/jnca.2002.0130},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of Network and Computer Applications/2002/Morris, Blenkhorn, Zaidi/Morris, Blenkhorn, Zaidi - 2002 - Blink detection for real-time eye tracking.pdf:pdf},
issn = {10848045},
journal = {Journal of Network and Computer Applications},
keywords = {blink,gaze analysis},
mendeley-tags = {blink,gaze analysis},
month = apr,
number = {2},
pages = {129--143},
title = {{Blink detection for real-time eye tracking}},
url = {http://www.sciencedirect.com/science/article/pii/S108480450290130X http://linkinghub.elsevier.com/retrieve/pii/S108480450290130X},
volume = {25},
year = {2002}
}
@article{Diaz2013,
author = {Diaz, G and Cooper, J and Rothkopf, C and Hayhoe, M},
journal = {Journal of vision},
title = {{Saccades to future ball location reveal memory-based prediction in a virtual-reality interception task}},
url = {http://171.67.113.220/content/13/1/20.short},
year = {2013}
}
@inproceedings{Dow2013,
abstract = {Industry relies on higher education to prepare students for careers in innovation. Fulfilling this obligation is especially difficult in classroom settings, which often lack authentic interaction with the outside world. Online crowdsourcing has the potential to change this. Our research explores if and how online crowds can support student learning in the classroom. We explore how scalable, diverse, immediate (and often ambiguous and conflicting) input from online crowds affects student learning and motivation for project-based innovation work. In a pilot study with three classrooms, we explore interactions with the crowd at four key stages of the innovation process: needfinding, ideating, testing, and pitching. Students reported that online crowds helped them quickly and inexpensively identify needs and uncover issues with early-stage prototypes, although they favored face-to-face interactions for more contextual feed-back. We share early evidence and discuss implications for creating a socio-technical infrastructure to more effectively use crowdsourcing in education.},
address = {New York, New York, USA},
author = {Dow, Steven and Gerber, Elizabeth and Wong, Audris},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470686},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13/2013/Dow, Gerber, Wong/Dow, Gerber, Wong - 2013 - A pilot study of using crowds in the classroom.pdf:pdf},
isbn = {9781450318990},
pages = {227--236},
publisher = {ACM Press},
title = {{A pilot study of using crowds in the classroom}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470686},
year = {2013}
}
@inproceedings{Hayashi2007,
author = {Hayashi, Kenichi and Kato, Hirokazu and Nishida, Shogo},
booktitle = {17th International Conference on Artificial Reality and Telexistence (ICAT 2007)},
doi = {10.1109/ICAT.2007.45},
file = {:home/acmt/Dropbox/Documentos/Mendeley/17th International Conference on Artificial Reality and Telexistence (ICAT 2007)/2007/Hayashi, Kato, Nishida/Hayashi, Kato, Nishida - 2007 - A New Framework for Tracking by Maintaining Multiple Global Hypotheses for Augmented Reality.pdf:pdf},
isbn = {0-7695-3056-7},
month = nov,
pages = {15--22},
publisher = {IEEE},
title = {{A New Framework for Tracking by Maintaining Multiple Global Hypotheses for Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4414611},
year = {2007}
}
@article{Rougier2013Text,
author = {Rougier, Nicolas P},
journal = {Journal of Computer Graphics Techniques (JCGT)},
number = {1},
pages = {50--64},
title = {{Higher Quality 2D Text Rendering}},
url = {http://jcgt.org/published/0002/01/04/},
volume = {2},
year = {2013}
}
@article{Sauter1991,
abstract = {A simple but efficient algorithm has been developed for computer analysis of eye tracking movements. The program separates smooth pursuit and saccadic eye movements. Separation of the two components is achieved using a twostep process of saccade detection. First, an AR model of the velocity of the smooth component is identified and used to determine a Kalman filter. Secondly the innovation sequence generated by this filter allows saccade detection. The precise beginning and end of each saccade are found using a Hinkley algorithm. Examples are given of analysis procedure for eye tracking of a random moving target. The method proved to be highly reliable and could be easily extended to other eye movements such as nystagmus.},
author = {Sauter, D and Martin, B. J. and Renzo, N. and Vomscheid, C.},
doi = {10.1007/BF02446297},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Medical \& Biological Engineering \& Computing/1991/Sauter et al/Sauter et al. - 1991 - Analysis of eye tracking movements using innovations generated by a Kalman filter.pdf:pdf},
issn = {0140-0118},
journal = {Medical \& Biological Engineering \& Computing},
keywords = {gaze analysis,saccade,smooth pursuit},
mendeley-tags = {gaze analysis,saccade,smooth pursuit},
month = jan,
number = {1},
pages = {63--69},
title = {{Analysis of eye tracking movements using innovations generated by a Kalman filter}},
url = {http://link.springer.com/article/10.1007/BF02446297 http://link.springer.com/10.1007/BF02446297},
volume = {29},
year = {1991}
}
@inproceedings{Schnyder2011,
abstract = {Given video from a single camera, conversion to two-view stereoscopic 3D is a challenging problem. We present a system to automatically create high quality stereoscopic video from monoscopic footage of field-based sports by exploiting context-specific priors, such as the ground plane, player size and known background. Our main contribution is a novel technique that constructs per-shot panoramas to ensure temporally consistent stereoscopic depth in video reconstructions. Players are rendered as billboards at correct depths on the ground plane. Our method uses additional sports priors to disambiguate segmentation artifacts and produce synthesized 3D shots that are in most cases, indistinguishable from stereoscopic ground truth footage.},
author = {Schnyder, Lars and Wang, Oliver and Smolic, Aljoscha},
booktitle = {2011 18th IEEE International Conference on Image Processing},
doi = {10.1109/ICIP.2011.6115857},
isbn = {978-1-4577-1303-3},
issn = {1522-4880},
keywords = {2D to 3D conversion,2D-3D conversion,3D Reconstruction,Cameras,Conferences,Image segmentation,Mosaicing,Segmentation,Sports Visualization,Stability analysis,Stereo image processing,Stereo vision,Streaming media,Three dimensional displays,high quality stereoscopic video,monoscopic footage,panoramas,per shot panoramas,reconstruction,segmentation artifacts,sport,sports content,stereoscopic ground truth footage,two view stereoscopic 3D,video reconstructions,video signal processing},
mendeley-tags = {reconstruction},
month = sep,
pages = {1961--1964},
publisher = {IEEE},
shorttitle = {Image Processing (ICIP), 2011 18th IEEE Internatio},
title = {{2D to 3D conversion of sports content using panoramas}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6115857},
year = {2011}
}
@article{Winfield122005,
author = {Winfield12, D and Li, D and Babcock, J and Parkhurst12, DJ},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2005/Winfield12 et al/Winfield12 et al. - 2005 - Towards an open-hardware open-software toolkit for robust low-cost eye tracking in HCI applications.pdf:pdf},
keywords = {eye tracker},
mendeley-tags = {eye tracker},
title = {{Towards an open-hardware open-software toolkit for robust low-cost eye tracking in HCI applications}},
url = {http://thirtysixthspan.com/openEyes/ISU-HCI-2005-04.pdf},
year = {2005}
}
@article{Oliveira2008,
author = {de Oliveira, Rita F. and Oudejans, Ra\^{o}ul R. D. and Beek, Peter J.},
doi = {10.1080/02701367.2008.10599504},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Research Quarterly for Exercise and Sport/2008/de Oliveira, Oudejans, Beek/de Oliveira, Oudejans, Beek - 2008 - Gaze Behavior in Basketball Shooting.pdf:pdf},
issn = {0270-1367},
journal = {Research Quarterly for Exercise and Sport},
month = sep,
number = {3},
pages = {399--404},
title = {{Gaze Behavior in Basketball Shooting}},
url = {http://www.tandfonline.com/doi/pdf/10.1080/02701367.2008.10599504},
volume = {79},
year = {2008}
}
@article{Lerner2009,
abstract = {There are various techniques for simulating crowds, however, in most cases the quality of the simulation is measured by examining its “look-and-feel”. Even if the aggregate movement of the crowd looks natural from afar, the behaviors of individuals might look odd when examined more closely. In this paper we present a data-driven approach for evaluating the behaviors of individuals within a simulated crowd. Each decision of an individual agent is expressed as a state-action pair, which stores a representation of the characteristics being evaluated and the factors that influence it. Based on video footage of a real crowd, a database of state-action examples is generated. Using a similarity measure, the queries are matched with the database of examples. The degree of similarity can be interpreted as the level of “naturalness” of the behavior. Essentially, this sort of evaluation offers an objective answer to the question of how similar are the simulated behaviors compared to real ones. By changing the input video we can change the context of evaluation.},
author = {Lerner, Alon and Chrysanthou, Yiorgos and Shamir, Ariel and Cohen-or, Daniel},
doi = {10.1007/978-3-642-10347-6\_7},
journal = {Motion in Games},
keywords = {Pattern Recognition,User Interfaces and Human Computer Interaction,computer graphics,data-driven,modeling and simulation},
mendeley-tags = {data-driven},
pages = {75--83},
title = {{Data Driven Evaluation of Crowds}},
volume = {5889},
year = {2009}
}
@article{Szeliski2004,
abstract = {A new approach to computing a panoramic (360 degrees) depth map is presented in this paper. Our approach uses a large collection of images taken by a camera whose motion has been constrained to planar concentric circles. We resample regular perspective images to produce a set of multiperspective panoramas and then compute depth maps directly from these resampled panoramas. Our panoramas sample uniformly in three dimensions: rotation angle, inverse radial distance, and vertical elevation. The use of multiperspective panoramas eliminates the limited overlap present in the original input images and, thus, problems as in conventional multibaseline stereo can be avoided. Our approach differs from stereo matching of single-perspective panoramic images taken from different locations, where the epipolar constraints are sine curves. For our multiperspective panoramas, the epipolar geometry, to the first order approximation, consists of horizontal lines. Therefore, any traditional stereo algorithm can be applied to multiperspective panoramas with little modification. In this paper, we describe two reconstruction algorithms. The first is a cylinder sweep algorithm that uses a small number of resampled multiperspective panoramas to obtain dense 3D reconstruction. The second algorithm, in contrast, uses a large number of multiperspective panoramas and takes advantage of the approximate horizontal epipolar geometry inherent in multiperspective panoramas. It comprises a novel and efficient 1D multibaseline matching technique, followed by tensor voting to extract the depth surface. Experiments show that our algorithms are capable of producing comparable high quality depth maps which can be used for applications such as view interpolation.},
author = {Shum, Heung-Yeung and Tang, Chi-Keung and Szeliski, R.},
doi = {10.1109/TPAMI.2004.1261078},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Pattern Analysis and Machine Intelligence/2004/Shum, Tang, Szeliski/Shum, Tang, Szeliski - 2004 - Stereo reconstruction from multiperspective panoramas.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = jan,
number = {1},
pages = {45--62},
title = {{Stereo reconstruction from multiperspective panoramas}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=1261078\&contentType=Journals+\&+Magazines\&searchField=Search\_All\&queryText=Stereo+reconstruction+from+multiperspective+panoramas},
volume = {26},
year = {2004}
}
@inproceedings{Gibson2000,
abstract = {We propose solutions for seamlessly integrating synthetic objects into background photographs at interactive rates. Recently developed image-based methods are used to capture real-world illumination, and sphere-mapping is used illuminate and render the synthetic objects. We present a new procedure for approximating shadows cast by the real-world illumination using standard hardware-based shadow mapping, and a novel image composition algorithm that uses frame-buffer hardware to correctly overlay the synthetic objects and their shadows onto the background image. We show results of an OpenGL implementation of the algorithm that is capable of rendering complex synthetic objects and their shadows at rates of up to 10 frames per second on an SGI Onyx2.},
address = {Brno, Czech Republic},
author = {Gibson, Simon and Murta, Alan},
booktitle = {Proceedings of the Eurographics Workshop},
doi = {10.1007/978-3-7091-6303-0\_33},
pages = {365--376},
publisher = {Springer Vienna},
title = {{Interactive Rendering with Real-World Illumination}},
url = {http://link.springer.com/chapter/10.1007/978-3-7091-6303-0\_33},
year = {2000}
}
@article{Huggins1962,
author = {Huggins, William H.},
doi = {10.1109/TE.1962.4322249},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IRE Transactions on Education/1962/Huggins/Huggins - 1962 - System Theory.pdf:pdf},
issn = {0893-7141},
journal = {IRE Transactions on Education},
number = {2},
pages = {61--63},
title = {{System Theory}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4322249},
volume = {5},
year = {1962}
}
@inproceedings{wilson,
abstract = {We explore the application of depth-sensing cameras to detect touch on a tabletop. Limits of depth estimate resolution and line of sight requirements dictate that the determination of the moment of touch will not be as precise as that of more direct sensing techniques such as capacitive touch screens. However, using a depth-sensing camera to detect touch has significant advantages: first, the interactive surface need not be instrumented. Secondly, this approach allows touch sensing on non-flat surfaces. Finally, information about the shape of the users and their arms and hands above the surface may be exploited in useful ways, such as determining hover state, or that multiple touches are from same hand or from the same user. We present techniques and findings using Microsoft Kinect.},
address = {New York, New York, USA},
author = {Wilson, Andrew D},
booktitle = {ACM International Conference on Interactive Tabletops and Surfaces - ITS '10},
doi = {10.1145/1936652.1936665},
isbn = {9781450303996},
keywords = {SBGames},
mendeley-tags = {SBGames},
organization = {ACM},
pages = {69},
publisher = {ACM Press},
title = {{Using a depth camera as a touch sensor}},
url = {http://portal.acm.org/citation.cfm?doid=1936652.1936665},
year = {2010}
}
@article{Beardsley2005,
abstract = {Handheld projection lets users create opportunistic displays on any suitable nearby surface. This projection method is a practical and useful addition to mobile computing. Incorporating a projector into a handheld device raises the issue of how to interact with a projection. The focus of the article is interactive projection: a technique for moving a cursor across a projection, allowing all the familiar mouse interactions from the desktop to be transposed to a handheld projector, and requiring only natural, one-handed pointing motion by the user. This article is available with a short video documentary on CD-ROM.},
author = {Beardsley, P and van Baar, J. and Raskar, R. and Forlines, C.},
doi = {10.1109/MCG.2005.12},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Computer Graphics And Applications/2005/Beardsley et al/Beardsley et al. - 2005 - Interaction using a handheld projector.pdf:pdf},
issn = {0272-1716},
journal = {IEEE Computer Graphics And Applications},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
month = jan,
number = {1},
pages = {39--43},
title = {{Interaction using a handheld projector}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1381223 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1381223},
volume = {25},
year = {2005}
}
@article{Lans2011,
abstract = {We propose a new fully automated velocity-based algorithm to identify fixations from eye-movement records of both eyes, with individual-specific thresholds. The algorithm is based on robust minimum determinant covariance estimators (MDC) and control chart procedures, and is conceptually simple and computationally attractive. To determine fixations, it uses velocity thresholds based on the natural within-fixation variability of both eyes. It improves over existing approaches by automatically identifying fixation thresholds that are specific to (a) both eyes, (b) x- and y- directions, (c) tasks, and (d) individuals. We applied the proposed Binocular-Individual Threshold (BIT) algorithm to two large datasets collected on eye-trackers with different sampling frequencies, and compute descriptive statistics of fixations for larger samples of individuals across a variety of tasks, including reading, scene viewing, and search on supermarket shelves. Our analysis shows that there are considerable differences in the characteristics of fixations not only between these tasks, but also between individuals.},
annote = {- cited by: 9
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {van der Lans, Ralf and Wedel, Michel and Pieters, Rik},
doi = {10.3758/s13428-010-0031-2},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Behavior research methods/2011/van der Lans, Wedel, Pieters/van der Lans, Wedel, Pieters - 2011 - Defining eye-fixation sequences across individuals and tasks the Binocular-Individual Threshold (B.pdf:pdf;:home/acmt/Dropbox/Documentos/Mendeley/Behavior research methods/2011/van der Lans, Wedel, Pieters/van der Lans, Wedel, Pieters - 2011 - Defining eye-fixation sequences across individuals and tasks the Binocular-Individual Threshold (.epub:epub},
issn = {1554-3528},
journal = {Behavior research methods},
keywords = {Adolescent,Algorithms,Binocular,Binocular: physiology,Blinking,Data Interpretation,Female,Fixation,Functional Laterality,Functional Laterality: physiology,Humans,Male,Ocular,Ocular: physiology,Psychomotor Performance,Psychomotor Performance: physiology,Reading,Saccades,Sensory Thresholds,Sensory Thresholds: physiology,Statistical,Vision,Young Adult,gaze analysis},
mendeley-tags = {gaze analysis},
month = mar,
number = {1},
pages = {239--57},
pmid = {21287116},
title = {{Defining eye-fixation sequences across individuals and tasks: the Binocular-Individual Threshold (BIT) algorithm.}},
url = {http://link.springer.com/article/10.3758/s13428-010-0031-2},
volume = {43},
year = {2011}
}
@article{Wu2008,
author = {Wu, TP Tai-Pang and Sun, Jian and Tang, CK Chi-Keung and Shum, Heung-Yeung HY},
doi = {10.1145/1409060.1409072},
isbn = {978-1-4503-1831-0},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = dec,
number = {5},
pages = {1},
publisher = {ACM},
title = {{Interactive normal reconstruction from a single image}},
url = {http://dl.acm.org/citation.cfm?id=1409060.1409072 http://dl.acm.org/citation.cfm?id=1409072},
volume = {27},
year = {2008}
}
@article{Gelenbe2005,
author = {Gelenbe, E and Hussain, K and Kaptan, V},
journal = {Journal of Systems and Software},
title = {{Simulating autonomous agents in augmented reality}},
url = {http://www.sciencedirect.com/science/article/pii/S0164121204000032},
year = {2005}
}
@article{Mollers2013,
address = {New York, New York, USA},
author = {M\"{o}llers, Max and Dumont, Norbert and Ladwig, Stefan and Borchers, Jan},
doi = {10.1145/2470654.2470760},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {755},
publisher = {ACM Press},
title = {{Improving touch accuracy on large tabletops using predecessor and successor}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470760},
year = {2013}
}
@inproceedings{Ortega2013,
abstract = {This paper presents IUCA (Interaction Using Camera Animations), a new interaction technique for 3D objects manipulation. IUCA allows efficient interaction in a full-resolution perspective view by integrating transients animated transitions to orthographic views into the manipulation task. This provides an interaction in context, with precise object positioning and alignment. An evaluation of the technique shows that, compared to the classical configurations, IUCA allows to reduce pointing time by 14\% on average. Testing with professional 3D designers and novice users indicate that IUCA is easy to use and to learn; and that users feel comfortable with it.},
address = {New York, New York, USA},
author = {Ortega, Michael},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470681},
isbn = {9781450318990},
pages = {193--196},
publisher = {ACM Press},
title = {{3D object position using automatic viewpoint transitions}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470681},
year = {2013}
}
@book{bennett2011affine,
author = {Bennett, K},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2011/Bennett/Bennett - 2011 - Affine and Projective Geometry.pdf:pdf},
isbn = {9781118030820},
publisher = {Wiley},
title = {{Affine and Projective Geometry}},
url = {http://books.google.com.br/books?id=DF9Jb83ashUC},
year = {2011}
}
@article{Buchau2009,
author = {Buchau, Andr\'{e} and Rucker, Wolfgang M. and W\"{o}ssner, Uwe and Becker, Martin},
doi = {10.1108/03321640910959026},
file = {:home/acmt/Dropbox/Documentos/Mendeley/COMPEL The International Journal for Computation and Mathematics in Electrical and Electronic Engineering/2009/Buchau et al/Buchau et al. - 2009 - Augmented reality in teaching of electrodynamics.pdf:pdf},
issn = {0332-1649},
journal = {COMPEL: The International Journal for Computation and Mathematics in Electrical and Electronic Engineering},
number = {4},
pages = {948--963},
title = {{Augmented reality in teaching of electrodynamics}},
url = {http://www.emeraldinsight.com/10.1108/03321640910959026},
volume = {28},
year = {2009}
}
@inproceedings{Balcisoy2002,
abstract = {Homogenous virtual environments evolved into mixed environments thanks to recent augmented reality techniques. We have done research on novel interaction techniques in virtual environments exploiting the usage of 3D virtual space. We will briefly present our augmented reality set-up for mixed reality experiments and we will analyze two distinct mixed environment case studies, and present two new interaction techniques where we use virtual humans as mediators between the real and virtual world.},
author = {Balcisoy, S and Kallmann, M. and Torre, R and Fua, P. and Thalmann, Daniel},
booktitle = {5th IEEE EMBS International Summer School on Biomedical Imaging, 2002.},
doi = {10.1109/SSBI.2002.1233993},
file = {:home/acmt/Dropbox/Documentos/Mendeley/5th IEEE EMBS International Summer School on Biomedical Imaging, 2002/2002/Balcisoy et al/Balcisoy et al. - 2002 - Interaction techniques with virtual humans in mixed environments.pdf:pdf},
isbn = {0-7803-7507-6},
keywords = {crowd simulation},
mendeley-tags = {crowd simulation},
pages = {VI\_2\_1--VI\_2\_6},
publisher = {IEEE},
title = {{Interaction techniques with virtual humans in mixed environments}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1233993 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1233993},
year = {2002}
}
@book{Baggio2012,
author = {Baggio, Daniel L\'{e}lis and Emami, Shervin and Escriv\'{a}, David Mill\'{a}n and Ievgen, Khvedchenia and Mahmood, Naureen and Saragih, Jasonl and Shilkrot, Roy},
isbn = {1849517827},
keywords = {opencv},
mendeley-tags = {opencv},
pages = {340},
publisher = {Packt Publishing},
title = {{Mastering OpenCV with Practical Computer Vision Projects}},
url = {http://www.amazon.com/Mastering-OpenCV-Practical-Computer-Projects/dp/1849517827},
year = {2012}
}
@inproceedings{Bimber2002,
abstract = {This paper presents projector-based illumination techniques for creating correct occlusion effects for optical see-through setups. We project view-dependent occlusion shadows onto the real surfaces that are located behind virtual objects. This results in a perfect occlusion of real objects by virtual ones. We have implemented and tested our approach in the context of the Virtual Showcase display. We describe a hardware extension for projecting light into the showcase and present our rendering techniques for displaying occlusion shadows for single and multi-user environments as well as for single and multi-light-projector configurations. We also report on the limitations of our system for multi-user situations and describe our experiences with a first experimental prototype.},
address = {Darmstadt},
author = {Bimber, O and Frohlich, B},
booktitle = {Proceedings. International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2002.1115088},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings. International Symposium on Mixed and Augmented Reality/2002/Bimber, Frohlich/Bimber, Frohlich - 2002 - Occlusion shadows using projected light to generate realistic occlusion effects for view-dependent optical see.pdf:pdf},
isbn = {0-7695-1781-1},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {186--319},
publisher = {IEEE Comput. Soc},
title = {{Occlusion shadows: using projected light to generate realistic occlusion effects for view-dependent optical see-through displays}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1115088 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1115088},
year = {2002}
}
@article{Hattenberger2009,
abstract = {The overarching goal of this research was to compare different rendering solutions in order to understand why some yield better results specifically when applied to rendering synthetic objects into real photographs. A psychophysical experiment was conducted in which the composite images were judged for accuracy against the original photograph. In addition, iCAM, an image color appearance model was also used to calculate image differences for the same set of images. Conclusions obtained included the effect of global illumination on the accuracy of the final composite rendering. Also, it was discovered that the original rendering with all of its artifacts is not necessarily an indicator of the final composite image's judged accuracy. Finally, initial results show promise in using iCAM to predict a relationship similar to the psychophysics, which could eventually be used in-the-rendering-loop to achieve photorealism with minimized computation.},
author = {Hattenberger, Timothy J. and Fairchild, Mark D. and Johnson, Garrett M. and Salvaggio, Carl},
doi = {10.1145/1462055.1462057},
issn = {15443558},
journal = {ACM Transactions on Applied Perception},
month = feb,
number = {1},
pages = {1--22},
title = {{A psychophysical investigation of global illumination algorithms used in augmented reality}},
url = {http://portal.acm.org/citation.cfm?doid=1462055.1462057},
volume = {6},
year = {2009}
}
@article{Schall1999,
abstract = {We review neural correlates of perceptual and motor decisions, examining whether the time they occupy explains the duration and variability of behavioral reaction times. The location of a salient target is identified through a spatiotemporal evolution of visually evoked activation throughout the visual system. Selection of the target leads to stochastic growth of movement-related activity toward a fixed threshold to generate the gaze shift. For a given image, the neural concomitants of perceptual processing occupy a relatively constant interval so that stochastic variability in response generation introduces additional variability in reaction times.},
author = {Schall, J D and Thompson, K G},
doi = {10.1146/annurev.neuro.22.1.241},
issn = {0147-006X},
journal = {Annual review of neuroscience},
keywords = {Animals,Eye Movements,Eye Movements: physiology,Humans,Nervous System Physiological Phenomena,Ocular,Ocular: physiology,Psychomotor Performance,Psychomotor Performance: physiology,Saccades,Saccades: physiology,Vision},
month = jan,
pages = {241--59},
pmid = {10202539},
title = {{Neural selection and control of visually guided eye movements.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10202539},
volume = {22},
year = {1999}
}
@inproceedings{Lensing2012,
abstract = {For seamless integration of virtual content into real scenes, realizing mutual global lighting effects between both worlds belongs to the most important and challenging goals. Therefore, plenty of global illumination approaches exist, which mostly share the same restriction: the real scene is approximated by a static model, which was built in advance and thus has to remain static. In our paper, we propose an image-space global illumination approach, based on reflective shadow maps, combined with the use of an RGB-D camera, to simulate first bounce diffuse indirect illumination without any pre-computations. Our approach supports indirect illumination in both directions (real to virtual and vice versa) and runs in real-time. Furthermore, it does not require advanced shader properties, since we developed an implementation making efficient usage of the Z-Buffer algorithm for calculating indirect illumination.},
address = {Atlanta, GA},
author = {Lensing, Philipp and Broll, Wolfgang},
booktitle = {2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2012.6402547},
isbn = {978-1-4673-4662-7},
month = nov,
pages = {109--118},
publisher = {IEEE},
title = {{Instant indirect illumination for dynamic mixed reality scenes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6402547},
year = {2012}
}
@inproceedings{Schindler2004,
abstract = {Edges in man-made environments, grouped according to vanishing point directions, provide single-view constraints that have been exploited before as a precursor to both scene understanding and camera calibration. A Bayesian approach to edge grouping was proposed in the "Manhattan World" paper by Coughlan and Yuille, where they assume the existence of three mutually orthogonal vanishing directions in the scene. We extend the thread of work spawned by Coughlan and Yuille in several significant ways. We propose to use the expectation maximization (EM) algorithm to perform the search over all continuous parameters that influence the location of the vanishing points in a scene. Because EM behaves well in high-dimensional spaces, our method can optimize over many more parameters than the exhaustive and stochastic algorithms used previously for this task. Among other things, this lets us optimize over multiple groups of orthogonal vanishing directions, each of which induces one additional degree of freedom. EM is also well suited to recursive estimation of the kind needed for image sequences and/or in mobile robotics. We present experimental results on images of "Atlanta worlds", complex urban scenes with multiple orthogonal edge-groups, that validate our approach. We also show results for continuous relative orientation estimation on a mobile robot.},
author = {Schindler, G and Dellaert, F},
booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
doi = {10.1109/CVPR.2004.1315033},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004/2004/Schindler, Dellaert/Schindler, Dellaert - 2004 - Atlanta world an expectation maximization framework for simultaneous low-level edge grouping and camera cal.pdf:pdf},
isbn = {0-7695-2158-4},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {203--209},
publisher = {IEEE},
title = {{Atlanta world: an expectation maximization framework for simultaneous low-level edge grouping and camera calibration in complex man-made environments}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1315033 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1315033},
volume = {1},
year = {2004}
}
@article{Lanir2013,
address = {New York, New York, USA},
author = {Lanir, Joel and Stone, Ran and Cohen, Benjamin and Gurevich, Pavel},
doi = {10.1145/2470654.2481309},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13/2013/Lanir et al/Lanir et al. - 2013 - Ownership and control of point of view in remote assistance.pdf:pdf},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2243},
publisher = {ACM Press},
title = {{Ownership and control of point of view in remote assistance}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481309},
year = {2013}
}
@inproceedings{Schild2013,
abstract = {Creating graphical user interfaces (GUI) for stereoscopic 3D (S3D) games is a difficult choice between visual comfort and effect. We pres ent a S3D Game GUI Desi gn Space and a list s of S3D-specific attributes that emphasizes integrating visually comfort able interfaces into the ga me world, story and S3D view. To showcase our approach, we created two GUI concepts and evaluated them with 32 users. Our results show quality improvements for combination of bottom position and visual attachment fo r a menu. In a referencing inter face, placing the reference n ear to the target depth significantly improved perceived quali ty, game integration, and increased presence. These resu lts confirm the need to create S3D GUIs with perceptual constraints in mind, demons trt ating the potential to exte nd the user experience. Additionally, our design space offers a formal and flexible way to create new effects in S3D GUIs.},
address = {New York, New York, USA},
author = {Schild, Jonas and B\"{o}licke, Liane and {LaViola Jr.}, Joseph J. and Masuch, Maic},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470678},
isbn = {9781450318990},
pages = {169--178},
publisher = {ACM Press},
title = {{Creating and analyzing stereoscopic 3D graphical user interfaces in digital games}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470678},
year = {2013}
}
@inproceedings{Santner2013,
abstract = {The estimation of human attention has recently been addressed in the context of human robot interaction. Today, joint work spaces already exist and challenge cooperating systems to jointly focus on common objects, scenes and work niches. With the advent of Google glasses and increasingly affordable wearable eye-tracking, monitoring of human attention will soon become ubiquitous. The presented work describes for the first time a method for the estimation of human fixations in 3D environments that does not require any artificial landmarks in the field of view and enables attention mapping in 3D models. It enables full 3D recovery of the human view frustum and the gaze pointer in a previously acquired 3D model of the environment in real time. The study on the precision of this method reports a mean projection error ≈1.1 cm and a mean angle error ≈0.6° within the chosen 3D model - the precision does not go below the one of the technical instrument (≈1°) This innovative methodology will open new opportunities for joint attention studies as well as for bringing new potential into automated processing for human factors technologies.},
author = {Santner, Katrin and Fritz, Gerald and Paletta, Lucas and Mayer, Heinz},
booktitle = {2013 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6631185},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2013 IEEE International Conference on Robotics and Automation/2013/Santner et al/Santner et al. - 2013 - Visual recovery of saliency maps from human attention in 3D environments.pdf:pdf},
isbn = {978-1-4673-5643-5},
keywords = {gaze 3D},
mendeley-tags = {gaze 3D},
month = may,
pages = {4297--4303},
publisher = {IEEE},
title = {{Visual recovery of saliency maps from human attention in 3D environments}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6631185 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6631185},
year = {2013}
}
@article{Brasil2011,
author = {Brasil, Igor Saraiva and Neto, Francisco Milton Mendes and Chagas, Jos\'{e} Ferdinandy Silva and Lima, Rodrigo Monteiro De and Souza, Daniel Faustino Lacerda and Bonates, Mara Franklin and Dantas, Andre},
doi = {10.1109/SVR.2011.13},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 XIII Symposium on Virtual Reality/2011/Brasil et al/Brasil et al. - 2011 - An Inteligent Agent-Based Virtual Game for Oil Drilling Operators Training.pdf:pdf},
isbn = {978-1-4577-0661-5},
journal = {2011 XIII Symposium on Virtual Reality},
keywords = {-educative games,multi-agent systems,virtual re-},
month = may,
pages = {9--17},
publisher = {Ieee},
title = {{An Inteligent Agent-Based Virtual Game for Oil Drilling Operators Training}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951830},
year = {2011}
}
@book{Gutierrez2008,
address = {London},
author = {Guti\'{e}rrez, Mario a. a. and Vexo, Fr\'{e}d\'{e}ric and Thalmann, Daniel},
doi = {10.1007/978-1-84800-117-6},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2008/Guti\'{e}rrez, Vexo, Thalmann/Guti\'{e}rrez, Vexo, Thalmann - 2008 - Stepping into Virtual Reality.pdf:pdf},
isbn = {978-1-84800-116-9},
publisher = {Springer London},
title = {{Stepping into Virtual Reality}},
url = {http://www.springerlink.com/index/10.1007/978-1-84800-117-6},
year = {2008}
}
@inproceedings{Winfield2005,
abstract = {Knowing the user's point of gaze has significant potential to enhance current human-computer interfaces, given that eye movements can be used as an indicator of the attentional state of a user. The primary obstacle of integrating eye movements into today's interfaces is the availability of a reliable, low-cost open-source eye-tracking system. Towards making such a system available to interface designers, we have developed a hybrid eye-tracking algorithm that integrates feature-based and model-based approaches and made it available in an open-source package. We refer to this algorithm as "starburst" because of the novel way in which pupil features are detected. This starburst algorithm is more accurate than pure feature-based approaches yet is signi?cantly less time consuming than pure modelbased approaches. The current implementation is tailored to tracking eye movements in infrared video obtained from an inexpensive head-mounted eye-tracking system. A validation study was conducted and showed that the technique can reliably estimate eye position with an accuracy of approximately one degree of visual angle.},
author = {Winfield, D. and Parkhurst, D.J.},
booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops},
doi = {10.1109/CVPR.2005.531},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops/2005/Winfield, Parkhurst/Winfield, Parkhurst - 2005 - Starburst A hybrid algorithm for video-based eye tracking combining feature-based and model-based approache.pdf:pdf},
isbn = {0-7695-2372-2},
issn = {1063-6919},
keywords = {Algorithm design and analysis,Availability,Computer interfaces,Computer vision,Costs,Hardware,Human computer interaction,Keyboards,Open source software,Tracking,eye tracker,toshi},
mendeley-tags = {eye tracker,toshi},
pages = {79--79},
publisher = {IEEE},
shorttitle = {Computer Vision and Pattern Recognition - Workshop},
title = {{Starburst: A hybrid algorithm for video-based eye tracking combining feature-based and model-based approaches}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1565386},
volume = {3},
year = {2005}
}
@inproceedings{Meiguins2006,
abstract = {The goal of this chapter is to provide the reader a brief introduction on the necessary characteristics of a good information visualization tool, as well as the tasks the user may perform in this type of tool. Data types for visualization and the more adequate technique for each data type are also discussed. Finally, we present some information visualization techniques applied to three-dimensional virtual environments.},
address = {Bel\'{e}m-PA},
author = {Meiguins, Bianchi Serique and Gon\c{c}alves, Aruanda Sim\~{o}es and Garcia, Marcelo de Brito and Godinho, Paulo Igor Alves and J\'{u}nior, Rosevaldo Dias de Souza},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {319--326},
publisher = {SBC},
title = {{Realidade Virtual e Aumentada em Visualiza\c{c}\~{a}o de Informa\c{c}\~{a}o}},
year = {2006}
}
@article{Yun2013,
address = {New York, New York, USA},
author = {Yun, Tae-Jung and Arriaga, Rosa I.},
doi = {10.1145/2470654.2466233},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1769},
publisher = {ACM Press},
title = {{A text message a day keeps the pulmonologist away}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466233},
year = {2013}
}
@article{Chung1999,
author = {Chung, Kyung H. and Shewchuk, John P. and Williges, Robert C.},
doi = {10.1002/(SICI)1520-6564(199923)9:4<331::AID-HFM1>3.0.CO;2-3},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Human Factors and Ergonomics in Manufacturing/1999/Chung, Shewchuk, Williges/Chung, Shewchuk, Williges - 1999 - An application of augmented reality to thickness inspection.pdf:pdf},
issn = {1090-8471},
journal = {Human Factors and Ergonomics in Manufacturing},
number = {4},
pages = {331--342},
title = {{An application of augmented reality to thickness inspection}},
url = {http://doi.wiley.com/10.1002/\%28SICI\%291520-6564\%28199923\%299\%3A4\%3C331\%3A\%3AAID-HFM1\%3E3.0.CO\%3B2-3},
volume = {9},
year = {1999}
}
@article{Woolard2006,
author = {Woolard, a. and Laliodati, V. and Hedley, N. and Carrigan, N. and Hammond, M. and Julien, J.},
doi = {10.1109/ISMAR.2003.1240727},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings/2006/Woolard et al/Woolard et al. - 2006 - Case studies in application of augmented reality in future media production.pdf:pdf},
isbn = {0-7695-2006-5},
journal = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
pages = {294--295},
publisher = {IEEE Comput. Soc},
title = {{Case studies in application of augmented reality in future media production}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240727},
year = {2006}
}
@inproceedings{Shi2013,
abstract = {Although privacy problems in Social Network Sites (SNS) have become more salient than ever in recent years, interpersonal privacy issues in SNS remain understudied. This study aims to generate insights in understanding users' interpersonal privacy concerns by expounding interpersonal privacy boundaries in SNS. Through a case analysis of Friendship Pages on Facebook, this paper identifies users' interpersonal privacy concerns that are rooted from informational norms outlined in the theory of contextual integrity, as well as the tensions that occur within and cross these informational norms. This paper concludes with a discussion of design implications and future research.},
address = {New York, New York, USA},
author = {Shi, Pan and Xu, Heng and Chen, Yunan},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470660},
isbn = {9781450318990},
keywords = {a growing body of,and their behaviors on,co-owners of shared,e,has studied the,i,inability to monitor friends,information,privacy research in hci,sns},
pages = {35--38},
publisher = {ACM Press},
title = {{Using contextual integrity to examine interpersonal information boundary on social network sites}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470660},
year = {2013}
}
@article{Foulsham2012,
abstract = {Complex stimuli and tasks elicit particular eye movement sequences. Previous research has focused on comparing between these scanpaths, particularly in memory and imagery research where it has been proposed that observers reproduce their eye movements when recognizing or imagining a stimulus. However, it is not clear whether scanpath similarity is related to memory performance and which particular aspects of the eye movements recur. We therefore compared eye movements in a picture memory task, using a recently proposed comparison method, MultiMatch, which quantifies scanpath similarity across multiple dimensions including shape and fixation duration. Scanpaths were more similar when the same participant’s eye movements were compared from two viewings of the same image than between different images or different participants viewing the same image. In addition, fixation durations were similar within a participant and this similarity was associated with memory performance.},
author = {Foulsham, T and Dewhurst, R and Nystr\"{o}m, M},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of Eye Movement Research/2012/Foulsham, Dewhurst, Nystr\"{o}m/Foulsham, Dewhurst, Nystr\"{o}m - 2012 - Comparing scanpaths during scene encoding and recognition A multi-dimensional approach.pdf:pdf},
journal = {Journal of Eye Movement Research},
keywords = {eye tracking,memory,scanpath,scanpaths,scene perception,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
number = {3},
pages = {1--12},
title = {{Comparing scanpaths during scene encoding and recognition: A multi-dimensional approach}},
url = {http://dspace.ou.nl/handle/1820/4627},
volume = {5},
year = {2012}
}
@article{Filho2011,
author = {Filho, Ronaldo Ferreira Dos Anjos and Teichrieb, Veronica and Kelner, Judith},
doi = {10.1109/SVR.2011.22},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 XIII Symposium on Virtual Reality/2011/Filho, Teichrieb, Kelner/Filho, Teichrieb, Kelner - 2011 - Hydra Virtual Environments Development Platform.pdf:pdf},
isbn = {978-1-4577-0661-5},
journal = {2011 XIII Symposium on Virtual Reality},
keywords = {customization,development,fast,virtual environments development platform},
month = may,
pages = {102--111},
publisher = {Ieee},
title = {{Hydra: Virtual Environments Development Platform}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951841},
year = {2011}
}
@article{Comport2006,
abstract = {Tracking is a very important research subject in a real-time augmented reality context. The main requirements for trackers are high accuracy and little latency at a reasonable cost. In order to address these issues, a real-time, robust, and efficient 3D model-based tracking algorithm is proposed for a "video see through" monocular vision system. The tracking of objects in the scene amounts to calculating the pose between the camera and the objects. Virtual objects can then be projected into the scene using the pose. Here, nonlinear pose estimation is formulated by means of a virtual visual servoing approach. In this context, the derivation of point-to-curves interaction matrices are given for different 3D geometrical primitives including straight lines, circles, cylinders, and spheres. A local moving edges tracker is used in order to provide real-time tracking of points normal to the object contours. Robustness is obtained by integrating an M-estimator into the visual control law via an iteratively reweighted least squares implementation. This approach is then extended to address the 3D model-free augmented reality problem. The method presented in this paper has been validated on several complex image sequences including outdoor environments. Results show the method to be robust to occlusion, changes in illumination, and mistracking.},
author = {Comport, A.I. and Marchand, \'{E}ric and Pressigout, Muriel and Chaumette, Fran\c{c}ois},
doi = {10.1109/TVCG.2006.78},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Visualization and Computer Graphics/2006/Comport et al/Comport et al. - 2006 - Real-time markerless tracking for augmented reality the virtual visual servoing framework.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Algorithms,Computer Graphics,Computer Systems,Computer-Assisted,Computer-Assisted: methods,Feedback,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Information Storage and Retrieval,Information Storage and Retrieval: methods,Signal Processing,Three-Dimensional,Three-Dimensional: methods,User-Computer Interface},
number = {4},
pages = {615--628},
pmid = {16805268},
publisher = {IEEE Computer Society},
title = {{Real-time markerless tracking for augmented reality: the virtual visual servoing framework}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16805268 http://www.computer.org/portal/web/csdl/doi/10.1109/tvcg.2006.78},
volume = {12},
year = {2006}
}
@inproceedings{Tu1994,
abstract = {This paper proposes a framework for animation that can achieve the intricacy of motion evident in certain natural ecosystems with minimal input from the animator. The realistic appearance, movement, and behavior of individual animals, as well as the patterns of behavior evident in groups of animals fall within the scope of the framework. Our approach to emulating this level of natural complexity is to model each animal holistically as an autonomous agent situated in its physical world. To demonstrate the approach, we develop a physics-based, virtual marine world. The world is inhabited by artificial fishes that can swim hydrodynamically in simulated water through the motor control of internal muscles that motivates fins. Their repertoire of behaviors relies on their perception of the dynamic environment. As in nature, the detailed motions of artificial fishes in their virtual habitat are not entirely predictable because they are not scripted.},
address = {New York, New York, USA},
author = {Tu, Xiaoyuan and Terzopoulos, Demetri},
booktitle = {Proceedings of the 21st annual conference on Computer graphics and interactive techniques - SIGGRAPH '94},
doi = {10.1145/192161.192170},
isbn = {0897916670},
keywords = {abstract,achieve the intricacy of,animate vision,artificial life,autonomous agents,behavioral animation,can,framework for animation that,locomotion control,motion evident in certain,natural ecosystems,physics-based modeling,this paper proposes a},
pages = {43--50},
publisher = {ACM Press},
title = {{Artificial fishes: physics, locomotion, perception, behavior}},
url = {http://portal.acm.org/citation.cfm?doid=192161.192170},
year = {1994}
}
@inproceedings{Komarov2013,
abstract = {Online labor markets, such as Amazon's Mechanical Turk (MTurk), provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings. However, because the experimenter gives up the direct control over the participants' environments and behavior, concerns about the quality of the data collected in online settings are pervasive. In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via MTurk. We implemented three performance experiments to re-evaluate three previously well-studied user interface designs. We conducted each experiment both in lab and online with participants recruited via MTurk. The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on MTurk and the effect sizes were similar. In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments. These results suggest that MTurk may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.},
address = {New York, New York, USA},
author = {Komarov, Steven and Reinecke, Katharina and Gajos, Krzysztof Z.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470684},
isbn = {9781450318990},
pages = {207--216},
publisher = {ACM Press},
title = {{Crowdsourcing performance evaluations of user interfaces}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470684},
year = {2013}
}
@inproceedings{Garbin2006,
abstract = {By Augmented Reality (RA) is possible to developing alternative systems to work with impairments needs children, allowing augmenting the sensorial channels and helping the perceptual process. This chapter presents interactive and alternative RA system for tasks with impairments needs children.},
address = {Bel\'{e}m-PA},
author = {Garbin, Tania Rossi and Dainese, Carlos Alberto and Kirner, Cl\'{a}udio (UNIFEI) and Dainesse, Carlos Alberto},
booktitle = {Symposium of Virtual Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Symposium of Virtual Reality/2006/Tori, Kirner/Tori, Kirner - 2006 - Fundamentos de Realidade Virtual.pdf:pdf},
pages = {289--297},
publisher = {SBC},
title = {{Sistema de Realidade Aumentada para Trabalho com Crian\c{c}as Portadoras de Necessidades Especiais}},
year = {2006}
}
@misc{Loureiro2012,
author = {Loureiro, S\'{e}rgio A.},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2012/Loureiro/Loureiro - 2012 - Revis\~{a}o Sistem\'{a}tica da Literatura.pdf:pdf},
keywords = {RBS},
mendeley-tags = {RBS},
title = {{Revis\~{a}o Sistem\'{a}tica da Literatura}},
url = {http://vision.ime.usp.br/~acmt/revisao-sistematica-literatura.pdf},
urldate = {2013-11-22},
year = {2012}
}
@inproceedings{Sugimura2009,
abstract = {In this work, we propose a method for tracking individuals in crowds. Our method is based on a trajectory-based clustering approach that groups trajectories of image features that belong to the same person. The key novelty of our method is to make use of a person's individuality, that is, the gait features and the temporal consistency of local appearance to track each individual in a crowd. Gait features in the frequency domain have been shown to be an effective biometric cue in discriminating between individuals, and our method uses such features for tracking people in crowds for the first time. Unlike existing trajectory-based tracking methods, our method evaluates the dissimilarity of trajectories with respect to a group of three adjacent trajectories. In this way, we incorporate the temporal consistency of local patch appearance to differentiate trajectories of multiple people moving in close proximity. Our experiments show that the use of gait features and the temporal consistency of local appearance contributes to significant performance improvement in tracking people in crowded scenes.},
address = {Kyoto},
author = {Sugimura, Daisuke and Kitani, Kris M and Okabe, Takahiro and Sato, Yoichi and Sugimoto, Akihiro},
booktitle = {2009 IEEE 12th International Conference on Computer Vision},
doi = {10.1109/ICCV.2009.5459286},
isbn = {978-1-4244-4420-5},
month = sep,
number = {Iccv},
pages = {1467--1474},
publisher = {IEEE},
title = {{Using individuality to track individuals: Clustering individual trajectories in crowds using local appearance and frequency trait}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459286},
year = {2009}
}
@incollection{Madsen2010,
author = {Madsen and Lal},
booktitle = {Augmented Reality},
chapter = {2},
editor = {Maad, Soha},
isbn = {9789537619695},
number = {January},
pages = {15--31},
publisher = {Intech},
title = {{Probeless Illumination Estimation for Outdoor Augmented Reality}},
url = {http://www.intechopen.com/books/augmented-reality/p robeless-illumination-estimation-for-outdoor- augmented-reality},
year = {2010}
}
@article{Pilet2005,
author = {Pilet, J. and Lepetit, V. and Fua, P.},
doi = {10.1109/ISMAR.2005.18},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)/2005/Pilet, Lepetit, Fua/Pilet, Lepetit, Fua - 2005 - Augmenting deformable objects in real-time.pdf:pdf},
isbn = {0-7695-2459-1},
journal = {Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)},
pages = {134--137},
publisher = {Ieee},
title = {{Augmenting deformable objects in real-time}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544675},
volume = {1},
year = {2005}
}
@book{N.Magne2004,
author = {Magnenat-Thalmann, Nadia and Thalmann, Daniel},
isbn = {0470023163},
keywords = {crowd simulation,virtual human},
mendeley-tags = {crowd simulation,virtual human},
pages = {468},
publisher = {Wiley},
title = {{Handbook of Virtual Humans}},
url = {http://www.amazon.com/Handbook-Virtual-Humans-Nadia-Magnenat-Thalmann/dp/0470023163},
year = {2004}
}
@article{postwimp,
author = {van Dam, Andries},
doi = {10.1145/253671.253708},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {SBGames},
mendeley-tags = {SBGames},
month = feb,
number = {2},
pages = {63--67},
publisher = {ACM},
title = {{Post-WIMP user interfaces}},
url = {http://portal.acm.org/citation.cfm?doid=253671.253708},
volume = {40},
year = {1997}
}
@inproceedings{Saxena2007,
abstract = {We consider the problem of estimating detailed 3D structure from a single still image of an unstructured environment. Our goal is to create 3D models which are both quantitatively accurate as well as visually pleasing. For each small homogeneous patch in the image, we use a Markov random field (MRF) to infer a set of "plane parameters" that capture both the 3D location and 3D orientation of the patch. The MRF, trained via supervised learning, models both image depth cues as well as the relationships between different parts of the image. Inference in our model is tractable, and requires only solving a convex optimization problem. Other than assuming that the environment is made up of a number of small planes, our model makes no explicit assumptions about the structure of the scene; this enables the algorithm to capture much more detailed 3D structure than does prior art (such as Saxena et ah, 2005, Delage et ah, 2005, and Hoiem et el, 2005), and also give a much richer experience in the 3D flythroughs created using image-based rendering, even for scenes with significant non-vertical structure. Using this approach, we have created qualitatively correct 3D models for 64.9\% of 588 images downloaded from the Internet, as compared to Hoiem et al.'s performance of 33.1\%. Further, our models are quantitatively more accurate than either Saxena et al. or Hoiem et al.},
author = {Saxena, Ashutosh and Sun, Min and Ng, Andrew Y.},
booktitle = {2007 IEEE 11th International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408828},
isbn = {978-1-4244-1630-1},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {1--8},
publisher = {IEEE},
title = {{Learning 3-D Scene Structure from a Single Still Image}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4408828 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4408828},
year = {2007}
}
@article{Hagemann2006,
abstract = {A major element in expert sports performance, particularly racket-and-ball games, is excellent anticipatory skill. A prestudy combined the temporal and spatial occlusion paradigms to ascertain which key stimuli badminton players use for anticipating the direction of overhead shots. The main study then evaluated a program for training anticipatory skills; 200 video clips were employed to orient attention toward these key stimuli. Participants were 63 badminton novices, 20 national league players, and 21 local league players. A transparent red patch (exogenous orienting) was used to orient attention toward the trunk up to 160 ms before racket-shuttle contact; the arm, from 160 ms to 80 ms before contact; and the racket, from 80 ms before to actual contact. Results showed that badminton novices who trained with this program signifi cantly improved their anticipatory skill between post- and retention test compared with controls. Whereas local league players improved from pre- to posttest, training had no effect on expert national league players. It is concluded that using red transparent patches to highlight the most informative cues in perceptual training programs is a promising way to improve anticipatory skill.},
author = {Hagemann, N},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of Sport \& Exercise Psychology/2006/Hagemann/Hagemann - 2006 - Training perceptual skill by orienting visual attention.pdf:pdf},
journal = {Journal of Sport \& Exercise Psychology},
number = {2},
pages = {143--158},
title = {{Training perceptual skill by orienting visual attention}},
url = {http://dialnet.unirioja.es/servlet/articulo?codigo=2261317\&info=resumen\&idioma=ENG},
volume = {28},
year = {2006}
}
@book{Huang2014,
address = {New York, NY},
doi = {10.1007/978-1-4614-7485-2},
editor = {Huang, Weidong},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2014/Unknown/Unknown - 2014 - Handbook of Human Centric Visualization.pdf:pdf},
isbn = {978-1-4614-7484-5},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
publisher = {Springer New York},
title = {{Handbook of Human Centric Visualization}},
url = {http://link.springer.com/10.1007/978-1-4614-7485-2},
year = {2014}
}
@inproceedings{Hillaire2008,
abstract = {This paper describes the use of user's focus point to improve some visual effects in virtual environments (VE). First, we describe how to retrieve user's focus point in the 3D VE using an eye-tracking system. Then, we propose the adaptation of two rendering techniques which aim at improving users' sensations during first-person navigation in VE using his/her focus point: (1) a camera motion which simulates eyes movement when walking, i.e., corresponding to vestibulo-ocular and vestibulocollic reflexes when the eyes compensate body and head movements in order to maintain gaze on a specific target, and (2) a depth-of-field (DoF) blur effect which simulates the fact that humans perceive sharp objects only within some range of distances around the focal distance. Second, we describe the results of an experiment conducted to study users' subjective preferences concerning these visual effects during first-person navigation in VE. It showed that participants globally preferred the use of these effects when they are dynamically adapted to the focus point in the VE. Taken together, our results suggest that the use of visual effects exploiting users' focus point could be used in several VR applications involving first- person navigation such as the visit of architectural site, training simulations, video games, etc.},
author = {Hillaire, Sebastien and Lecuyer, Anatole and Cozot, Remi and Casiez, Gery},
booktitle = {2008 IEEE Virtual Reality Conference},
doi = {10.1109/VR.2008.4480749},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2008 IEEE Virtual Reality Conference/2008/Hillaire et al/Hillaire et al. - 2008 - Using an Eye-Tracking System to Improve Camera Motions and Depth-of-Field Blur Effects in Virtual Environments.pdf:pdf},
isbn = {978-1-4244-1971-5},
keywords = {eye tracking,video game},
mendeley-tags = {eye tracking,video game},
pages = {47--50},
publisher = {IEEE},
title = {{Using an Eye-Tracking System to Improve Camera Motions and Depth-of-Field Blur Effects in Virtual Environments}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4480749 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4480749},
year = {2008}
}
@article{Nikodym2013,
abstract = {We propose a framework that captures multiple high dynamic range environment maps and decomposes them into sets of directional light sources in real-time. The environment maps, captured and processed on stand-alone devices (e.g. Nokia N900 smartphone), are available to rendering engines via a server that provides wireless access. We compare three different importance sampling techniques in terms of the quality of sampling pattern, temporal coherence, and performance. Furthermore, we propose a novel idea of merging the directional light sources from multiple cameras by interpolation. We then discuss the pros and cons when using multiple cameras},
author = {Nikod\'{y}m, T and Havran, V and Bittner, J},
file = {:home/acmt/Dropbox/Documentos/Mendeley/International Conference on Computer Graphics, Visualization and Computer Vision/2013/Nikod\'{y}m, Havran, Bittner/Nikod\'{y}m, Havran, Bittner - 2013 - Multiple Live Video Environment Map Sampling(2).pdf:pdf},
journal = {International Conference on Computer Graphics, Visualization and Computer Vision},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
number = {1},
title = {{Multiple Live Video Environment Map Sampling}},
url = {http://dcgi.felk.cvut.cz/home/havran/ARTICLES/nikodym2013wscg.pdf},
volume = {21},
year = {2013}
}
@article{Lee2013b,
address = {New York, New York, USA},
author = {Lee, Uichin and Kim, Jihyoung and Yi, Eunhee and Sung, Juyup and Gerla, Mario},
doi = {10.1145/2470654.2470730},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {533},
publisher = {ACM Press},
title = {{Analyzing crowd workers in mobile pay-for-answer q\&a}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470730},
year = {2013}
}
@inproceedings{Raij2004,
abstract = {By treating projectors as pin-hole cameras, we show it is possible to calibrate the projectors of a casually-aligned, multi-projector display wall using the principles of planar auto-calibration. We also use a pose estimation technique for planar scenes to reconstruct the relative pose of a calibration camera, the projectors and the plane they project on. Together with assumptions about the pose of the camera, we use the reconstruction to automatically compute the projector-display homographies needed to render properly scaled and oriented imagery on the display wall. The main contribution of this paper is thus to provide a fully automated approach to calibrate a multi-projector display wall without the need for fiducials or interaction.},
author = {Raij, A and Pollefeys, M},
booktitle = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
doi = {10.1109/ICPR.2004.1333994},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004/2004/Raij, Pollefeys/Raij, Pollefeys - 2004 - Auto-calibration of multi-projector display walls.pdf:pdf},
isbn = {0-7695-2128-2},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
pages = {14--17 Vol.1},
publisher = {IEEE},
title = {{Auto-calibration of multi-projector display walls}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1333994 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1333994},
year = {2004}
}
@inproceedings{Snowdon2001,
author = {Snowdon, Dave and Churchill, E.F. and Munro, A.J.},
booktitle = {Collaborative Virtual Environments},
doi = {10.1.1.114.9226},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Collaborative Virtual Environments/2001/Snowdon, Churchill, Munro/Snowdon, Churchill, Munro - 2001 - Collaborative virtual environments Digital spaces and places for CSCW An introduction.pdf:pdf},
pages = {3--17},
publisher = {Citeseer},
title = {{Collaborative virtual environments: Digital spaces and places for CSCW: An introduction}},
year = {2001}
}
@inproceedings{Duchowski2001,
abstract = {This paper presents novel software techniques for binocular eye tracking within Virtual Reality and discusses their application to aircraft inspection training. The aesthetic appearance of the environment is driven by standard graphical techniques augmented by realistic texture maps of the physical environment. The user's gaze direction, as well as head position and orientation, are tracked to allow recording of the user's fixations within the environment. Methods are given for (1) integration of the eye tracker into a Virtual Reality framework, (2) stereo calculation of the user's 3D gaze vector, (3) a new 3D calibration technique developed to estimate the user's inter-pupillary distance post-facto, and (4) a new technique for eye movement analysis in 3-space. The 3D eye movement analysis technique is an improvement over traditional 2D approaches since it takes into account the 6 degrees of freedom of head movements and is resolution independent. Results indicate that although the current signal analysis approach is somewhat noisy and tends to underestimate the identified number of fixations, recorded eye movements provide valuable human factors process measures complementing performance statistics used to gauge training effectiveness.},
address = {New York, New York, USA},
author = {Duchowski, Andrew T. and Medlin, Eric and Gramopadhye, Anand and Melloy, Brian and Nair, Santosh},
booktitle = {Proceedings of the ACM symposium on Virtual reality software and technology - VRST '01},
doi = {10.1145/505008.505010},
file = {::},
isbn = {1581134274},
keywords = {gaze 3D},
mendeley-tags = {gaze 3D},
pages = {1},
publisher = {ACM Press},
title = {{Binocular eye tracking in VR for visual inspection training}},
url = {http://dl.acm.org/citation.cfm?id=505010 http://portal.acm.org/citation.cfm?doid=505008.505010},
year = {2001}
}
@article{Kim2002,
author = {Kim, Taewoo and Lee, Jinho and Fishwick, Paul},
doi = {10.1145/643114.643118},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM Transactions on Modeling and Computer Simulation/2002/Kim, Lee, Fishwick/Kim, Lee, Fishwick - 2002 - A two-stage modeling and simulation process for web-based modeling and simulation.pdf:pdf},
issn = {10493301},
journal = {ACM Transactions on Modeling and Computer Simulation},
month = jul,
number = {3},
pages = {230--248},
title = {{A two-stage modeling and simulation process for web-based modeling and simulation}},
url = {http://portal.acm.org/citation.cfm?doid=643114.643118},
volume = {12},
year = {2002}
}
@article{Zhou2007,
author = {Zhou, ZhiYing and Cheok, Adrian David and Qiu, Yan and Yang, Xubo},
doi = {10.1109/TSMCA.2006.886376},
issn = {1083-4427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
keywords = {augmented reality,sound},
mendeley-tags = {augmented reality,sound},
month = mar,
number = {2},
pages = {262--272},
title = {{The Role of 3-D Sound in Human Reaction and Performance in Augmented Reality Environments}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4100785 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4100785},
volume = {37},
year = {2007}
}
@inproceedings{Pessoa2009,
abstract = {This paper presents a solution for the photorealistic rendering of synthetic objects into dynamic real scenes, in Augmented Reality applications. In order to achieve this goal, an Image Based Lighting approach is proposed, where environment maps with different levels of glossiness are generated for each virtual object in the scene at every frame. Due to this, illumination effects, such as color bleeding and specular reflections, can be simulated for virtual objects in a consistent way, even under the presence of scene changes. A unifying sampling method for the spherical harmonics transformation pass is also used. It is independent of map format and does not need to apply different weights for each sample. The developed technique is combined with an extended version of Lafortune Spatial BRDF, featuring Fresnel effect and an innovative tangent rotation parameterization. The solution is evaluated in various Augmented Reality case studies, where other features like shadowing and lens effects are also exploited.},
address = {Lafayette, LA},
author = {Pessoa, S.a. and {de S. Moura}, G. and Lima, J.P.S.M. and Teichrieb, V. and Kelner, J.},
booktitle = {2009 IEEE Virtual Reality Conference},
doi = {10.1109/VR.2009.4811036},
isbn = {978-1-4244-3943-0},
issn = {1087-8270},
month = mar,
pages = {243--244},
publisher = {IEEE},
title = {{A Global Illumination and BRDF Solution Applied to Photorealistic Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4811036},
year = {2009}
}
@inproceedings{Zhu2010,
abstract = {In teleoperation, operators usually have to control multiple devices simultaneously, which requires frequent hand switches between different controllers. We designed and implemented two prototypes, one by applying head motion and the other by integrating eye gaze as intrinsic elements of teleoperation for remote camera control in a multi-control setting. We report a user study of a modeled multi-control experiment that compares the performance of head tracking control, eye tracking control and traditional joystick control. The results provide clear evidence that eye tracking control significantly outperforms joystick and head tracking control in both objective measures and subjective measures.},
address = {New York, New York, USA},
annote = {- cited by: 3
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Zhu, Dingyun and Gedeon, Tom and Taylor, Ken},
booktitle = {Proceedings of the 28th of the international conference extended abstrats on Human factors in computing systems - CHI EA '10},
doi = {10.1145/1753846.1753963},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 28th of the international conference extended abstrats on Human factors in computing systems - CHI EA '10/2010/Zhu, Gedeon, Taylor/Zhu, Gedeon, Taylor - 2010 - Natural interaction enhanced remote camera control for teleoperation.pdf:pdf},
isbn = {9781605589305},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {3229--3234},
publisher = {ACM Press},
title = {{Natural interaction enhanced remote camera control for teleoperation}},
url = {http://dl.acm.org/citation.cfm?id=1753963 http://portal.acm.org/citation.cfm?doid=1753846.1753963},
year = {2010}
}
@article{Behrens1992,
abstract = {An algorithm is described to discriminate automatically between saccades and slow eye movements. Sampled data of the eye position have been used to calculate the momentary acceleration of the eye. The higher acceleration values of the saccadic eye movements as opposed to the slow compensatory or pursuit eye movements served to differentiate between the two. The method is demonstrated by search-coil data in squirrel monkeys.},
author = {Behrens, F and Weiss, LR},
doi = {10.1016/0042-6989(92)90031-D},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Vision Research/1992/Behrens, Weiss/Behrens, Weiss - 1992 - An algorithm separating saccadic from nonsaccadic eye movements automatically by use of the acceleration sign(2).pdf:pdf},
issn = {00426989},
journal = {Vision Research},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = may,
number = {5},
pages = {889--893},
title = {{An algorithm separating saccadic from nonsaccadic eye movements automatically by use of the acceleration signal}},
url = {http://www.sciencedirect.com/science/article/pii/004269899290031D http://linkinghub.elsevier.com/retrieve/pii/004269899290031D},
volume = {32},
year = {1992}
}
@phdthesis{Criminisi2001,
abstract = {The work described in this thesis develops the theory of computing world measurements from photographs of scenes and reconstructing three-dimensional models of the scene. The main tool used is Projective Geometry which forms the basis for accurate estimation algorithms. The techniques presented employ uncalibrated images; no knowledge of the camera internal parameters (such as focal length and aspect ratio) or its pose (position and orientation) is required at any time. Extensive use is made of geometric characteristics of the scene, thus there is no need for specialized calibration devices. A hierarchy of novel, accurate and flexible techniques is developed to address a number of different situations ranging from where no scene metric information is known to cases where some distances are known but there is not sufficient information for a complete camera calibration. The geometry of single views is explored and monocular vision shown to be sufficient to ob- tain a partial or complete three-dimensional reconstruction of a scene. To achieve this the properties of planar homographies and planar homologies are extensively exploited. The geometry of multiple views is also investigated, particularly the use of a parallax-based approach for structure and camera recovery. The duality between two-view and three-view configurations is described in detail. Measured distances must be associated with a measurement accuracy to be meaningful. Therefore, an uncertainty propagation analysis is developed in order to take account of the pos- sible sources of error and how they affect the uncertainty in the final measurements. The general techniques developed in this thesis can be applied to several areas. Examples are presented of commercial, industrial and artistic use.},
author = {Criminisi, A},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2001/Criminisi/Criminisi - 2001 - Accurate visual metrology from single and multiple uncalibrated images.pdf:pdf},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
school = {University of Oxford},
title = {{Accurate visual metrology from single and multiple uncalibrated images}},
url = {http://books.google.com/books?hl=pt-BR\&lr=\&id=SwdGgnFb\_bgC\&oi=fnd\&pg=PA1\&ots=kAIFNP\_Lq2\&sig=myeicupQg3e\_3oLGtPAZ79t9PGA},
year = {2001}
}
@inproceedings{Lee2008,
address = {New York, New York, USA},
author = {Lee, Sangwon and Feng, David and Gooch, Bruce},
booktitle = {Proceedings of the 2008 symposium on Interactive 3D graphics and games - SI3D '08},
doi = {10.1145/1342250.1342269},
isbn = {9781595939838},
keywords = {perception,reconstruction,single-view reconstruction},
mendeley-tags = {reconstruction},
month = feb,
pages = {123},
publisher = {ACM Press},
title = {{Automatic construction of 3D models from architectural line drawings}},
url = {http://dl.acm.org/citation.cfm?id=1342250.1342269},
year = {2008}
}
@inproceedings{Lee2009b,
abstract = {We study the problem of generating plausible interpretations of a scene from a collection of line segments automatically extracted from a single indoor image. We show that we can recognize the three dimensional structure of the interior of a building, even in the presence of occluding objects. Several physically valid structure hypotheses are proposed by geometric reasoning and verified to find the best fitting model to line segments, which is then converted to a full 3D model. Our experiments demonstrate that our structure recovery from line segments is comparable with methods using full image appearance. Our approach shows how a set of rules describing geometric constraints between groups of segments can be used to prune scene interpretation hypotheses and to generate the most plausible interpretation.},
address = {Miami, FL},
author = {Lee, DC and Hebert, M and Kanade, T},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206872},
isbn = {978-1-4244-3992-8},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
month = jun,
pages = {2136--2143},
publisher = {IEEE},
title = {{Geometric reasoning for single image structure recovery}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5206872 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206872},
year = {2009}
}
@book{Sanders2007,
author = {Sanders, William and Cumaranatunge, Chandima},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2007/Sanders, Cumaranatunge/Sanders, Cumaranatunge - 2007 - ActionScript 3.0 Design Patterns Object Oriented Programming Techniques (Adobe Developer Library).pdf:pdf},
isbn = {0596528469},
pages = {530},
publisher = {Adobe Developer Library},
title = {{ActionScript 3.0 Design Patterns: Object Oriented Programming Techniques (Adobe Developer Library)}},
url = {http://www.amazon.com/ActionScript-3-0-Design-Patterns-Programming/dp /0596528469},
year = {2007}
}
@article{Fares2013,
address = {New York, New York, USA},
author = {Fares, Ribel and Fang, Shaomin and Komogortsev, Oleg V},
doi = {10.1145/2470654.2466183},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Input,MAGIC,animated,eye,mouse,mult,tracking},
pages = {1387},
publisher = {ACM Press},
title = {{Can we beat the mouse with MAGIC?}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466183},
year = {2013}
}
@article{Peck2013,
address = {New York, New York, USA},
author = {Peck, Evan M M. and Yuksel, Beste F. and Ottley, Alvitta and Jacob, Robert J.K. and Chang, Remco},
doi = {10.1145/2470654.2470723},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {473},
publisher = {ACM Press},
title = {{Using fNIRS brain sensing to evaluate information visualization interfaces}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470723},
year = {2013}
}
@article{Angst2012,
abstract = {The radial undistortion model proposed by Fitzgibbon and the radial fundamental matrix were early steps to extend classical epipolar geometry to distorted cameras. Later minimal solvers have been proposed to find relative pose and radial distortion, given point correspondences between images. However, a big drawback of all these approaches is that they require the distortion center to be exactly known. In this paper we show how the distortion center can be absorbed into a new radial fundamental matrix. This new formulation is much more practical in reality as it allows also digital zoom, cropped images and camera-lens systems where the distortion center does not exactly coincide with the image center. In particular we start from the setting where only one of the two images contains radial distortion, analyze the structure of the particular radial fundamental matrix and show that the technique also generalizes to other linear multi-view relationships like trifocal tensor and homography. For the new radial fundamental matrix we propose different estimation algorithms from 9,10 and 11 points. We show how to extract the epipoles and prove the practical applicability on several epipolar geometry image pairs with strong distortion that - to the best of our knowledge - no other existing algorithm can handle properly.},
author = {Angst, Roland and Zach, Christopher and Branco, Pedro and Pollefeys, Marc},
doi = {10.1007/978-3-642-37447-0\_11},
journal = {Computer Vision - ACCV},
number = {2013},
pages = {136--149},
title = {{Unknown Radial Distortion Centers in Multiple View Geometry Problems}},
volume = {7727},
year = {2012}
}
@article{Hedegaard2013,
address = {New York, New York, USA},
author = {Hedegaard, Steffen and Simonsen, Jakob Grue},
doi = {10.1145/2470654.2481286},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2089},
publisher = {ACM Press},
title = {{Extracting usability and user experience information from online user reviews}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481286},
year = {2013}
}
@article{Sung2004a,
abstract = {Crowd simulation for virtual environments offers many challenges centered on the trade-offs between rich behavior, control and computational cost. In this paper we present a new approach to controlling the behavior of agents in a crowd. Our method is scalable in the sense that increasingly complex crowd behaviors can be created without a corresponding increase in the complexity of the agents. Our approach is also more authorable; users can dynamically specify which crowd behaviors happen in various parts of an environment. Finally, the character motion produced by our system is visually convincing. We achieve our aims with a situation-based control structure. Basic agents have very limited behaviors. As they enter new situations, additional, situation-specific behaviors are composed on the fly to enable agents to respond appropriately. The composition is done using a probabilistic mechanism. We demonstrate our system with three environments including a city street and a theater.},
author = {Sung, Mankyu and Gleicher, Michael and Chenney, Stephen},
doi = {10.1111/j.1467-8659.2004.00783.x},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer Graphics Forum/2004/Sung, Gleicher, Chenney/Sung, Gleicher, Chenney - 2004 - Scalable behaviors for crowd simulation.pdf:pdf},
issn = {0167-7055},
journal = {Computer Graphics Forum},
month = sep,
number = {3},
pages = {519--528},
title = {{Scalable behaviors for crowd simulation}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2004.00783.x/full http://doi.wiley.com/10.1111/j.1467-8659.2004.00783.x},
volume = {23},
year = {2004}
}
@inproceedings{Mostafa2013,
abstract = {Microseismic visualization systems present complex 3D data of small seismic events within oil reservoirs to allow experts to explore and interact with that data. Yet existing systems suffer several problems: 3D spatial navigation and orientation is difficult, and selecting 3D data is challenging due to the problems of occlusion and lack of depth perception. Our work mitigates these problems by applying both proxemic interactions and a spatial input device to simplify how experts navigate through the visualization, and a painting metaphor to simplify how they select that information.},
address = {New York, New York, USA},
author = {Mostafa, Ahmed E and Greenberg, Saul and {Vital Brazil}, Emilio and Sharlin, Ehud and Sousa, Mario C},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13},
doi = {10.1145/2468356.2468670},
isbn = {9781450319522},
pages = {1749--1754},
publisher = {ACM Press},
title = {{Interacting with microseismic visualizations}},
year = {2013}
}
@article{Rayner1979,
author = {Rayner, K},
journal = {Perception},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Eye guidance in reading: Fixation locations within words}},
url = {http://www.perceptionweb.com/perception/fulltext/p08/p080021.pdf},
year = {1979}
}
@article{Supan2006,
abstract = {This work presents an approach to render appropriate shadows with Image Based Lighting in Augmented Reality applications. To approximate the result of environment lighting and shadowing, the system uses a dome of shadow casting light sources. The color of each shadow is determined by the area of the environment behind the casting light source. As a result it is possible that changes in the lighting conditions immidiately affect the shadow casting of virtual objects on real objects.},
author = {Supan, P and Stuppacher, I and Haller, M},
file = {:home/acmt/Dropbox/Documentos/Mendeley/International Journal of Virtual Reality/2006/Supan, Stuppacher, Haller/Supan, Stuppacher, Haller - 2006 - Image Based Shadowing in Real-Time Augmented Reality.pdf:pdf},
journal = {International Journal of Virtual Reality},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
number = {3},
pages = {1--7},
title = {{Image Based Shadowing in Real-Time Augmented Reality.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.3451\&rep=rep1\&type=pdf},
volume = {5},
year = {2006}
}
@inproceedings{Perse2005,
abstract = {The paper deals with the problem of computer vision based multi-person motion tracking, which in many cases suffers from lack of discriminating features of observed persons. To solve this problem, a physics based model of human motion is proposed, which includes internal forces of the persons by the means of the Kalman filter, and the cylindrical envelopes, which produce collision avoiding forces when observed persons come to close proximity. We tested the proposed method on two sequences, one from squash match, and the other from the basketball play and found out that the number of tracker mistakes significantly decreased.},
author = {Perse, M. and Pers, J. and Kristan, M. and Kovacic, S. and Vuckovic, G.},
booktitle = {ISPA 2005. Proceedings of the 4th International Symposium on Image and Signal Processing and Analysis, 2005.},
doi = {10.1109/ISPA.2005.195432},
isbn = {953-184-089-X},
issn = {1845-5921},
keywords = {Application software,Biological system modeling,Collision avoidance,Computer vision,Games,Humans,Image segmentation,Kalman filter,Kalman filters,Physics,Testing,Tracking,basketball,collision avoidance algorithm,computer vision,human motion,image motion analysis,image sequences,multiperson motion tracking,physics-based modelling,tracking},
mendeley-tags = {basketball,tracking},
pages = {328--333},
publisher = {IEEE},
shorttitle = {Image and Signal Processing and Analysis, 2005. IS},
title = {{Physics-based modelling of human motion using Kalman filter and collision avoidance algorithm}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1521311},
year = {2005}
}
@inproceedings{Jung2013,
abstract = {Past research has demonstrated a link between perceptions of social capital and use of the popular social network site, Facebook. Williams' Internet Social Capital Scales, based on Putnam's formulation, tap into sub-dimensions of social capital that have not been broadly used yet may enlighten our understanding of the different ways in which connecting with others online can facilitate access to resources embedded within our social relationships. In this study, we segment Williams' Internet Social Capital Scales into various sub-dimensions using factor analysis and explicate the distinct facets of social capital through a lab experiment in which Facebook users (N=98) request a small favor from their Facebook network. We find that some sub-dimensions play a significant role in getting favors from Facebook friends while bonding and bridging social capital do not significantly predict responses to favor requests.},
address = {New York, New York, USA},
author = {Jung, Yumi and Gray, Rebecca and Lampe, Cliff and Ellison, Nicole},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470657},
isbn = {9781450318990},
pages = {11--20},
publisher = {ACM Press},
title = {{Favors from facebook friends: unpacking dimensions of social capital}},
url = {https://www.msu.edu/~nellison/JungGrayLampeEllison2013CHI.pdf http://dl.acm.org/citation.cfm?doid=2470654.2470657},
year = {2013}
}
@article{Debevec2008,
author = {Debevec, P},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM SIGGRAPH 2008 classes/2008/Debevec/Debevec - 2008 - Rendering synthetic objects into real scenes Bridging traditional and image-based graphics with global illumination and.pdf:pdf},
journal = {ACM SIGGRAPH 2008 classes},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
title = {{Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography}},
url = {http://dl.acm.org/citation.cfm?id=1401175},
year = {2008}
}
@inproceedings{Tohidi2006,
abstract = {We present a study comparing usability testing of a single interface versus three functionally equivalent but stylistically distinct designs. We found that when presented with a single design, users give significantly higher ratings and were more reluctant to criticize than when presented with the same design in a group of three. Our results imply that by presenting users with alternative design solutions, subjective ratings are less prone to inflation and give rise to more and stronger criticisms when appropriate. Contrary to our expectations, our results also suggest that usability testing by itself, even when multiple designs are presented, is not an effective vehicle for soliciting constructive suggestions about how to improve the design from end users. It is a means to identify problems, not provide solutions.},
address = {New York, New York, USA},
author = {Tohidi, Maryam and Buxton, William and Baecker, Ronald and Sellen, Abigail},
booktitle = {Proceedings of the SIGCHI conference on Human Factors in computing systems - CHI '06},
doi = {10.1145/1124772.1124960},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the SIGCHI conference on Human Factors in computing systems - CHI '06/2006/Tohidi et al/Tohidi et al. - 2006 - Getting the right design and the design right.pdf:pdf},
isbn = {1595933727},
pages = {1243--1252},
publisher = {ACM Press},
title = {{Getting the right design and the design right}},
url = {http://portal.acm.org/citation.cfm?doid=1124772.1124960},
year = {2006}
}
@article{Kujala2013,
address = {New York, New York, USA},
author = {Kujala, Sari and Miron-Shatz, Talya},
doi = {10.1145/2470654.2466135},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1061},
publisher = {ACM Press},
title = {{Emotions, experiences and usability in real-life mobile phone use}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466135},
year = {2013}
}
@inproceedings{Pilet2010,
abstract = {Tracking is a major issue of virtual and augmented real- ity applications. Single object tracking on monocular video streams is fairly well understood. However, when it comes to multiple objects, existing methods lack scalability and can recognize only a limited number of objects. Thanks to recent progress in feature matching, state-of-the-art image retrieval techniques can deal with millions of images. However, these methods do not focus on real-time video processing and can not track retrieved objects. In this paper,we present a method that combines the speed and accuracy of tracking with the scalability of image re- trieval. At the heart of our approach is a bi-layer clustering process that allows our system to index and retrieve objects based on tracks of features, thereby effectively summarizing the information available on multiple video frames. As a result, our system is able to track in real-time multiple objects, recognized with low delay from a database of more than 300 entries.},
author = {Pilet, Julien and Saito, Hideo},
booktitle = {2010 IEEE Virtual Reality Conference (VR)},
doi = {10.1109/VR.2010.5444811},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2010 IEEE Virtual Reality Conference (VR)/2010/Pilet, Saito/Pilet, Saito - 2010 - Virtually augmenting hundreds of real pictures An approach based on learning, retrieval, and tracking.pdf:pdf},
isbn = {978-1-4244-6237-7},
month = mar,
pages = {71--78},
publisher = {IEEE},
title = {{Virtually augmenting hundreds of real pictures: An approach based on learning, retrieval, and tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5444811},
year = {2010}
}
@article{Wachs2011,
author = {Wachs, Juan Pablo and K\"{o}lsch, Mathias and Stern, Helman and Edan, Yael},
doi = {10.1145/1897816.1897838},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Communications of the ACM/2011/Wachs et al/Wachs et al. - 2011 - Vision-based hand-gesture applications.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = feb,
number = {2},
pages = {60},
title = {{Vision-based hand-gesture applications}},
url = {http://portal.acm.org/citation.cfm?doid=1897816.1897838},
volume = {54},
year = {2011}
}
@article{Idrees2014,
abstract = {Methods designed for tracking in dense crowds typically employ prior knowledge to make this difficult problem tractable. In this paper, we show that it is possible to handle this problem, without any priors, by utilizing the visual and contextual information already available in such scenes. We propose a novel tracking method tailored to dense crowds which provides an alternative and complementary approach to methods that require modeling of crowd flow and, simultaneously, is less likely to fail in the case of dynamic crowd flows and anomalies by minimally relying on previous frames. Our method begins with the automatic identification of prominent individuals from the crowd that are easy to track. Then, we use Neighborhood Motion Concurrence to model the behavior of individuals in a dense crowd, this predicts the position of an individual based on the motion of its neighbors. When the individual moves with the crowd flow, we use Neighborhood Motion Concurrence to predict motion while leveraging five-frame instantaneous flow in case of dynamically changing flow and anomalies. All these aspects are then embedded in a framework which imposes hierarchy on the order in which positions of individuals are updated. Experiments on a number of sequences show that the proposed solution can track individuals in dense crowds without requiring any pre-processing, making it a suitable online tracking algorithm for dense crowds.},
author = {Idrees, Haroon and Warner, Nolan and Shah, Mubarak},
doi = {10.1016/j.imavis.2013.10.006},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Crowd analysis,Dense crowds,Hierarchical tracking,Neighborhood motion concurrence,Prominence,Tracking},
month = jan,
number = {1},
pages = {14--26},
publisher = {Elsevier B.V.},
title = {{Tracking in dense crowds using prominence and neighborhood motion concurrence}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885613001637},
volume = {32},
year = {2014}
}
@article{Zhao2005,
abstract = {We propose a novel system for tracking pedestrians in a wide and open area, such as a shopping mall and exhibition hall, using a number of single-row laser-range scanners (LD-A), which have a profiling rate of 10 Hz and a scanning angle of 270°. LD-As are set directly on the floor doing horizontal scanning at an elevation of about 20 cm above the ground, so that horizontal cross sections of the surroundings, containing moving feet of pedestrians as well as still objects, are obtained in a rectangular coordinate system of real dimension. The data of moving feet are extracted through background subtraction by the client computers that control each LD-A, and sent to a server computer, where they are spatially and temporally integrated into a global coordinate system. A simplified pedestrian's walking model based on the typical appearance of moving feet is defined and a tracking method utilizing Kalman filter is developed to track pedestrian's trajectories. The system is evaluated through both real experiment and computer simulation. A real experiment is conducted in an exhibition hall, where three LD-As are used covering an area of about 60×60 m2. Changes in visitors' flow during the whole exhibition day are analyzed, where in the peak hour, about 100 trajectories are extracted simultaneously. On the other hand, a computer simulation is conducted to quantitatively examine system performance with respect to different crowd density.},
author = {Zhao, H and Shibasaki, R},
doi = {10.1109/TSMCA.2005.843396},
issn = {1083-4427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
keywords = {trajectory extraction},
mendeley-tags = {trajectory extraction},
month = mar,
number = {2},
pages = {283--291},
title = {{A Novel System for Tracking Pedestrians Using Multiple Single-Row Laser-Range Scanners}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1396163 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1396163},
volume = {35},
year = {2005}
}
@inproceedings{Waycott2013,
abstract = {Older adults are normally characterized as consumers, rather than producers, of digital content. Current research concerning the design of technologies for older adults typically focuses on providing access to digital resources. Access is important, but is often insufficient, especially when establishing new social relationships. This paper investigates the nature and role of digital content that has been created by older adults, for the purpose of forging new relationships. We present a unique field study in which seven older adults (aged 71-92 years), who did not know each other, used a prototype iPad application (Enmesh) to create and share photographs and messages. The findings demonstrate that older adults, even those in the $\backslash$'1c"oldest old$\backslash$'1d" age group, embraced opportunities to express themselves creatively through digital content production. We show that self-expression and social engagement with peers can be realized when socio-technical systems are suitably designed to allow older adults to create and share their own digital content.},
address = {New York, New York, USA},
author = {Waycott, Jenny and Vetere, Frank and Pedell, Sonja and Kulik, Lars and Ozanne, Elizabeth and Gruner, Alan and Downs, John},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470662},
isbn = {9781450318990},
pages = {39--48},
publisher = {ACM Press},
title = {{Older adults as digital content producers}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470662},
year = {2013}
}
@inproceedings{Klingner2010,
abstract = {We propose a new way of analyzing pupil measurements made in conjunction with eye tracking: fixation-aligned pupillary response averaging, in which short windows of continuous pupil measurements are selected based on patterns in eye tracking data, temporally aligned, and averaged together. Such short pupil data epochs can be selected based on fixations on a particular spot or a scan path. The windows of pupil data thus selected are aligned by temporal translation and linear warping to place corresponding parts of the gaze patterns at corresponding times and then averaged together. This approach enables the measurement of quick changes in cognitive load during visual tasks, in which task components occur at unpredictable times but are identifiable via gaze data. We illustrate the method through example analyses of visual search and map reading. We conclude with a discussion of the scope and limitations of this new method.},
address = {New York, New York, USA},
annote = {- cited by: 8
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Klingner, Jeff},
booktitle = {Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10},
doi = {10.1145/1743666.1743732},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10/2010/Klingner/Klingner - 2010 - Fixation-aligned pupillary response averaging.pdf:pdf},
isbn = {9781605589947},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {275},
publisher = {ACM Press},
title = {{Fixation-aligned pupillary response averaging}},
url = {http://dl.acm.org/citation.cfm?id=1743732 http://portal.acm.org/citation.cfm?doid=1743666.1743732},
year = {2010}
}
@article{Hayhoe2005,
abstract = {The classic experiments of Yarbus over 50 years ago revealed that saccadic eye movements reflect cognitive processes. But it is only recently that three separate advances have greatly expanded our understanding of the intricate role of eye movements in cognitive function. The first is the demonstration of the pervasive role of the task in guiding where and when to fixate. The second has been the recognition of the role of internal reward in guiding eye and body movements, revealed especially in neurophysiological studies. The third important advance has been the theoretical developments in the fields of reinforcement learning and graphic simulation. All of these advances are proving crucial for understanding how behavioral programs control the selection of visual information.},
author = {Hayhoe, Mary and Ballard, Dana},
doi = {10.1016/j.tics.2005.02.009},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Trends in cognitive sciences/2005/Hayhoe, Ballard/Hayhoe, Ballard - 2005 - Eye movements in natural behavior.pdf:pdf},
issn = {1364-6613},
journal = {Trends in cognitive sciences},
keywords = {Ambulatory,Ambulatory: instrumentation,Ambulatory: methods,Behavior,Behavior: physiology,Cognition,Cognition: physiology,Eye Movements,Eye Movements: physiology,Humans,Monitoring,Ocular,Ocular: physiology,Vision},
month = apr,
number = {4},
pages = {188--94},
pmid = {15808501},
title = {{Eye movements in natural behavior.}},
url = {http://www.sciencedirect.com/science/article/pii/S1364661305000598 http://www.ncbi.nlm.nih.gov/pubmed/15808501},
volume = {9},
year = {2005}
}
@article{Bulling2008,
abstract = {Physical activity has emerged as a novel input modality for so-called active video games. Input devices such as music instruments, dance mats or the Wii accessories allow for novel ways of interaction and a more immersive gaming experience. In this work we describe how eye movements recognised from electrooculographic (EOG) signals can be used for gaming purposes in three different scenarios. In contrast to common video-based systems, EOG can be implemented as a wearable and light-weight system which allows for long-term use with unconstrained simultaneous physical activity. In a stationary computer game we show that eye gestures of varying complexity can be recognised online with equal performance to a state-of-the-art video-based system. For pervasive gaming scenarios, we show how eye movements can be recognised in the presence of signal artefacts caused by physical activity such as walking. Finally, we describe possible future context-aware games which exploit unconscious eye movements and show which possibilities this new input modality may open up.},
author = {Bulling, A and Roggen, D and Tr\"{o}ster, G},
doi = {10.1007/978-3-540-88322-7\_4},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Fun and Games/2008/Bulling, Roggen, Tr\"{o}ster/Bulling, Roggen, Tr\"{o}ster - 2008 - EyeMote–towards context-aware gaming using eye movements recorded from wearable electrooculography.pdf:pdf},
journal = {Fun and Games},
keywords = {eye tracking,video game},
mendeley-tags = {eye tracking,video game},
pages = {33--45},
title = {{EyeMote–towards context-aware gaming using eye movements recorded from wearable electrooculography}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-88322-7\_4},
volume = {5294},
year = {2008}
}
@inproceedings{Bulling2013,
abstract = {In this work we present EyeContext, a system to infer high-level contextual cues from human visual behaviour. We conducted a user study to record eye movements of four participants over a full day of their daily life, totalling 42.5 hours of eye movement data. Participants were asked to self-annotate four non-mutually exclusive cues: social (interacting with somebody vs. no interaction), cognitive (concentrated work vs. leisure), physical (physically active vs. not active), and spatial (inside vs. outside a building). We evaluate a proof-of-concept EyeContext system that combines encoding of eye movements into strings and a spectrum string kernel support vector machine (SVM) classifier. Our results demonstrate the large information content available in long-term human visual behaviour and opens up new venues for research on eye-based behavioural monitoring and life logging.},
address = {New York, New York, USA},
author = {Bulling, Andreas and Weichel, Christian and Gellersen, Hans},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470697},
isbn = {9781450318990},
keywords = {Context Recognition,Electrooculography (EOG),Eye Movement Analysis,Visual Behaviour},
pages = {305--308},
publisher = {ACM Press},
title = {{EyeContext: recognition of high-level contextual cues from human visual behaviour}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470697},
year = {2013}
}
@inproceedings{Sodhi2013,
abstract = {We present BeThere, a proof-of-concept system designed to explore 3D input for mobile collaborative interactions. With BeThere, we explore 3D gestures and spatial input which al- lowremote users to perform a variety of virtual interactions in a local user’s physical environment. Our system is completely self-contained and uses depth sensors to track the location of a user’s fingers, as well as to capture the 3D shape of objects in front of the sensor. We illustrate the unique capabilities of our system through a series of interactions that allow users to control and manipulate 3D virtual content. We also pro- vide qualitative feedback from a preliminary user study which confirmed that users can complete a shared collaborative task using our system.},
address = {New York, New York, USA},
author = {Sodhi, Rajinder S and Jones, Brett R and Forsyth, David and Bailey, Brian P and Maciocci, Giuliano},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470679},
isbn = {9781450318990},
pages = {179--188},
publisher = {ACM Press},
title = {{BeThere: 3D mobile collaboration with spatial input}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470679},
year = {2013}
}
@article{Sundareswaran2006,
author = {Sundareswaran, V. and Wang, K. and Chen, S. and Behringer, R. and McGee, J. and Tam, C. and Zahorik, P.},
doi = {10.1109/ISMAR.2003.1240728},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings/2006/Sundareswaran et al/Sundareswaran et al. - 2006 - 3D audio augmented reality implementation and experiments.pdf:pdf},
isbn = {0-7695-2006-5},
journal = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
pages = {296--297},
publisher = {IEEE Comput. Soc},
title = {{3D audio augmented reality: implementation and experiments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240728},
year = {2006}
}
@article{Narain2009,
abstract = {Large dense crowds show aggregate behavior with reduced individual freedom of movement. We present a novel, scalable approach for simulating such crowds, using a dual representation both as discrete agents and as a single continuous system. In the continuous setting, we introduce a novel variational constraint called unilateral incompressibility, to model the large-scale behavior of the crowd, and accelerate inter-agent collision avoidance in dense scenarios. This approach makes it possible to simulate very large, dense crowds composed of up to a hundred thousand agents at near-interactive rates on desktop computers.},
author = {Narain, Rahul and Golas, Abhinav and Curtis, Sean and Lin, Ming C.},
doi = {10.1145/1618452.1618468},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM Transactions on Graphics/2009/Narain et al/Narain et al. - 2009 - Aggregate dynamics for dense crowd simulation.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {crowd simulation},
mendeley-tags = {crowd simulation},
month = dec,
number = {5},
pages = {1},
title = {{Aggregate dynamics for dense crowd simulation}},
url = {http://dl.acm.org/citation.cfm?id=1618468 http://portal.acm.org/citation.cfm?doid=1618452.1618468},
volume = {28},
year = {2009}
}
@misc{Ulloa2010,
author = {Ulloa, Carlos and Grden, John and Knip, Tim and Zupko, Andy},
booktitle = {Papervision3D},
title = {{Papervision3D}},
url = {http://blog.papervision3d.org/},
urldate = {04/10/2011},
year = {2010}
}
@article{Kitamura2002,
address = {New York, New York, USA},
author = {Kitamura, Yoshifumi and Ogata, Susumu and Kishino, Fumio},
doi = {10.1145/585771.585774},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the ACM symposium on Virtual reality software and technology - VRST '02/2002/Kitamura, Ogata, Kishino/Kitamura, Ogata, Kishino - 2002 - A manipulation environment of virtual and real objects using a magnetic metaphor.pdf:pdf},
isbn = {1581135300},
journal = {Proceedings of the ACM symposium on Virtual reality software and technology - VRST '02},
pages = {201},
publisher = {ACM Press},
title = {{A manipulation environment of virtual and real objects using a magnetic metaphor}},
url = {http://portal.acm.org/citation.cfm?doid=585740.585774},
year = {2002}
}
@phdthesis{Raskar2002,
abstract = {Light projectors can be arranged into electronic displays that offer large, bright, and high resolution images. However, despite their unique characteristics, projectors have been treated like any other two-dimensional display devices, e.g. CRT monitors or LCD panels, to create flat and usually rectangular images. Even the growth of three dimensional computer graphics has followed this limitation. To improve and widen the range of applications of projectors, in this dissertation I present a single unified geometric framework for projector-based graphics. It is based on the notion of the projector as the dual of a camera. The geometric framework is based on (i) the analytical projection model, (ii) the geometric representation of the display surface and (iii) the viewer location. The framework can be used for practically all the projector-based applications. For classic projector- based systems, such as tiled displays or immersive panoramic displays, the framework provides a fundamentally different approach that allows greater freedom and flexibility. In addition, it enables a new class of projector-based visualization methods.},
author = {Raskar, R},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2002/Raskar/Raskar - 2002 - Projector-based three dimensional graphics.pdf:pdf},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
pages = {133},
school = {University of North Carolina},
title = {{Projector-based three dimensional graphics}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.9008\&rep=rep1\&type=pdf},
year = {2002}
}
@inproceedings{Feng2008,
abstract = {To realize a seamless augmented reality system, proper simulation of real lighting is an important factor. In this paper, two spheres with Lambert surface are introduced in the real scene. Through image analysis and analytical geometry of three dimensions, the illumination parameters of the real scene are estimated automatically. These parameters describe a point light source and ambient light. Using the estimated illumination parameters, the synthetic scene is relighted through inserting proper virtual light sources in the scene graph tree. This work explores an image-based and post-illumination-processing approach to rapidly improve illumination consistency in augmented reality. Experimental results with a virtual blue goblet mixed into the real scene show that the proposed system can make the illumination in both scenes are identical.},
address = {Sanya, China},
author = {Feng, Yan},
booktitle = {2008 Congress on Image and Signal Processing},
doi = {10.1109/CISP.2008.87},
isbn = {978-0-7695-3119-9},
pages = {771--775},
publisher = {IEEE},
title = {{Estimation of Light Source Environment for Illumination Consistency of Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4566587},
year = {2008}
}
@article{Abreu2011,
author = {Abreu, Priscilla F. De and Werneck, Vera Maria B. and Costa, Rosa Maria E. Moreira Da and Carvalho, Luis Alfredo V. De},
doi = {10.1109/SVR.2011.32},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 XIII Symposium on Virtual Reality/2011/Abreu et al/Abreu et al. - 2011 - Employing Multi-agents in 3-D Game for Cognitive Stimulation.pdf:pdf},
isbn = {978-1-4577-0661-5},
journal = {2011 XIII Symposium on Virtual Reality},
keywords = {an automated control,can be integrated into,cognitive stimulation,different applications and provide,games,mas,multi-agents systems,serious,the multi-agent systems,virtual reality},
month = may,
pages = {73--78},
publisher = {Ieee},
title = {{Employing Multi-agents in 3-D Game for Cognitive Stimulation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951837},
year = {2011}
}
@article{Cui2013,
address = {New York, New York, USA},
author = {Cui, Yanqing and Kangas, Jari and Holm, Jukka and Grassel, Guido},
doi = {10.1145/2470654.2466125},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {981},
publisher = {ACM Press},
title = {{Front-camera video recordings as emotion responses to mobile photos shared within close-knit groups}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466125},
year = {2013}
}
@article{Camilli2008,
abstract = {In human factors and ergonomics research, the analysis of eye movements has gained popularity as a method for obtaining information concerning the operators cognitive strategies and for drawing inferences about the cognitive state of an individual. For example, recent studies have shown that the distribution of eye fixations is sensitive to variations in mental workload\&\#x2014;dispersed when workload is high, and clustered when workload is low. Spatial statistics algorithms can be used to obtain information about the type of distribution and can be applied over fixations recorded during small epochs of time to assess online changes in the level of mental load experienced by the individuals. In order to ease the computation of the statistical index and to encourage research on the spatial properties of visual scanning, A Simple Tool for Examining Fixations has been developed. The software application implements functions for fixation visualization, management, and analysis, and includes a tool for fixation identification from raw gaze point data. Updated information can be obtained online at www .astef.info, where the installation package is freely downloadable.},
annote = {- cited by: 25
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Camilli, Marco and Nacchia, Roberto and Terenzi, Michela and Nocera, Francesco},
doi = {10.3758/BRM.40.2.373},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Behavior research methods/2008/Camilli et al/Camilli et al. - 2008 - ASTEF A simple tool for examining fixations.pdf:pdf},
issn = {1554-351X},
journal = {Behavior research methods},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = may,
number = {2},
pages = {373--382},
title = {{ASTEF: A simple tool for examining fixations}},
url = {http://link.springer.com/article/10.3758/BRM.40.2.373 http://www.springerlink.com/index/10.3758/BRM.40.2.373},
volume = {40},
year = {2008}
}
@inproceedings{Carneiro2013,
abstract = {In this work, we propose a computer model for simulating and studying crowd evacuation behavior, and apply it to study the evacuation of a soccer stadium. Our model uses 2D cellular automata defined over multiple grids that represent different levels of the simulated environment. A pedestrian moves from a given grid cell to another by analyzing multiple movement options and resolving conflicts with other pedestrians. Validation tests were performed, demonstrating that the proposed model is able to simulate crowd evacuation successfully.},
author = {Carneiro, Lilian de Oliveira and Cavalcante-Neto, Joaquim Bento and Vidal, Creto Augusto and Dutra, Teofilo Bezerra},
booktitle = {2013 XV Symposium on Virtual and Augmented Reality},
doi = {10.1109/SVR.2013.29},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2013 XV Symposium on Virtual and Augmented Reality/2013/Carneiro et al/Carneiro et al. - 2013 - Crowd Evacuation Using Cellular Automata Simulation in a Soccer Stadium.pdf:pdf},
isbn = {978-0-7695-5001-5},
month = may,
pages = {240--243},
publisher = {IEEE},
title = {{Crowd Evacuation Using Cellular Automata: Simulation in a Soccer Stadium}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6655789 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6655789},
year = {2013}
}
@inproceedings{Wang2013a,
abstract = {Although both men and women communicate frequently on Facebook, we know little about what they talk about, whether their topics differ and how their network responds. Using Latent Dirichlet Allocation (LDA), we identify topics from more than half a million Facebook status updates and determine which topics are more likely to receive feedback, such as likes and comments. Women tend to share more personal topics (e.g., family matters), while men discuss more public ones (e.g., politics and sports). Generally, women receive more feedback than men, but "male" topics (those more often posted by men) receive more feedback, especially when posted by women.},
address = {New York, New York, USA},
author = {Wang, Yi-chia and Burke, Moira and Kraut, Robert E.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470659},
isbn = {9781450318990},
pages = {31--34},
publisher = {ACM Press},
title = {{Gender, topic, and audience response: an analysis of user-generated content on facebook}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470659},
year = {2013}
}
@article{Trappenberg2001,
abstract = {Significant advances in cognitive neuroscience can be achieved by combining techniques used to measure behavior and brain activity with neural modeling. Here we apply this approach to the initiation of rapid eye movements (saccades), which are used to redirect the visual axis to targets of interest. It is well known that the superior colliculus (SC) in the midbrain plays a major role in generating saccadic eye movements, and physiological studies have provided important knowledge of the activity pattern of neurons in this structure. Based on the observation that the SC receives localized sensory (exogenous) and voluntary (endogenous) inputs, our model assumes that this information is integrated by dynamic competition across local collicular interactions. The model accounts well for the effects upon saccadic reaction time (SRT) due to removal of fixation, the presence of distractors, execution of pro-versus antisaccades, and variation in target probability, and suggests a possible mechanism for the generation of express saccades. In each of these cases, the activity patterns of “neurons” within the model closely resemble actual cell behavior in the intermediate layer of the SC. The interaction structure we employ is instrumental for producing a physiologically faithful model and results in new insights and hypotheses regarding the neural mechanisms underlying saccade initiation.},
author = {Trappenberg, Thomas P. and Dorris, Michael C. and Munoz, Douglas P. and Klein, Raymond M.},
doi = {10.1162/089892901564306},
file = {::},
issn = {0898-929X},
journal = {Journal of Cognitive Neuroscience},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
month = feb,
number = {2},
pages = {256--271},
title = {{A Model of Saccade Initiation Based on the Competitive Integration of Exogenous and Endogenous Signals in the Superior Colliculus}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089892901564306},
volume = {13},
year = {2001}
}
@article{Ince2009,
abstract = {The systems let user track their eye gaze information have been technologically possible for several decades. However, they are still very expensive. They have limited use of eye tracking and blink detection infra-structure. The purpose of this paper is to evaluate cost effects in the sector and explain our new approach in detail which reduces high costs of current systems apparently. This paper introduces an algorithm for fast and sub-pixel precise detection of eye blobs for extracting eye features. The algorithm is based on differential geometry and still exists in OpenCpV library as a class. Hence, blobs of arbitrary size that means eye size can be extracted by just adjusting the scale parameter in the class function. In addition, center point and boundary of an eye blob, also are extracted. These describe the specific eye location in the face boundary to run several algorithms to find the eye-ball location with its central coordinates. Several examples on real simple web-cam images illustrate the performance of the proposed algorithm and yield an efficient result on the idea of low-cost eye tracking, blink detection and drowsiness detection system.},
author = {Ince, IF and Yang, TC},
doi = {10.1007/978-3-642-04070-2\_58},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Emerging Intelligent Computing Technology and Applications/2009/Ince, Yang/Ince, Yang - 2009 - A new low-cost eye tracking and blink detection approach extracting eye features with blob extraction.pdf:pdf},
journal = {Emerging Intelligent Computing Technology and Applications},
keywords = {blink,gaze analysis},
mendeley-tags = {blink,gaze analysis},
pages = {526--533},
title = {{A new low-cost eye tracking and blink detection approach: extracting eye features with blob extraction}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-04070-2\_58},
volume = {5754},
year = {2009}
}
@article{Metoyer2004,
abstract = {Architectural and urban planning applications require animations of people to present an accurate and compelling view of a new environment. Ideally, these animations would be easy for a non-programmer to construct, just as buildings and streets can be modeled by an architect or artist using commercial modeling software. In this paper, we explore an approach for generating reactive path following based on the user’s examples of the desired behavior. The examples are used to build a model of the desired reactive behavior. The model is combined with reactive control methods to produce natural 2D pedestrian trajectories. The system then automatically generates 3D pedestrian locomotion using a motion-graph approach. We discuss the accuracy of the learned model of pedestrian motion and show that simple direction primitives can be recorded and used to build natural, reactive, path-following behaviors.},
author = {Metoyer, Ronald A and Hodgins, Jessica K},
doi = {10.1007/s00371-004-0265-z},
issn = {0178-2789},
journal = {The Visual Computer},
month = nov,
number = {10},
pages = {635--649},
title = {{Reactive pedestrian path following from examples}},
url = {http://link.springer.com/10.1007/s00371-004-0265-z},
volume = {20},
year = {2004}
}
@inproceedings{Furmanski2002,
abstract = {One unique feature of mixed and augmented reality (MR/AR) systems is that hidden and occluded objects an be readily visualized. We call this specialized use of MR/AR, obscured information visualization (OIV). In this paper, we describe the beginning of a research program designed to develop such visualizations through the use of principles derived from perceptual psychology and cognitive science. In this paper we surveyed the cognitive science literature as it applies to such visualization tasks, described experimental questions derived from these cognitive principles, and generated general guidelines that can be used in designing future OIV systems (as well improving AR displays more generally). We also report the results from an experiment that utilized a functioning AR-OIV system: we found that in relative depth judgment, subjects reported rendered objects as being in front of real-world objects, except when additional occlusion and motion cues were presented together.},
address = {Darmstadt},
author = {Furmanski, C and Azuma, R and Daily, M},
booktitle = {Proceedings. International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2002.1115091},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings. International Symposium on Mixed and Augmented Reality/2002/Furmanski, Azuma, Daily/Furmanski, Azuma, Daily - 2002 - Augmented-reality visualizations guided by cognition perceptual heuristics for combining visible and ob.pdf:pdf},
isbn = {0-7695-1781-1},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {215--320},
publisher = {IEEE Comput. Soc},
title = {{Augmented-reality visualizations guided by cognition: perceptual heuristics for combining visible and obscured information}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1115091 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1115091},
year = {2002}
}
@article{Tausczik2013,
address = {New York, New York, USA},
author = {Tausczik, Yla R. and Pennebaker, James W.},
doi = {10.1145/2470654.2470720},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {459},
publisher = {ACM Press},
title = {{Improving teamwork using real-time language feedback}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470720},
year = {2013}
}
@inproceedings{inktuitive,
author = {Mistry, Pranav and Sekiya, Kayato and Bradshaw, Andrea},
booktitle = {4th International Conference on Intelligent Environments (IE 08)},
doi = {10.1049/cp:20081089},
isbn = {978 0 86341 894 5},
keywords = {SBGames},
mendeley-tags = {SBGames},
pages = {P11--P11},
publisher = {IET},
title = {{Inktuitive: an intuitive physical design workspace}},
url = {http://digital-library.theiet.org/content/conferences/10.1049/cp\_20081089},
year = {2008}
}
@article{Oyekoya2006,
abstract = {Eye-tracking technology offers a natural and immediate way of communicating human intentions to a computer. Eye movements reflect interests and may be analysed to drive computer functionality in games, image and video search, and other visual tasks. This paper examines current eye tracking technologies and their applications. Experiments are described that show that target images can be identified more rapidly by eye tracking than using a mouse interface. Further results show that an eye-tracking technology provides an efficient interface for locating images in a large database. Finally the paper speculates about how the technology may enter the mass market as costs decrease.},
author = {Oyekoya, O. K. and Stentiford, F. W. M.},
doi = {10.1007/s10550-006-0076-z},
file = {:home/acmt/Dropbox/Documentos/Mendeley/BT Technology Journal/2006/Oyekoya, Stentiford/Oyekoya, Stentiford - 2006 - Eye tracking — A new interface for visual exploration.pdf:pdf},
issn = {1358-3948},
journal = {BT Technology Journal},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = jul,
number = {3},
pages = {57--66},
title = {{Eye tracking — A new interface for visual exploration}},
url = {http://link.springer.com/article/10.1007/s10550-006-0076-z http://link.springer.com/10.1007/s10550-006-0076-z},
volume = {24},
year = {2006}
}
@article{Guy2012,
abstract = {We present an information-theoretic method to measure the similarity between a given set of observed, real-world data and visual simulation technique for aggregate crowd motions of a complex system consisting of many individual agents. This metric uses a two-step process to quantify a simulator's ability to reproduce the collective behaviors of the whole system, as observed in the recorded real-world data. First, Bayesian inference is used to estimate the simulation states which best correspond to the observed data, then a maximum likelihood estimator is used to approximate the prediction errors. This process is iterated using the EM-algorithm to produce a robust, statistical estimate of the magnitude of the prediction error as measured by its entropy (smaller is better). This metric serves as a simulator-to-data similarity measurement. We evaluated the metric in terms of robustness to sensor noise, consistency across different datasets and simulation methods, and correlation to perceptual metrics.},
author = {Guy, Stephen J. and van den Berg, Jur and Liu, Wenxi and Lau, Rynson and Lin, Ming C. and Manocha, Dinesh},
doi = {10.1145/2366145.2366209},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {crowd simulation,data-driven simulations,validation},
month = nov,
number = {6},
pages = {1},
title = {{A statistical similarity measure for aggregate crowd dynamics}},
url = {http://dl.acm.org/citation.cfm?doid=2366145.2366209},
volume = {31},
year = {2012}
}
@article{Tuch2013,
address = {New York, New York, USA},
author = {Tuch, Alexandre N. and Trusell, Rune and Hornb\ae k, Kasper},
doi = {10.1145/2470654.2481285},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2079},
publisher = {ACM Press},
title = {{Analyzing users' narratives to understand experience with interactive products}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481285},
year = {2013}
}
@inproceedings{Shibo2012,
abstract = {Kinect, as a new type of human-machine interaction sensor, is able to capture range images and color images of the scene simultaneously, but there is space parallax between them. A stereo calibration is expected to overcome it. Much different from the previous approaches, we introduced a new method in this paper to solve this problem which uses the range image directly. The method obtains the range calibration images of a newly designed calibration board and extracts calibration point pairs by an algorithm developed in this paper. Our experiments show that the method can complete calibration of the Kinect successfully.},
author = {Shibo, Li and Qing, Zhuo},
booktitle = {2012 4th International Conference on Intelligent Human-Machine Systems and Cybernetics},
doi = {10.1109/IHMSC.2012.156},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 4th International Conference on Intelligent Human-Machine Systems and Cybernetics/2012/Shibo, Qing/Shibo, Qing - 2012 - A New Approach to Calibrate Range Image and Color Image from Kinect.pdf:pdf},
isbn = {978-1-4673-1902-7},
month = aug,
pages = {252--255},
publisher = {IEEE},
title = {{A New Approach to Calibrate Range Image and Color Image from Kinect}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6305770\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=kinect+calibration},
year = {2012}
}
@inproceedings{He2006,
abstract = {The idea of event detection is to identify interesting patterns from a constant stream of incoming news documents. Previous research in event detection has largely focused on identifying the first event or tracking subsequent events belonging to a set of pre-assigned topics such as earthquakes, airline disasters, etc. In this paper, we propose a new problem, called Anticipatory Event Detection (AED), which aims to detect if a user-specified event has transpired. AED can be viewed as a personalized combination of event tracking and new event detection. We propose using a sentence classification approach to solve the AED problem for a restricted domain; detecting articles that describe final game results from NBA basketball news. Experimental results demonstrate the feasibility of our proposed AED solution.},
author = {He, Qi and Chang, Kuiyu and Lim, Ee-Peng},
booktitle = {2006 IEEE International Conference on Systems, Man and Cybernetics},
doi = {10.1109/ICSMC.2006.384554},
isbn = {1-4244-0099-6},
keywords = {Cybernetics,Earthquakes,Event detection,Game theory,Ground penetrating radar,Helium,Information systems,Internet,Mobile handsets,Real time systems,anticipatory event detection,basketball,classification,electronic publishing,information retrieval,news document,sentence classification,text analysis,tracking},
mendeley-tags = {basketball,tracking},
month = oct,
pages = {1143--1148},
publisher = {IEEE},
shorttitle = {Systems, Man and Cybernetics, 2006. SMC '06. IEEE},
title = {{Anticipatory Event Detection via Sentence Classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4274002},
volume = {2},
year = {2006}
}
@inproceedings{Harpstead2013,
abstract = {The field of Educational Games has seen many calls for added rigor. One avenue for improving the rigor of the field is developing more generalizable methods for measuring student learning within games. Throughout the process of development, what is relevant to measure and assess may change as a game evolves into a finished product. The field needs an approach for game developers and researchers to be able to prototype and experiment with different measures that can stand up to rigorous scrutiny, as well as provide insight into possible new directions for development. We demonstrate a toolkit and analysis tools that capture and analyze students' performance within open educational games. The system records relevant events during play, which can be used for analysis of player learning by designers. The tools support replaying student sessions within the original game's environment, which allows researchers and developers to explore possible explanations for student behavior. Using this system, we were able to facilitate a number of analyses of student learning in an open educational game developed by a team of our collaborators as well as gain greater insight into student learning with the game and where to focus as we iterate.},
address = {New York, New York, USA},
author = {Harpstead, Erik and Myers, Brad A and Aleven, Vincent},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470667},
isbn = {9781450318990},
pages = {79--88},
publisher = {ACM Press},
title = {{In search of learning: facilitating data analysis in educational games}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470667},
year = {2013}
}
@article{Junior2012,
author = {Junior, Antonio Jose Melo Leite and Gomes, George Allan Menezes and Junior, Natal Anacleto Chicca and Santos, Alysson Diniz Dos and Vidal, Creto Augusto and Cavalcante-Neto, Joaquim Bento and Gattass, Marcelo},
doi = {10.1109/SVR.2012.12},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Junior et al/Junior et al. - 2012 - System Model for Shooting Training Based on Interactive Video, Three-Dimensional Computer Graphics and Laser Ray.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {dimensional computer graphics,shooting training},
month = may,
pages = {254--260},
publisher = {Ieee},
title = {{System Model for Shooting Training Based on Interactive Video, Three-Dimensional Computer Graphics and Laser Ray Capture}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297537},
year = {2012}
}
@article{Tsang2010,
abstract = {We introduce eSeeTrack, an eye-tracking visualization prototype that facilitates exploration and comparison of sequential gaze orderings in a static or a dynamic scene. It extends current eye-tracking data visualizations by extracting patterns of sequential gaze orderings, displaying these patterns in a way that does not depend on the number of fixations on a scene, and enabling users to compare patterns from two or more sets of eye-gaze data. Extracting such patterns was very difficult with previous visualization techniques. eSeeTrack combines a timeline and a tree-structured visual representation to embody three aspects of eye-tracking data that users are interested in: duration, frequency and orderings of fixations. We demonstrate the usefulness of eSeeTrack via two case studies on surgical simulation and retail store chain data. We found that eSeeTrack allows ordering of fixations to be rapidly queried, explored and compared. Furthermore, our tool provides an effective and efficient mechanism to determine pattern outliers. This approach can be effective for behavior analysis in a variety of domains that are described at the end of this paper.},
author = {Tsang, Hoi Ying and Tory, Melanie and Swindells, Colin},
doi = {10.1109/TVCG.2010.149},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE transactions on visualization and computer graphics/2010/Tsang, Tory, Swindells/Tsang, Tory, Swindells - 2010 - eSeeTrack--visualizing sequential fixation patterns.pdf:pdf},
issn = {1077-2626},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Computer Graphics,Computer Simulation,Computer-Assisted Instruction,Eye Movement Measurements,Eye Movement Measurements: statistics \& numerical,Fixation,Humans,Laparoscopy,Laparoscopy: education,Ocular,eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
number = {6},
pages = {953--62},
pmid = {20975132},
title = {{eSeeTrack--visualizing sequential fixation patterns.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5613432 http://www.ncbi.nlm.nih.gov/pubmed/20975132},
volume = {16},
year = {2010}
}
@article{Hirooka2004,
abstract = {In this paper, we propose a virtual display system onto a real object surface using a video projector, so that the viewer can feel as if digital images are printed on the real surface with arbitrary shape. We apply a homography between the source digital image plane and the projector image plane to render the images on the object surface. For adapting not only the single planar but also the arbitrary shape of the surface, we divide the object surface into many small rectangular regions, and generate warped image of each region by calculating this homography of the plane of each divided region. By projecting the warped image on the real surface, the image can be observed as if the image is printed on it. In this system, the rendered image can be updated easily if the object surface moves and changes its shape, or refined when it is standing still. Our system consists of the uncalibrated camera and video projector connected to PC, and has the advantage of needing neither a prior setup nor other expensive equipments.},
author = {Hirooka, S and Saito, H},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 14th International Conference on Artificial Reality and Telexistence/2004/Hirooka, Saito/Hirooka, Saito - 2004 - Virtual display system using video projector onto real object surface.pdf:pdf},
journal = {Proceedings of the 14th International Conference on Artificial Reality and Telexistence},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
title = {{Virtual display system using video projector onto real object surface}},
url = {http://hvrl.ics.keio.ac.jp/paper/pdf/international\_Conference/2004/ICAT2004\_shinichiro.pdf},
year = {2004}
}
@inproceedings{Foster2013,
abstract = {We investigate the unique educational benefits of 1-on-1 competitive games, arguing that such games can be just as easy to design as single-player educational games, while yielding a more diverse and sustainable learning experience. We present a study of chess and StarCraft II in order to inform the design of similar educational games and their communities. We discuss a competitive game we designed to teach Java programming. We evaluate the game by discussing its user study. Our main contributions are 1) an argument that the use of 1-on-1 competition can solve two existing problems inherent to single-player games, 2) an analysis of the features that make competitive games effective learning environments, and 3) an early but encouraging description of the emergent learning environment one can expect from designing an educational game with these features.},
address = {New York, New York, USA},
author = {Foster, Stephen R and Esper, Sarah and Griswold, William G.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470669},
isbn = {9781450318990},
pages = {99--108},
publisher = {ACM Press},
title = {{From competition to metacognition: designing diverse, sustainable educational games}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470669},
year = {2013}
}
@inproceedings{Lee2004a,
abstract = {Traditional Tangible Augmented Reality (Tangible AR) interfaces combine a mixture of tangible user interface and augmented reality technology, complementing each other for novel interaction methods and real world anchored visualization. However, well known conventional one and two dimensional interaction methods such as pressing buttons, changing slider values, or menu selections are often quite difficult to apply to Tangible AR interfaces. In this paper we suggest a new approach, occlusion based interaction, in which visual occlusion of physical markers are used to provide intuitive two dimensional interaction in Tangible AR environments. We describe how to implement occlusion based interfaces for Tangible AR environments, give several examples of applications and describe results from informal user studies.},
address = {New York, New York, USA},
author = {Lee, Gun A. and Billinghurst, Mark and Kim, Gerard Jounghyun},
booktitle = {Proceedings of the 2004 ACM SIGGRAPH international conference on Virtual Reality continuum and its applications in industry - VRCAI '04},
doi = {10.1145/1044588.1044680},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2004 ACM SIGGRAPH international conference on Virtual Reality continuum and its applications in industry - VRCAI '04/2004/Lee, Billinghurst, Kim/Lee, Billinghurst, Kim - 2004 - Occlusion based interaction methods for tangible augmented reality environments.pdf:pdf},
isbn = {1581138849},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {419},
publisher = {ACM Press},
title = {{Occlusion based interaction methods for tangible augmented reality environments}},
url = {http://dl.acm.org/citation.cfm?id=1044680 http://portal.acm.org/citation.cfm?doid=1044588.1044680},
year = {2004}
}
@article{Alper2013,
address = {New York, New York, USA},
author = {Alper, Basak and Bach, Benjamin and {Henry Riche}, Nathalie and Isenberg, Tobias and Fekete, Jean-Daniel},
doi = {10.1145/2470654.2470724},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Graph comparison,brain connectivity analysis,brain connectivity visualization.},
pages = {483},
publisher = {ACM Press},
title = {{Weighted graph comparison techniques for brain connectivity analysis}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470724},
year = {2013}
}
@incollection{Moreira,
address = {Bauru-SP},
author = {Moreira, Glaudiney and Junior, Mendon\c{c}a and Vidal, Creto Augusto and Jos\'{e}, Antonio and Leite, Melo and Allan, George and Gomes, Menezes},
booktitle = {Intera\c{c}\~{a}o em Realidade Virtual e Aumentada},
chapter = {3},
edition = {1},
editor = {Brega, Jos\'{e} Remo Ferreira and Kelner, Judith},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Intera\c{c}\~{a}o em Realidade Virtual e Aumentada/2010/Moreira et al/Moreira et al. - 2010 - Intera\c{c}\~{a}o e Comportamento de Entidades em Ambientes Virtuais.pdf:pdf},
pages = {35--53},
publisher = {Canal6},
title = {{Intera\c{c}\~{a}o e Comportamento de Entidades em Ambientes Virtuais}},
year = {2010}
}
@inproceedings{Eshel2008,
abstract = {Tracking people in a dense crowd is a challenging problem for a single camera tracker due to occlusions and extensive motion that make human segmentation difficult. In this paper we suggest a method for simultaneously tracking all the people in a densely crowded scene using a set of cameras with overlapping fields of view. To overcome occlusions, the cameras are placed at a high elevation and only peoplepsilas heads are tracked. Head detection is still difficult since each foreground region may consist of multiple subjects. By combining data from several views, height information is extracted and used for head segmentation. The head tops, which are regarded as 2D patches at various heights, are detected by applying intensity correlation to aligned frames from the different cameras. The detected head tops are then tracked using common assumptions on motion direction and velocity. The method was tested on sequences in indoor and outdoor environments under challenging illumination conditions. It was successful in tracking up to 21 people walking in a small area (2.5 people per m2), in spite of severe and persistent occlusions.},
address = {Anchorage, AK},
author = {Eshel, Ran and Moses, Yael},
booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2008.4587539},
isbn = {978-1-4244-2242-5},
month = jun,
pages = {1--8},
publisher = {IEEE},
title = {{Homography based multiple camera detection and tracking of people in a dense crowd}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4587539},
year = {2008}
}
@article{Diaz2013a,
abstract = {Despite the growing popularity of virtual reality environments, few laboratories are equipped to investigate eye movements within these environments. This primer is intended to reduce the time and effort required to incorporate eye-tracking equipment into a virtual reality environment. We discuss issues related to the initial startup and provide algorithms necessary for basic analysis. Algorithms are provided for the calculation of gaze angle within a virtual world using a monocular eye-tracker in a three-dimensional environment. In addition, we provide algorithms for the calculation of the angular distance between the gaze and a relevant virtual object and for the identification of fixations, saccades, and pursuit eye movements. Finally, we provide tools that temporally synchronize gaze data and the visual stimulus and enable real-time assembly of a video-based record of the experiment using the Quicktime MOV format, available at http://sourceforge.net/p/utdvrlibraries/. This record contains the visual stimulus, the gaze cursor, and associated numerical data and can be used for data exportation, visual inspection, and validation of calculated gaze movements.},
annote = {- cited by: 1
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Diaz, Gabriel and Cooper, Joseph and Kit, Dmitry and Hayhoe, Mary},
doi = {10.1167/13.12.5},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of vision/2013/Diaz et al/Diaz et al. - 2013 - Real-time recording and classification of eye movements in an immersive virtual environment.pdf:pdf},
issn = {1534-7362},
journal = {Journal of vision},
keywords = {eye movements,gaze,gaze analysis,methods,virtual reality},
mendeley-tags = {gaze analysis},
month = jan,
number = {12},
pages = {1--14},
pmid = {24113087},
title = {{Real-time recording and classification of eye movements in an immersive virtual environment.}},
url = {http://w.journalofvision.org/content/13/12/5.short},
volume = {13},
year = {2013}
}
@techreport{Fournier1994,
abstract = {The ability to merge a real video image (RVI) with a computer-generated image (CGI) enhances the usefulness of both. To go beyond "cut and paste" and chroma-keying, and merge the two images successfully, one must solve the problems of common viewing parameters. common visibility and common illumination. The result can be dubbed Computer Augmented Reality (CAR). The solution needs contributions from both computer graphics and computer vision. The problems of common illumination are especially challenging, because they test our understanding and practice of shadow and global illumination computation. In this paper we will describe and illustrate work in our laboratory where the emphasis is on extracting illumination information from real images and computing the common illumination between the real and the computer generated scene.},
author = {Fournier, A},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/1994/Fournier/Fournier - 1994 - Illumination problems in computer augmented reality.pdf:pdf},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
title = {{Illumination problems in computer augmented reality}},
url = {http://dl.acm.org/citation.cfm?id=902024},
year = {1994}
}
@article{Nilsson2009a,
abstract = {This paper presents results from a study on using an AR application to support collaborative command and control activities requiring the collaboration of three different civil service organisations. The technology is used to create a common ground between the organisations and allows the users to interact, plan resources and react to the ongoing events on a digital map. The AR application was developed and evaluated in a study where a forest fire scenario was simulated. Participants from the involved organisations acted as command and control teams in the simulated scenario and both quantitative and qualitative results were obtained. The results show that AR can become a useful tool in these situations in the future.},
author = {Nilsson, Susanna and Johansson, Bj\"{o}rn J E and J\"{o}nsson, Arne},
doi = {10.1145/1670252.1670291},
isbn = {9781605589121},
journal = {Proceedings of the 8th International Conference on Virtual Reality Continuum and its Applications in Industry VRCAI 09},
pages = {179--184},
publisher = {ACM Press},
series = {VRCAI '09},
title = {{A co-located collaborative augmented reality application}},
url = {http://portal.acm.org/citation.cfm?doid=1670252.1670291},
year = {2009}
}
@article{Spakov2007,
abstract = {Eye tracking devices generate enormous amount of data, which requires a well-balanced approach to selective visualization of the data. This approach involves employing some data clustering algorithm. Most of the tradi- tional algorithms, however, are too slow as well as inadequately deterministic to be applied to eye gaze data. This paper describes our software implementation of two modifications of the clustering algorithm suitable for visualization of eye gaze data. Such a visualization greatly facilitates data analysis by grouping the individual samples into more meaningful units referred to as gaze fixations.},
annote = {- cited by: 3
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {\v{S}pakov, Oleg and Miniotas, D},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Information Technology and Control/2007/\v{S}pakov, Miniotas/\v{S}pakov, Miniotas - 2007 - Application of clustering algorithms in eye gaze visualizations.pdf:pdf},
journal = {Information Technology and Control},
keywords = {clustering algorithms,data clusters,eye tracking,gaze analysis,information visualization,pie charts.},
mendeley-tags = {gaze analysis},
number = {2},
pages = {213--216},
title = {{Application of clustering algorithms in eye gaze visualizations}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.116.1091\&rep=rep1\&type=pdf},
volume = {36},
year = {2007}
}
@article{Pijnappel2013,
address = {New York, New York, USA},
author = {Pijnappel, Sebastiaan and Mueller, Florian},
doi = {10.1145/2470654.2466165},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1271},
publisher = {ACM Press},
title = {{4 Design Themes for Skateboarding}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466165},
year = {2013}
}
@inproceedings{Duchowski2002,
abstract = {This paper presents an improved 3D eye movement analysis algorithm for binocular eye tracking within Virtual Reality for visual inspection training. The user's gaze direction, head position and orientation are tracked to allow recording of the user's fixations within the environment. The paper summarizes methods for (1) integrating the eye tracker into a Virtual Reality framework, (2) calculating the user's 3D gaze vector, and (3) calibrating the software to estimate the user's inter-pupillary distance post-facto. New techniques are presented for eye movement analysis in 3D for improved signal noise suppression. The paper describes (1) the use of Finite Impulse Response (FIR) filters for eye movement analysis, (2) the utility of adaptive thresholding and fixation grouping, and (3) a heuristic method to recover lost eye movement data due to miscalibration. While the linear signal analysis approach is itself not new, its application to eye movement analysis in three dimensions advances traditional 2D approaches since it takes into account the 6 degrees of freedom of head movements and is resolution independent. Results indicate improved noise suppression over our previous signal analysis approach.},
address = {New York, New York, USA},
author = {Duchowski, Andrew T. and Medlin, Eric and Cournia, Nathan and Gramopadhye, Anand and Melloy, Brian and Nair, Santosh},
booktitle = {Proceedings of the symposium on Eye tracking research \& applications - ETRA '02},
doi = {10.1145/507072.507094},
isbn = {1581134673},
keywords = {3d gaze},
mendeley-tags = {3d gaze},
pages = {103},
publisher = {ACM Press},
title = {{3D eye movement analysis for VR visual inspection training}},
url = {http://dl.acm.org/citation.cfm?id=507094 http://portal.acm.org/citation.cfm?doid=507072.507094},
year = {2002}
}
@book{Coxeter2003,
abstract = {In Euclidean geometry, constructions are made with ruler and compass. Projective geometry is simpler: its constructions require only a ruler. In projective geometry one never measures anything, instead, one relates one set of points to another by a projectivity. The first two chapters of this book introduce the important concepts of the subject and provide the logical foundations. The third and fourth chapters introduce the famous theorems of Desargues and Pappus. Chapters 5 and 6 make use of projectivities on a line and plane, respectively. The next three chapters develop a self-contained account of von Staudt's approach to the theory of conics. The modern approach used in that development is exploited in Chapter 10, which deals with the simplest finite geometry that is rich enough to illustrate all the theorems nontrivially. The concluding chapters show the connections among projective, Euclidean, and analytic geometry.},
author = {Coxeter, Harold S. M.},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2003/Coxeter/Coxeter - 2003 - Projective geometry.pdf:pdf},
isbn = {0387406239},
pages = {162},
publisher = {Springer},
title = {{Projective geometry}},
url = {http://books.google.com/books?id=gjAAI4FW0tsC\&pgis=1},
year = {2003}
}
@inproceedings{Martinez2006,
abstract = {This paper describes the tasks carried out to develop a control tool using the changes detected in gaze, which are captured in the electrooculogram signal. The objective is to use these changes to control a user interface such as Dasher. A software tool for generating visual stimuli and acquiring the eye signal has been developed. These signals were later processed with a first derivative-based algorithm in order to detect the changes. The optimal parameters for the algorithm have been determined, and also the sensitivity (S>97\%) and the predictive positive value (+PV>90\%) of the detector have also been calculated. The preliminary results are promising, but a study with a greater number of individuals should be made to check the on-line performance with longer registers.},
address = {Tenerife, Canary Islands, Spain},
author = {Martinez, M and Soria, E and Magdalena, R},
booktitle = {Proceedings of the 6th WSEAS international conference on Applied computer science},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 6th WSEAS international conference on Applied computer science/2006/Martinez, Soria, Magdalena/Martinez, Soria, Magdalena - 2006 - Identification of saccades in Electrooculograms and their use as a control tool.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {291--295},
title = {{Identification of saccades in Electrooculograms and their use as a control tool}},
url = {http://www.wseas.us/e-library/conferences/2006tenerife/papers/541-212.pdf},
year = {2006}
}
@book{Nakano2013,
address = {London},
doi = {10.1007/978-1-4471-4784-8},
editor = {Nakano, Yukiko I. and Conati, Cristina and Bader, Thomas},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2013/Unknown/Unknown - 2013 - Eye Gaze in Intelligent User Interfaces.pdf:pdf},
isbn = {978-1-4471-4783-1},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
publisher = {Springer London},
title = {{Eye Gaze in Intelligent User Interfaces}},
url = {http://www.springerlink.com/index/10.1007/978-1-4471-4784-8},
year = {2013}
}
@article{Goldenstein1999,
abstract = {We present a dynamic systems approach to modeling and generating low-level behaviors for autonomous agents. Such behaviors include real-time target tracking and obstacle avoidance in time-varying environments. The novelty of the method lies on the integration of distinct non-linear dynamic systems to model the agent’s interaction with the environment. An angular velocity control dynamic system guides the agent’s direction angle, while another dynamic system selects the environmental input that will be used in the control system. The agent interacts with the environment through its knowledge of the position of stationary and moving objects. In our system agents automatically avoid stationary and moving obstacles to reach the desired target(s). This approach allows us to prove the stability conditions that result in a principled methodology for the computation of the system’s dynamic parameters. We present a variety of real-time simulations that illustrate the power of our approach.},
author = {Goldenstein, Siome and Large, Edward and Metaxas, Dimitris},
doi = {10.1007/s003710050184},
issn = {0178-2789},
journal = {The Visual Computer},
month = mar,
number = {7-8},
pages = {349--364},
title = {{Non-linear dynamical system approach to behavior modeling}},
url = {http://link.springer.com/10.1007/s003710050184},
volume = {15},
year = {1999}
}
@inproceedings{Takacs2008,
abstract = {We have built an outdoors augmented reality system for mobile phones that matches camera-phone images against a large database of location-tagged images using a robust image retrieval algorithm. We avoid network latency by implementing the algorithm on the phone and deliver excellent performance by adapting a state-of-the-art image retrieval algorithm based on robust local descriptors. Matching is performed against a database of highly relevant features, which is continuously updated to reflect changes in the environment. We achieve fast updates and scalability by pruning of irrelevant features based on proximity to the user. By compressing and incrementally updating the features stored on the phone we make the system amenable to low-bandwidth wireless connections. We demonstrate system robustness on a dataset of location-tagged images and show a smart-phone implementation that achieves a high image matching rate while operating in near real-time.},
address = {New York, New York, USA},
author = {Takacs, Gabriel and Chandrasekhar, Vijay and Gelfand, Natasha and Xiong, Yingen and Chen, Wei-Chao and Bismpigiannis, Thanos and Grzeszczuk, Radek and Pulli, Kari and Girod, Bernd},
booktitle = {Proceeding of the 1st ACM international conference on Multimedia information retrieval - MIR '08},
doi = {10.1145/1460096.1460165},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceeding of the 1st ACM international conference on Multimedia information retrieval - MIR '08/2008/Takacs et al/Takacs et al. - 2008 - Outdoors augmented reality on mobile phone using loxel-based visual feature organization.pdf:pdf},
isbn = {9781605583129},
keywords = {augmented reality,outdoors},
mendeley-tags = {augmented reality,outdoors},
pages = {427},
publisher = {ACM Press},
title = {{Outdoors augmented reality on mobile phone using loxel-based visual feature organization}},
url = {http://dl.acm.org/citation.cfm?id=1460165 http://portal.acm.org/citation.cfm?doid=1460096.1460165},
year = {2008}
}
@inproceedings{Berger1997,
abstract = {We present a new approach for resolving occlusions in augmented reality. The main interest is that it does not require 3D reconstruction of the considered scene. Our idea is to use a contour based approach and to label each contour point as being “behind” or “in front of”, depending on whether it is in front of or behind the virtual object. This labeling step only requires that the contours can be tracked from frame to frame. A proximity graph is then built in order to group the contours that belong to the same occluding object. Finally, we use some kind of active contours to accurately recover the mask of the occluding object},
address = {San Juan},
author = {Berger, MO},
booktitle = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.1997.609304},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition/1997/Berger/Berger - 1997 - Resolving occlusion in augmented reality a contour based approach without 3D reconstruction.pdf:pdf},
isbn = {0-8186-7822-4},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {91--96},
publisher = {IEEE Comput. Soc},
title = {{Resolving occlusion in augmented reality: a contour based approach without 3D reconstruction}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=609304 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=609304},
year = {1997}
}
@article{Nguyen2013,
address = {New York, New York, USA},
author = {Nguyen, Cuong and Niu, Yuzhen and Liu, Feng},
doi = {10.1145/2470654.2466150},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1169},
publisher = {ACM Press},
title = {{Direct manipulation video navigation in 3D}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466150},
year = {2013}
}
@inproceedings{Xu2003,
address = {Singapore},
author = {Xu, Min and Duan, Ling-Yu and Xu, Changsheng and Kankanhalli, Mohan and Tian, Qi},
publisher = {IEEE},
title = {{Event Detection in Basketball Video Using Multiple Modalities}},
year = {2003}
}
@article{Williams2002,
abstract = {. Skilled (n = 12) and less skilled (n = 12) billiards players participated in 2 experiments in which the relationship between quiet eye duration, expertise, and task complexity was examined in a near and a far aiming task. Quiet eye was defined as the final fixation on the target prior to the initiation of movement. In Experiment 1, skilled performers exhibited longer fixations on the target (quiet eye) during the preparation phase of the action than their less skilled counterparts did. Quiet eye duration increased as a function of shot difficulty and was proportionally longer on successful than on unsuccessful shots for both groups of participants. In Experiment 2, participants executed shots under 3 different time-constrained conditions in which quiet eye periods were experimentally manipulated. Shorter quiet eye periods resulted in poorer performance, irrespective of participant skill level. The authors argue that quiet eye duration represents a critical period for movement programming in the aiming response.},
author = {Williams, A Mark and Singer, Robert N and Frehlich, Shane G},
doi = {10.1080/00222890209601941},
issn = {0022-2895},
journal = {Journal of motor behavior},
keywords = {Adult,Blinking,Eye Movements,Eye Movements: physiology,Fixation,Humans,Motor Skills,Ocular,Random Allocation,Time Factors,Visual Perception},
month = jun,
number = {2},
pages = {197--207},
pmid = {12057892},
title = {{Quiet eye duration, expertise, and task complexity in near and far aiming tasks.}},
url = {http://www.tandfonline.com/doi/full/10.1080/00222890209601941 http://www.ncbi.nlm.nih.gov/pubmed/12057892},
volume = {34},
year = {2002}
}
@article{Morimoto1997,
abstract = {The authors present a fast electronic image stabilization system that compensates for 3D rotation. The extended Kalman filter framework is employed to estimate the rotation between frames, which is represented using unit quaternions. A small set of automatically selected and tracked feature points are used as measurements. The effectiveness of this technique is also demonstrated by constructing mosaic images from the motion estimates, and comparing them to mosaics built from 2D stabilization algorithms. Two different stabilization schemes are presented. The first, implemented in a real-time platform based on a Datacube MV200 board, estimates the motion between two consecutive frames and is able to process gray level images of resolution 128×120 at 10 Hz. The second scheme estimates the motion between the current frame and an inverse mosaic; this allows better estimation without the need for indexing the new image frames. Experimental results for both schemes using real and synthetic image sequences are presented},
address = {San Juan},
author = {Morimoto, C. and Chellappa, R.},
doi = {10.1109/CVPR.1997.609396},
isbn = {0-8186-7822-4},
journal = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {660--665},
publisher = {IEEE Comput. Soc},
title = {{Fast 3D stabilization and mosaic construction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=609396 http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=609396},
year = {1997}
}
@article{Kramer2013,
address = {New York, New York, USA},
author = {Kr\"{a}mer, Jan-Peter and Karrer, Thorsten and Kurz, Joachim and Wittenhagen, Moritz and Borchers, Jan},
doi = {10.1145/2470654.2466419},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3073},
publisher = {ACM Press},
title = {{How tools in IDEs shape developers' navigation behavior}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466419},
year = {2013}
}
@article{Lin2013,
address = {New York, New York, USA},
author = {Lin, Sharon and Hanrahan, Pat},
doi = {10.1145/2470654.2466424},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3101},
publisher = {ACM Press},
title = {{Modeling how people extract color themes from images}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466424},
year = {2013}
}
@inproceedings{Tavares2010,
author = {Tavares, Anderson Carlos M and Fernandes, Sergio Murilo M and {Lencastre P. de Menezes Cruz}, Maria},
booktitle = {2010 International Conference on Cyberworlds},
doi = {10.1109/CW.2010.71},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2010 International Conference on Cyberworlds/2010/Tavares, Fernandes, Lencastre P. de Menezes Cruz/Tavares, Fernandes, Lencastre P. de Menezes Cruz - 2010 - NHE Collaborative Virtual Environment with Augmented Reality on Web.pdf:pdf},
isbn = {978-1-4244-8301-3},
keywords = {- virtual collaborative spaces,augmented,collaboration,computer vision,mixed and virtual,networked},
month = oct,
pages = {438--444},
publisher = {IEEE},
title = {{NHE: Collaborative Virtual Environment with Augmented Reality on Web}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5656154},
year = {2010}
}
@article{Kamuro2009,
author = {Kamuro, Sho and Minamizawa, Kouta and Kawakami, Naoki and Tachi, Susumu},
doi = {10.1109/ROMAN.2009.5326217},
isbn = {978-1-4244-5081-7},
journal = {RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication},
month = sep,
pages = {436--441},
publisher = {Ieee},
title = {{Ungrounded kinesthetic pen for haptic interaction with virtual environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5326217},
year = {2009}
}
@article{Henderson2003,
abstract = {In human vision, acuity and color sensitivity are best at the point of fixation, and the visual-cognitive system exploits this fact by actively controlling gaze to direct fixation towards important and informative scene regions in real time as needed. How gaze control operates over complex real-world scenes has recently become of central concern in several core cognitive science disciplines including cognitive psychology, visual neuroscience, and machine vision. This article reviews current approaches and empirical findings in human gaze control during real-world scene perception.},
author = {Henderson, JM},
doi = {10.1016/j.tics.2003.09.006},
file = {::},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
month = nov,
number = {11},
pages = {498--504},
title = {{Human gaze control during real-world scene perception}},
url = {http://www.sciencedirect.com/science/article/pii/S1364661303002481 http://linkinghub.elsevier.com/retrieve/pii/S1364661303002481},
volume = {7},
year = {2003}
}
@article{Reitmayr2001,
abstract = {The combination of mobile computing and collaborative augmented reality into a single system makes the power of computer enhanced interaction and communication in the real world accessible anytime and everywhere. The paper describes our work to build a mobile collaborative augmented reality system that supports true stereoscopic 3D graphics, a pen and pad interface and direct interaction with virtual objects. The system is assembled from off-the-shelf hardware components and serves as a basic test bed for user interface experiments related to computer supported collaborative work in augmented reality. A mobile platform implementing the described features and collaboration between mobile and stationary users are demonstrated},
author = {Reitmayr, G and Schmalstieg, D},
doi = {10.1109/ISAR.2001.970521},
isbn = {0769513751},
journal = {Proceedings IEEE and ACM International Symposium on Augmented Reality},
keywords = {3d interaction,a user wearing,able computing,augmented reality,computer supported collaborative work,figure 1,hybrid tracking,mobile aug,mobile computing,wear},
pages = {114--123},
publisher = {IEEE Comput. Soc},
title = {{Mobile collaborative augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=970521},
year = {2001}
}
@article{Carlson2009,
author = {Carlson, T and Demiris, Y},
journal = {New Frontiers in Human Robot Interaction,  \ldots},
title = {{Using visual attention to evaluate collaborative control architectures for human robot interaction}},
url = {http://www.researchgate.net/publication/45680633\_Using\_Visual\_Attention\_to\_Evaluate\_Collaborative\_Control\_Architectures\_for\_Human\_Robot\_Interaction/file/9fcfd50929e79ef719.pdf},
year = {2009}
}
@inproceedings{Nakano2008,
abstract = {When human cannot perceive the inconsistency of artificial shadows which are not physically correct, they are acceptable as “perceptually-correct” shadows. This paper focuses on the simplification of light-source models for generating the perceptually-correct artificial shadows. First, we conducted subjective evaluations to obtain knowledge about the human perception of the shadows. Then the knowledge was applied to control the resolution of the light-source map to generate perceptually-correct artificial shadows. Comparative studies among artificial and real shadows justified perceptually correctness. All experiments were done using still images, not videos. Our research becomes a reference to determine the resolution of light-source map in an MR scene.},
author = {Nakano, G and Kitahara, I and Ohta, Y},
booktitle = {2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2008.4637352},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2008 7th IEEEACM International Symposium on Mixed and Augmented Reality/2008/Nakano, Kitahara, Ohta/Nakano, Kitahara, Ohta - 2008 - Generating perceptually-correct shadows for mixed reality.pdf:pdf;:home/acmt/Dropbox/Documentos/Mendeley/2008 7th IEEEACM International Symposium on Mixed and Augmented Reality/2008/Nakano, Kitahara, Ohta/Nakano, Kitahara, Ohta - 2008 - Generating perceptually-correct shadows for mixed reality(2).pdf:pdf},
isbn = {978-1-4244-2840-3},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = sep,
pages = {173--174},
publisher = {IEEE},
title = {{Generating perceptually-correct shadows for mixed reality}},
url = {http://dl.acm.org/citation.cfm?id=1605323 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4637352},
year = {2008}
}
@inproceedings{Karamouzas2009,
abstract = {Virtual worlds are nowadays commonly used in interactive applications, like computer games and simulations. Typically, such worlds are populated by a large number of virtual characters. On one hand, these characters have global goals with respect to the environment and thus, they must be able to plan their paths toward their desired locations. On the other hand, they should try to avoid collisions with each other and with the environment. In this paper, we present a new technique to improve the global routes that characters prefer to follow by taking into account the crowd density. Consequently, in our simulations, characters can intelligently plan their paths around congested areas, favoring less dense regions. This leads to an efficient flow of characters, reducing at the same time the amount of energy and time consuming avoidance maneuvers that the characters have to perform. The technique is fast and can plan paths for thousands of characters in real-time.},
address = {London},
author = {Karamouzas, Ioannis and Bakker, Jiri and Overmars, Mark H.},
booktitle = {2009 International IEEE Consumer Electronics Society's Games Innovations Conference},
doi = {10.1109/ICEGIC.2009.5293590},
isbn = {978-1-4244-4459-5},
month = aug,
pages = {160--168},
publisher = {IEEE},
title = {{Density constraints for crowd simulation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5293590},
year = {2009}
}
@incollection{Azuma1999a,
address = {Yokohama, Japan},
author = {Azuma, Ronald T},
booktitle = {Mixed Reality: Merging Real and Virtual Worlds},
chapter = {21},
editor = {Ohta, Yuichi and Tamura, Hideyuki},
isbn = {3540656235},
pages = {379--390},
publisher = {Springer-Verlag},
title = {{The Challenge of Making Augmented Reality Work Outdoors}},
url = {http://www.ronaldazuma.com/publications.html},
year = {1999}
}
@inproceedings{Platonov2007,
abstract = {We present a solution for automatic extraction of contour models out of polygonal CAD models. Such contour models can be used for markerless initialization and tracking purposes. To create a contour model we synthesize different views of the object using its CAD model. During the view generation we alternate the camera pose as well as light conditions in order to extract the most stable contours in terms of illumination invariance and view independence. We project the extracted 2D edges back into the 3D space and accumulate for every 3D point statistics over different views describing its visibility and stability under different illumination conditions. After filtering out all 3D points with probability below a certain threshold value we use the Euclidean Minimum Spanning Tree algorithm to get the connected contours out of the 3D point cloud. The result is a B-Spline or VRML representation of the most stable contours.},
address = {Nara-JP},
author = {Platonov, Juri and Langer, Marion},
booktitle = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2007.4538829},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality/2007/Platonov, Langer/Platonov, Langer - 2007 - Automatic contour model creation out of polygonal CAD models for markerless Augmented Reality.pdf:pdf},
isbn = {978-1-4244-1749-0},
month = nov,
pages = {1--4},
publisher = {IEEE},
title = {{Automatic contour model creation out of polygonal CAD models for markerless Augmented Reality}},
url = {http://dl.acm.org/citation.cfm?id=1514340 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538829},
year = {2007}
}
@article{Reitmaier2013,
address = {New York, New York, USA},
author = {Reitmaier, Thomas and Benz, Pierre and Marsden, Gary},
doi = {10.1145/2470654.2470709},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {381},
publisher = {ACM Press},
title = {{Designing and theorizing co-located interactions}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470709},
year = {2013}
}
@article{Voelker2013,
address = {New York, New York, USA},
author = {Voelker, Simon and Wacharamanotham, Chat and Borchers, Jan},
doi = {10.1145/2470654.2470759},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {745},
publisher = {ACM Press},
title = {{An evaluation of state switching methods for indirect touch systems}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470759},
year = {2013}
}
@inproceedings{Bednarik2005,
abstract = {Eye-movement tracking proved its potentials in many areas of human-computer interaction. Resting on a hypothesis that eye-direction and mind are linked, some of the HCI researchers have employed eye-movement trackers to investigate the visual attention focus of the participants completing their tasks. Others have used the eye-movement tracking in real-time applications, either as a direct interaction device or as an input to gaze-aware interfaces. Inspired by the previous HCI applications, we propose to utilize eye- movement trackers in adaptive systems research and development in two ways. First, the evaluations of adaptive systems could get an access to the information otherwise unavailable, as for instance to how the visual attention and cognitive processing are influenced by an adaptivity implemented into the evaluated system. Second, we propose to employ the eye-movement tracking technologies for a real-time registration of users’ loci of visual attention, therefore increasing the awareness of the adaptive systems about their current users. We discuss possible potentials, difficulties and pitfalls of eye-movement tracking when applied to adaptive systems. We argue that a methodological framework of applying eye-tracking into adaptive systems shall be developed.},
address = {Edinburgh, UK},
annote = {- cited by: 9
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Bednarik, Roman},
booktitle = {Proceedings of the Fourth Workshop on the Evaluation of Adaptive Systems},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the Fourth Workshop on the Evaluation of Adaptive Systems/2005/Bednarik/Bednarik - 2005 - Potentials of eye-movement tracking in adaptive systems.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {1--8},
title = {{Potentials of eye-movement tracking in adaptive systems}},
url = {http://www.cs.joensuu.fi/pages/int/pub/bednarik05a.pdf},
year = {2005}
}
@article{Musse2007,
abstract = {In this paper, we propose a new model to simulate the movement of virtual humans based on trajectories captured automatically from filmed video sequences. These trajectories are grouped into similar classes using an unsupervised clustering algorithm, and an extrapolated velocity field is generated for each class. A physically-based simulator is then used to animate virtual humans, aiming to reproduce the trajectories fed to the algorithm and at the same time avoiding collisions with other agents. The proposed approach provides an automatic way to reproduce the motion of real people in a virtual environment, allowing the user to change the number of simulated agents while keeping the same goals observed in the filmed video.},
author = {Musse, Soraia Raupp and Jung, Cl\'{a}udio R. and Jacques, Julio C. S. and Braun, Adriana},
doi = {10.1002/cav.163},
issn = {15464261},
journal = {Computer Animation and Virtual Worlds},
keywords = {audio r,by soraia r,cl,crowd simulation,data-driven,jacques jr and,julio c,jung,musse,s},
mendeley-tags = {crowd simulation,data-driven},
month = may,
number = {2},
pages = {83--93},
title = {{Using computer vision to simulate the motion of virtual agents}},
url = {http://doi.wiley.com/10.1002/cav.163},
volume = {18},
year = {2007}
}
@article{Dementhon1995,
author = {Dementhon, Daniel F. and Davis, Larry S.},
doi = {10.1007/BF01450852},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = jun,
number = {1-2},
pages = {123--141},
publisher = {Kluwer Academic Publishers},
title = {{Model-based object pose in 25 lines of code}},
url = {http://dl.acm.org/citation.cfm?id=204005.204015},
volume = {15},
year = {1995}
}
@inproceedings{Boivin2001,
abstract = {In this paper, we present a new method to recover an approximation of the bidirectional reflectance distribution function (BRDF) of the surfaces present in a real scene. This is done from a single photograph and a 3D geometric model of the scene. The result is a full model of the reflectance properties of all surfaces, which can be rendered under novel illumination conditions with, for example, viewpoint modification and the addition of new synthetic objects. Our technique produces a reflectance model using a small number of parameters. These parameters nevertheless approximate the BRDF and allow the recovery of the photometric properties of diffuse, specular, isotropic or anisotropic textured objects. The input data are a geometric model of the scene including the light source positions and the camera properties, and a single image captured using this camera. Our algorithm generates a new synthetic image using classic rendering techniques, and a lambertian hypothesis about the reflectance model of the surfaces. Then, it iteratively compares the original image to the new one, and chooses a more complex reflectance model if the difference between the two images is greater than a user-defined threshold. We present several synthetic images that are compared to the original ones, and some possible applications in augmented reality.},
address = {New York, New York, USA},
author = {Boivin, Samuel and Gagalowicz, Andre},
booktitle = {Proceedings of the 28th annual conference on Computer graphics and interactive techniques - SIGGRAPH '01},
doi = {10.1145/383259.383270},
isbn = {158113374X},
keywords = {brdf models,global illumination,image-based rendering,ing,inverse render-,radiance,radiosity,reflectance recovery,rendering,rerendering},
number = {August},
pages = {107--116},
publisher = {ACM Press},
title = {{Image-based rendering of diffuse, specular and glossy surfaces from a single image}},
url = {http://portal.acm.org/citation.cfm?doid=383259.383270},
year = {2001}
}
@article{Behrens2010,
abstract = {This analysis of time series of eye movements is a saccade-detection algorithm that is based on an earlier algorithm. It achieves substantial improvements by using an adaptive-threshold model instead of fixed thresholds and using the eye-movement acceleration signal. This has four advantages: (1) Adaptive thresholds are calculated automatically from the preceding acceleration data for detecting the beginning of a saccade, and thresholds are modified during the saccade. (2) The monotonicity of the position signal during the saccade, together with the acceleration with respect to the thresholds, is used to reliably determine the end of the saccade. (3) This allows differentiation between saccades following the main-sequence and non-main-sequence saccades. (4) Artifacts of various kinds can be detected and eliminated. The algorithm is demonstrated by applying it to human eye movement data (obtained by EOG) recorded during driving a car. A second demonstration of the algorithm detects microsleep episodes in eye movement data.},
annote = {- keyword coletado
        
Resumo: Detec\c{c}\~{a}o de sacadas com limiar adaptativo (usando acelera\c{c}\~{a}o)
O autor defende que o algoritmo cont\'{e}m 4 vantagens:
- 
        
        
Como \'{e} o m\'{e}todo:
- O m\'{e}todo cont\'{e}m como entrada a janela (T) de amostras para calcular o desvio padr\~{a}o do ru\'{\i}do (Sigma), e um valor de multiplicidade (N). Ele usou N=3.4, mas n\~{a}o justificou e n\~{a}o testou esta constante em outros experimentos.
Define uma janela de T amostras para calcular o Sigma. Define o limiar como sendo N*Sigma. Se a acelera\c{c}\~{a}o ultrapassar o limiar, o limiar \'{e} mantido constante por 200ms e em seguida cresce linearmente at\'{e} atingir N*Sigma, onde o limiar \'{e} novamente N*Sigma.
Cria-se uma fun\c{c}\~{a}o = +1 durante a sacada (ultrapassando o limiar),= -1 se ultrapassar o limiar negativo e = 0 quando n\~{a}o detectar sacada (vari\'{a}vel de estado)
O limiar adaptativo detecta outros tipos de movimento sac\'{a}dico, como glissadas.},
author = {Behrens, F and Mackeben, M and Schr\"{o}der-Preikschat, W},
doi = {10.3758/BRM.42.3.701},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Behavior research methods/2010/Behrens, Mackeben, Schr\"{o}der-Preikschat/Behrens, Mackeben, Schr\"{o}der-Preikschat - 2010 - An improved algorithm for automatic detection of saccades in eye movement data and for.pdf:pdf},
issn = {1554-3528},
journal = {Behavior research methods},
keywords = {Adaptation,Algorithms,Automation,Automobile Driving,Automobile Driving: psychology,Calibration,Data Collection,Data Collection: methods,Data Interpretation,Electrooculography,Eye Tracking,Fixation,Humans,Ocular,Physiological,Physiological: physiology,Saccades,Saccades: physiology,Segmentation,Sleep,Sleep: physiology,Statistical},
mendeley-tags = {Eye Tracking,Segmentation},
month = aug,
number = {3},
pages = {701--8},
pmid = {20805592},
title = {{An improved algorithm for automatic detection of saccades in eye movement data and for calculating saccade parameters.}},
url = {http://link.springer.com/article/10.3758/BRM.42.3.701 http://www.ncbi.nlm.nih.gov/pubmed/20805592},
volume = {42},
year = {2010}
}
@article{Abernethy1996,
author = {Abernethy, B},
journal = {American Journal of sports medicine},
title = {{Training the visual-perceptual skills of athletes: Insights from the study of motor expertise}},
url = {http://cat.inist.fr/?aModele=afficheN\&cpsidt=2517359},
year = {1996}
}
@article{Overney2008,
abstract = {In tennis, as in many disciplines of sport, fine spatio-temporal resolution is required to reach optimal performance. While many studies on tennis have focused on anticipatory skills or decision making, fewer have investigated the underlying visual perception abilities. In this study, we used a battery of seven visual tests that allowed us to assess which kind of visual information processing is performed better by tennis players than other athletes (triathletes) and non-athletes. We found that certain time-related skills, such as speed discrimination, are superior in tennis players compared to non-athletes and triathletes. Such tasks might be used to improve tennis performance in the future.},
author = {Overney, Leila S and Blanke, Olaf and Herzog, Michael H},
doi = {10.1371/journal.pone.0002380},
issn = {1932-6203},
journal = {PloS one},
keywords = {Adult,Attention,Humans,Male,Movement,Reaction Time,Tennis,Tennis: physiology,Vision Tests},
month = jan,
number = {6},
pages = {e2380},
pmid = {18545661},
title = {{Enhanced temporal but not attentional processing in expert tennis players.}},
url = {http://dx.plos.org/10.1371/journal.pone.0002380 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2398771\&tool=pmcentrez\&rendertype=abstract},
volume = {3},
year = {2008}
}
@inproceedings{Wilczkowiak2001,
abstract = {In this paper parallelepipeds and their use in camera calibration and 3D reconstruction processes are studied. Parallelepipeds naturally characterize rigidity constraints present in a scene, such as parallelism and orthogonality. A subclass of parallelepipeds-the cuboids-has been frequently used over the past to partially calibrate cameras. However, the full potential of parallelepipeds, in camera calibration as well as in scene reconstruction, has never been clearly established. We propose a new framework for the use of parallelepipeds which is based on an extensive study of this potential. In particular, we exhibit the complete duality that exists between the intrinsic metric characteristics of a parallelepiped and the intrinsic parameters of a camera. Our framework allows to fully exploit parallelepipeds and thus overcomes several limitations of calibration approaches based on cuboids. To illustrate this framework, we present an original and very efficient interactive method for 3D reconstruction from single images. This method allows to quickly build a scene model from a single uncalibrated image},
author = {Wilczkowiak, M and Boyer, E and Sturm, P},
booktitle = {Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001},
doi = {10.1109/ICCV.2001.937510},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001/2001/Wilczkowiak, Boyer, Sturm/Wilczkowiak, Boyer, Sturm - 2001 - Camera calibration and 3D reconstruction from single images using parallelepipeds.pdf:pdf},
isbn = {0-7695-1143-0},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {142--148},
publisher = {IEEE Comput. Soc},
title = {{Camera calibration and 3D reconstruction from single images using parallelepipeds}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=937510 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=937510},
volume = {1},
year = {2001}
}
@article{Moore2011,
author = {Moore, Brian E. and Ali, Saad and Mehran, Ramin and Shah, Mubarak},
doi = {10.1145/2043174.2043192},
issn = {00010782},
journal = {Communications of the ACM},
month = dec,
number = {12},
pages = {64},
title = {{Visual crowd surveillance through a hydrodynamics lens}},
url = {http://dl.acm.org/citation.cfm?doid=2043174.2043192},
volume = {54},
year = {2011}
}
@inproceedings{Debevec2005,
abstract = {We present a technique for approximating a light probe image as a constellation of light sources based on a median cut algorithm. The algorithm is efficient, simple to implement, and can realistically represent a complex lighting environment with as few as 64 point light sources.},
address = {New York, New York, USA},
author = {Debevec, Paul},
booktitle = {ACM SIGGRAPH 2005 Posters on - SIGGRAPH '05},
doi = {10.1145/1186954.1187029},
pages = {66},
publisher = {ACM Press},
title = {{A median cut algorithm for light probe sampling}},
url = {http://portal.acm.org/citation.cfm?doid=1186954.1187029},
year = {2005}
}
@inproceedings{Thrun2005,
abstract = {We describe a technique for reconstructing probable occluded surfaces from 3D range images. The technique exploits the fact that many objects possess shape symmetries that can be recognized even from partial 3D views. Our approach identifies probable symmetries and uses them to attend the partial 3D shape model into the occluded space. To accommodate objects consisting of multiple parts, we describe a technique for segmenting objects into parts characterized by different symmetries. Results are provided for a real-world database of 3D range images of common objects, acquired through an active stereo rig},
author = {Thrun, S and Wegbreit, B},
booktitle = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
doi = {10.1109/ICCV.2005.221},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1/2005/Thrun, Wegbreit/Thrun, Wegbreit - 2005 - Shape from symmetry.pdf:pdf},
isbn = {0-7695-2334-X},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {1824--1831 Vol. 2},
publisher = {IEEE},
title = {{Shape from symmetry}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1544938 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544938},
year = {2005}
}
@misc{Lindeijer2013,
author = {Lindeijer, Thorbj\o rn},
keywords = {sbgames},
mendeley-tags = {sbgames},
title = {{Tiled}},
url = {http://www.mapeditor.org/},
urldate = {2013/07/26},
year = {2013}
}
@article{Hallum2005,
abstract = {A visual tracking task was administered to 20 subjects afforded simulated prosthetic vision (a phosphene array); a total of 3h data was taken from each subject over the course of 10 visits. The experiment assessed prosthetic visual fixation, saccade and smooth pursuit and the effect of practice. Further, we demonstrated an image analysis technique that assisted fixation and pursuit (but not saccade) accuracy, and required less vigorous movement of the phosphene array in pursuing the target. As measured by mean deviation from the target, fixation and pursuit accuracies were improved by 8.3 and 3.3 min of visual arc, respectively (35.8\% and 6.8\%), for inter-phosphene spacing of 1.9 degrees . The analysis technique, involving overlapping Gaussian kernels, was an heuristic design; this is the first step of an iterative, experimental approach to devising effective image analysis to be contained in an electronic vision prosthesis. The approach should ultimately afford implanted patients improved prosthetic visual function.},
author = {Hallum, Luke E and Suaning, Gregg J and Taubman, David S and Lovell, Nigel H},
doi = {10.1016/j.visres.2004.09.032},
issn = {0042-6989},
journal = {Vision research},
keywords = {Analysis of Variance,Computer Simulation,Equipment Design,Eye Movements,Eye Movements: physiology,Female,Fixation,Humans,Learning,Learning: physiology,Male,Motion Perception,Motion Perception: physiology,Ocular,Ocular: physiology,Phosphenes,Phosphenes: physiology,Practice (Psychology),Prostheses and Implants,Psychophysics,Pursuit,Reaction Time,Retina,Retina: physiology,Saccades,Saccades: physiology,Smooth,Smooth: physiology},
month = mar,
number = {6},
pages = {775--88},
pmid = {15639504},
title = {{Simulated prosthetic visual fixation, saccade, and smooth pursuit.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15639504},
volume = {45},
year = {2005}
}
@article{Hollerer1999,
abstract = {We describe an experimental mobile augmented reality system (MARS) testbed that employs different user interfaces to allow outdoor and indoor users to access and manage information that is spatially registered with the real world. Outdoor users can experience spatialized multimedia presentations that are presented on a head-tracked, see-through, head-worn display used in conjunction with a hand-held pen-based computer. Indoor users can get an overview of the outdoor scene and communicate with outdoor users through a desktop user interface or a head- and hand-tracked immersive augmented reality user interface.},
author = {H\"{o}llerer, Tobias and Feiner, Steven and Terauchi, Tachio and Rashid, Gus and Hallaway, Drexel},
doi = {10.1016/S0097-8493(99)00103-X},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computers \& Graphics/1999/H\"{o}llerer et al/H\"{o}llerer et al. - 1999 - Exploring MARS developing indoor and outdoor user interfaces to a mobile augmented reality system.pdf:pdf},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {augmented reality,outdoors},
mendeley-tags = {augmented reality,outdoors},
month = dec,
number = {6},
pages = {779--785},
title = {{Exploring MARS: developing indoor and outdoor user interfaces to a mobile augmented reality system}},
url = {http://www.sciencedirect.com/science/article/pii/S009784939900103X http://linkinghub.elsevier.com/retrieve/pii/S009784939900103X},
volume = {23},
year = {1999}
}
@inproceedings{Prince2002,
abstract = {We present a complete system for live capture of 3D content and simultaneous presentation in augmented reality. The user sees the real world from his viewpoint, but modified so that the image of a remote collaborator is rendered into the scene. Fifteen cameras surround the collaborator, and the resulting video streams are used to construct a three-dimensional model of the subject using a shape-from-silhouette algorithm. Users view a two-dimensional fiducial marker using a video-see-through augmented reality interface. The geometric relationship between the marker and head-mounted camera is calculated, and the equivalent view of the subject is computed and drawn into the scene. Our system can generate 384 288 pixel images of the models at 25 fps, with a latency of < 100 ms. The result gives the strong impression that the subject is a real part of the 3D scene. We demonstrate applications of this system in 3D videoconferencing and entertainment.},
author = {Prince, Simon J D and Cheok, Adrian David and Farbiz, Farzam and Williamson, Todd and Johnson, Nik and Billinghurst, Mark and Kato, Hirokazu},
booktitle = {International Symposium on Mixed and Augmented Reality ISMAR},
doi = {10.1109/ISMAR.2002.1115062},
isbn = {0769517811},
keywords = {3-d reconstruction,appropriately,augmented reality,col-,laborator is three-dimensional and,mixed reality,present in the space,shape-from-,silhouette,stable percept that the,this results in the,video-conferencing,with},
pages = {7--13},
publisher = {IEEE Comput. Soc},
title = {3d live: real time captured content for mixed reality},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1115062},
year = {2002}
}
@article{Kusano2013,
address = {New York, New York, USA},
author = {Kusano, Koki and Nakatani, Momoko and Ohno, Takehiko},
doi = {10.1145/2470654.2470710},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {391},
publisher = {ACM Press},
title = {{Scenario-based interactive UI design}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470710},
year = {2013}
}
@inproceedings{Koh2009a,
abstract = {This paper evaluates the input performance capabilities of Velocity Threshold (I-VT) and Kalman Filter (I-KF) eye movement detection models when employed for eye-gaze-guided interface control. I-VT is a common eye movement identification model employed by the eye tracking community, but it is neither robust nor capable of handling high levels of noise present in the eye position data. Previous research implies that use of a Kalman filter reduces the noise in the eye movement signal and predicts the signal during brief eye movement failures, but the actual performance of I-KF was never evaluated. We evaluated the performance of I-VT and I-KF models using guidelines for ISO 9241 Part 9 standard, which is designed for evaluation of non keyboard/mouse input devices with emphasis on performance, comfort, and effort. Two applications were implemented for the experiment: 1) an accuracy test 2) a photo viewing application specifically designed for eye-gaze-guided control. Twenty-one subjects participated in the evaluation of both models completing a series of tasks. The results indicates that I-KF allowed participants to complete more tasks with shorter completion time while providing higher general comfort, accuracy and operation speeds with easier target selection than the I-VT model. We feel that these results are especially important to the engineers of new assistive technologies and interfaces that employ eye-tracking technology in their design.},
address = {New York, New York, USA},
author = {Koh, Do Hyong and {Munikrishne Gowda}, Sandeep A. and Komogortsev, Oleg V},
booktitle = {Proceedings of the 1st ACM SIGCHI symposium on Engineering interactive computing systems - EICS '09},
doi = {10.1145/1570433.1570470},
isbn = {9781605586007},
keywords = {eye tracker,eye tracking,human computer interaction,kalman filter,pointing device evaluation,video game},
mendeley-tags = {eye tracking,video game},
month = jul,
pages = {197},
publisher = {ACM Press},
title = {{Input evaluation of an eye-gaze-guided interface}},
url = {http://dl.acm.org/citation.cfm?id=1570433.1570470},
year = {2009}
}
@article{Price2013,
address = {New York, New York, USA},
author = {Price, Sara and Jewitt, Carey},
doi = {10.1145/2470654.2481402},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2907},
publisher = {ACM Press},
title = {{Interview approaches to researching embodiment}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481402},
year = {2013}
}
@article{Tafaj2013,
abstract = {Complex and hazardous driving situations often arise with the delayed perception of traffic objects. To automatically detect whether such objects have been perceived by the driver, there is a need for techniques that can reliably recognize whether the driver’s eyes have fixated or are pursuing the hazardous object (i.e., detecting fixations, saccades, and smooth pursuits from raw eye tracking data). This paper presents a system for analyzing the driver’s visual behavior based on an adaptive online algorithm for detecting and distinguishing between fixation clusters, saccades, and smooth pursuits.},
author = {Tafaj, E and K\"{u}bler, TC and Kasneci, G},
doi = {10.1007/978-3-642-40728-4\_56},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Artificial Neural Networks and Machine Learning - ICANN/2013/Tafaj, K\"{u}bler, Kasneci/Tafaj, K\"{u}bler, Kasneci - 2013 - Online Classification of Eye Tracking Data for Automated Analysis of Traffic Hazard Perception.pdf:pdf},
journal = {Artificial Neural Networks and Machine Learning - ICANN},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {442--450},
title = {{Online Classification of Eye Tracking Data for Automated Analysis of Traffic Hazard Perception}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-40728-4\_56},
volume = {8131},
year = {2013}
}
@article{Marmitt2002,
author = {Marmitt, G and Duchowski, AT},
file = {::},
keywords = {Gaze 3D},
mendeley-tags = {Gaze 3D},
title = {{Modeling visual attention in VR: Measuring the accuracy of predicted scanpaths}},
url = {http://andrewd.ces.clemson.edu/research/vislab/docs/eg02.pdf},
year = {2002}
}
@article{Ryan2008a,
author = {Ryan, WJ and Woodard, DL},
file = {:home/acmt/Dropbox/Documentos/Mendeley/\ldots and Systems, 2008. \ldots/2008/Ryan, Woodard/Ryan, Woodard - 2008 - Adapting starburst for elliptical iris segmentation.pdf:pdf},
journal = {\ldots  and Systems, 2008.  \ldots},
keywords = {eye tracker},
mendeley-tags = {eye tracker},
title = {{Adapting starburst for elliptical iris segmentation}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4699340},
year = {2008}
}
@article{Jensen2010,
abstract = {This paper presents an interactive vision based real-time system for estimating light source positions and generating credible shadows for augmented reality. The implementation uses ARToolkit as a basis for geometric tracking, and a reflective sphere for tracking light sources in the environment. The paper seeks to generate perceptually credible shadows. User testing was conducted in order to determine the minimum criteria for the credibility of the shadows. User testing showed that 64 shadows are sufficient and indistinguishable from more complex compositions and a real image. It was clear that users need a reference object to distinguish between real and virtual shadows. The implementation and performance are tested using a consumer-grade web-camera and a regular laptop computer. The implementation can perform real-time with 256 generated shadows.},
author = {Jensen, BF and Laursen, JS},
file = {::},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
title = {{Simplifying real-time light source tracking and credible shadow generation for augmented reality using ARToolkit}},
url = {http://jbmadsen.com/uploads/paper\_final.pdf},
year = {2010}
}
@article{Fuchs2013,
address = {New York, New York, USA},
author = {Fuchs, Johannes and Fischer, Fabian and Mansmann, Florian and Bertini, Enrico and Isenberg, Petra},
doi = {10.1145/2470654.2466443},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3237},
publisher = {ACM Press},
title = {{Evaluation of alternative glyph designs for time series data in a small multiple setting}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466443},
year = {2013}
}
@inproceedings{Shic2008,
abstract = {In this paper we evaluate several of the most popular algorithms for segmenting fixations from saccades by testing these algorithms on the scanning patterns of toddlers. We show that by changing the parameters of these algorithms we change the reported fixation durations in a systematic fashion. However, we also show how choices in analysis can lead to very different interpretations of the same eye-tracking data. Methods for reconciling the disparate results of different algorithms as well as suggestions for the use of fixation identification algorithms in analysis, are presented.},
address = {New York, New York, USA},
annote = {- keyword coletado
- cited by: 28
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000
        
Resumo: Shic explora diferentes algoritmos de identifica\c{c}\~{a}o de fixa\c{c}\~{o}es mostrando que suas interpreta\c{c}\~{o}es podem ser diferentes, mesmo trabalhando com os mesmos dados coletados.
Ele analisa os seguintes algoritmos baseados em dispers\~{a}o:
- Dispers\~{a}o de Dist\^{a}ncia: a dist\^{a}ncia entre dois pontos quaisquer na fixa\c{c}\~{a}o n\~{a}o pode superar um limiar. \'{E} executado em O(n²)
- Centr\'{o}ide: os pontos de uma fixa\c{c}\~{a}o n\~{a}o podem ser mais distantes do que um limiar para sua centr\'{o}ide. Pode construir uma vers\~{a}o em tempo real, computando apenas os novos pontos.
- Posi\c{c}\~{a}o-Vari\^{a}ncia: modela o grupo de pontos como uma distribui\c{c}\~{a}o gaussiana, e n\~{a}o podem ultrapassar um desvio padr\~{a}o de dist\^{a}ncia.
- I-DT de Salvucci: a soma da m\'{a}xima dist\^{a}ncia horizontal com a m\'{a}xima dist\^{a}ncia vertical deve ser menor que um limiar.
        
Ele viu que o tempo de fixa\c{c}\~{a}o m\'{e}dio segue um comportamento linear para valores que correspondem aos limites fisiol\'{o}gicos da vis\~{a}o foveal (desvio padr\~{a}o da dispers\~{a}o at\'{e} 1̣° e tempo m\'{\i}nimo de fixa\c{c}\~{a}o at\'{e} 200ms), mesmo que o n\'{u}mero de fixa\c{c}\~{o}es e o total de tempo gasto nas fixa\c{c}\~{o}es forem n\~{a}o lineares.},
author = {Shic, Frederick and Scassellati, Brian and Chawarska, Katarzyna},
booktitle = {Proceedings of the 2008 symposium on Eye tracking research \& applications - ETRA '08},
doi = {10.1145/1344471.1344500},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2008 symposium on Eye tracking research \& applications - ETRA '08/2008/Shic, Scassellati, Chawarska/Shic, Scassellati, Chawarska - 2008 - The incomplete fixation measure.pdf:pdf},
isbn = {9781595939821},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
pages = {111},
publisher = {ACM Press},
title = {{The incomplete fixation measure}},
url = {http://dl.acm.org/citation.cfm?id=1344500 http://portal.acm.org/citation.cfm?doid=1344471.1344500},
year = {2008}
}
@article{Smolic20111958,
abstract = {This paper gives an end-to-end overview of 3D video and free viewpoint video, which can be regarded as advanced functionalities that expand the capabilities of a 2D video. Free viewpoint video can be understood as the functionality to freely navigate within real world visual scenes, as it is known for instance from virtual worlds in computer graphics. 3D video shall be understood as the functionality that provides the user with a 3D depth impression of the observed scene, which is also known as stereo video. In that sense as functionalities, 3D video and free viewpoint video are not mutually exclusive but can very well be combined in a single system. Research in this area combines computer graphics, computer vision and visual communications. It spans the whole media processing chain from capture to display and the design of systems has to take all parts into account, which is outlined in different sections of this paper giving an end-to-end view and mapping of this broad area. The conclusion is that the necessary technology including standard media formats for 3D video and free viewpoint video is available or will be available in the future, and that there is a clear demand from industry and user for such advanced types of visual media. As a consequence we are witnessing these days how such technology enters our everyday life},
annote = {Computer Analysis of Images and Patterns },
author = {Smolic, Aljoscha},
doi = {10.1016/j.patcog.2010.09.005},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Pattern Recognition/2011/Smolic/Smolic - 2011 - 3D video and free viewpoint video—From capture to display.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {3D video,3DTV,Free viewpoint video,Stereo video},
number = {9},
pages = {1958--1968},
title = {{3D video and free viewpoint video—From capture to display}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320310004450},
volume = {44},
year = {2011}
}
@article{Barnard1983a,
abstract = {Ce papier parle des contraintes perspective: notamment les points de fuite, l'angle etc. pas mal.},
author = {Barnard, S T},
journal = {Artificial Intelligence},
number = {4},
pages = {435--462},
title = {{Interpreting Perspective Images}},
url = {http://dx.doi.org/10.1016/S0004-3702(83)80021-6},
volume = {21},
year = {1983}
}
@inproceedings{Li2003,
abstract = {In this paper, we present a method that integrates cues from shading, shadow and specular reflections for estimating directional illumination in a textured scene. Texture poses a problem for lighting estimation, since texture edges can be mistaken for changes in illumination condition, and unknown variations in albedo make reflectance model fitting impractical. Unlike previous works which all assume known or uniform reflectance, our method can deal with the effects of textures by capitalizing on physical consistencies that exist among the lighting cues. Since scene textures do not exhibit such coherence, we use this property to minimize the influence of texture on illumination direction estimation. For the recovered light source directions, a technique for estimating their intensities in the presence of texture is also proposed.},
address = {Nice, France},
author = {Li, Yuanzhen and Lin, Stephen and Lu, Hanqing and Shum, Heung-yeung},
booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2003.1238649},
isbn = {0-7695-1950-4},
pages = {1366--1373 vol.2},
publisher = {IEEE},
title = {{Multiple-cue illumination estimation in textured scenes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1238649},
year = {2003}
}
@inproceedings{Hammer2013,
abstract = {This paper presents a system for real-time analysis of 3D gaze data arising in mobile applications. Our system allows users to freely move in a known 3D environment while their gaze is computed on arbitrarily shaped objects. The scanpath is analysed fully automatically using fixations and areas-of-interest -- all in 3D and real time. Furthermore, the scanpath can be visualized in parallel in a 3D model of the environment. This enables to observe the scanning behaviour of a subject. We describe how this has been realized for a commercial off-the-shelf mobile eye tracker utilizing an inside-out tracking mechanism for head pose estimation. Moreover, we show examples of real gaze data collected in a museum.},
address = {New York, New York, USA},
author = {Hammer, Jan Hendrik and Maurus, Michael and Beyerer, J\"{u}rgen},
booktitle = {Proceedings of the 2013 Conference on Eye Tracking South Africa - ETSA '13},
doi = {10.1145/2509315.2509333},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2013 Conference on Eye Tracking South Africa - ETSA '13/2013/Hammer, Maurus, Beyerer/Hammer, Maurus, Beyerer - 2013 - Real-time 3D gaze analysis in mobile applications.pdf:pdf},
isbn = {9781450321105},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {75--78},
publisher = {ACM Press},
title = {{Real-time 3D gaze analysis in mobile applications}},
url = {http://dl.acm.org/citation.cfm?id=2509333 http://dl.acm.org/citation.cfm?doid=2509315.2509333},
year = {2013}
}
@article{Hansen2010,
abstract = {Despite active research and significant progress in the last 30 years, eye detection and tracking remains challenging due to the individuality of eyes, occlusion, variability in scale, location, and light conditions. Data on eye location and details of eye movements have numerous applications and are essential in face detection, biometric identification, and particular human-computer interaction tasks. This paper reviews current progress and state of the art in video-based eye detection and tracking in order to identify promising techniques as well as issues to be further addressed. We present a detailed review of recent eye models and techniques for eye detection and tracking. We also survey methods for gaze estimation and compare them based on their geometric properties and reported accuracies. This review shows that, despite their apparent simplicity, the development of a general eye detection technique involves addressing many challenges, requires further theoretical developments, and is consequently of interest to many other domains problems in computer vision and beyond.},
author = {Hansen, Dan Witzner and Ji, Qiang},
doi = {10.1109/TPAMI.2009.30},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Pattern Analysis and Machine Intelligence/2010/Hansen, Ji/Hansen, Ji - 2010 - In the eye of the beholder a survey of models for eyes and gaze.pdf:pdf},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Biological,Eye Movement Measurements,Eye Movements,Eye Movements: physiology,Fixation,Head Movements,Head Movements: physiology,Humans,Models,Ocular,Ocular: physiology,Regression Analysis,eye tracker},
mendeley-tags = {eye tracker},
month = mar,
number = {3},
pages = {478--500},
pmid = {20075473},
title = {{In the eye of the beholder: a survey of models for eyes and gaze.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4770110 http://www.ncbi.nlm.nih.gov/pubmed/20075473},
volume = {32},
year = {2010}
}
@article{Kato2006,
author = {Kato, H. and Naemura, T. and Harashima, H.},
doi = {10.1109/ISMAR.2003.1240733},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings/2006/Kato, Naemura, Harashima/Kato, Naemura, Harashima - 2006 - Graphic shadow augmenting your shadow on the floor.pdf:pdf},
isbn = {0-7695-2006-5},
journal = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
pages = {306--307},
publisher = {IEEE Comput. Soc},
title = {{Graphic shadow: augmenting your shadow on the floor}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240733},
year = {2006}
}
@inproceedings{Chen2008a,
abstract = {This paper proposes a 3D eye gaze estimation and tracking algorithm based on facial feature tracking using a single camera. Instead of using the infrared (IR) lights and the corneal reflections (glint), this algorithm estimates the 3D visual axis using the tracked facial feature points. For this, we first introduce an extended 3D eye model which includes both the eyeball and the eye-corners. Based on this eye model, we derive the equations to solve for the 3D eyeball center, the 3D pupil center and the 3D visual axis, from which we can solve for the point of gaze after a one-time personal calibration. The experimental results show the accuracy of this algorithm is less than 3deg. Compared with the existing IR-based eye tracking methods, the proposed method is simple to setup and can work both indoor and outdoor.},
author = {Chen, Jixu and Ji, Qiang},
booktitle = {2008 19th International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2008.4761343},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2008 19th International Conference on Pattern Recognition/2008/Chen, Ji/Chen, Ji - 2008 - 3D gaze estimation with a single camera without IR illumination.pdf:pdf},
isbn = {978-1-4244-2174-9},
issn = {1051-4651},
keywords = {3d gaze},
mendeley-tags = {3d gaze},
month = dec,
pages = {1--4},
publisher = {IEEE},
title = {{3D gaze estimation with a single camera without IR illumination}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4761343 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4761343},
year = {2008}
}
@article{Salojarvi2005,
abstract = {We organize a PASCAL EU Network of Excellence challenge for inferring relevance from eye movements, beginning 1 March 2005. The aim of this paper is to provide background material for the competitors: give references to related articles on eye movement modelling, describe the methods used for extracting the features used in the challenge, provide results of basic reference methods and to discuss open questions in the field.},
annote = {- cited by: 27
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Saloj\"{a}rvi, J and Puolam\"{a}ki, K and Simola, J and Kovanen, L},
doi = {10.1.1.96.6356},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2005/Saloj\"{a}rvi et al/Saloj\"{a}rvi et al. - 2005 - Inferring relevance from eye movements Feature extraction.pdf:pdf},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
title = {{Inferring relevance from eye movements: Feature extraction}},
url = {http://eprints.pascal-network.org/archive/00000963/},
year = {2005}
}
@book{Szeliski2010,
author = {Szeliski, R},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2010/Szeliski/Szeliski - 2010 - Computer vision Algorithms and applications.pdf:pdf;:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2010/Szeliski/Szeliski - 2010 - Computer vision Algorithms and applications(2).pdf:pdf},
keywords = {Computer Vision,Image Processing},
mendeley-tags = {Computer Vision,Image Processing},
title = {{Computer vision: Algorithms and applications}},
url = {http://books.google.com.br/books?hl=en\&lr=\&id=bXzAlkODwa8C\&oi=fnd\&pg=PR9\&dq=Computer+Vision:+Algorithms+and+Applications\&ots=gY1280pyCD\&sig=LgFpAO\_cJ5JvXpVU70q6IjDiZY0},
year = {2010}
}
@article{Hong2013,
address = {New York, New York, USA},
author = {Hong, Hwajung and Yarosh, Svetlana and Kim, Jennifer G. and Abowd, Gregory D. and Arriaga, Rosa I.},
doi = {10.1145/2470654.2466439},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {3207},
publisher = {ACM Press},
title = {{Investigating the use of circles in social networks to support independence of individuals with autism}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466439},
year = {2013}
}
@article{Xiong2010,
abstract = {Macroscopic and microscopic modeling have become mainstream methodologies for crowd simulation in dynamic environments. The two models make a trade-off between efficiency and accuracy, but neither of them is able to achieve both goals at the same time. With the aim of achieving both efficiency and accuracy, a hybrid modelling method is proposed in this paper for crowd simulation. This paper illustrates how the two types of models co-exist in a single simulation and work collaboratively. A case study for this method is also conducted, the simulation result of which shows that the proposed method can not only benefit from the macroscopic model by improving the simulation efficiency, but also obtain a fine-grained simulation result by adopting the microscopic model.},
author = {Xiong, Muzhou and Lees, Michael and Cai, Wentong and Zhou, Suiping and Low, Malcolm Yoke Hean},
doi = {10.1016/j.procs.2010.04.008},
isbn = {1877-0509},
issn = {18770509},
journal = {Procedia Computer Science},
month = may,
number = {1},
pages = {57--65},
title = {{Hybrid modelling of crowd simulation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050910000098},
volume = {1},
year = {2010}
}
@inproceedings{Pilet2007,
abstract = {We present a non-rigid registration technique that achieves spatial, photometric, and visibility accuracy. It lets us photo-realistically augment 3D deformable surfaces under complex illumination conditions and in spite of severe occlusions. There are many approaches that address some of these issues but very few that simultaneously handle all of them as we do. We use triangulated meshes to model the geometry and introduce explicit visibility maps as well as separate illumination parameters for each mesh vertex. We cast our registration problem in an Expectation Maximization framework that allows robust and fully automated operation. It provides explicit illumination and occlusion models that can be used for rendering purposes.},
address = {Nara-JP},
author = {Pilet, Julien and Lepetit, Vincent and Fua, Pascal},
booktitle = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2007.4538855},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality/2007/Pilet, Lepetit, Fua/Pilet, Lepetit, Fua - 2007 - Retexturing in the Presence of Complex Illumination and Occlusions.pdf:pdf},
isbn = {978-1-4244-1749-0},
keywords = {augmented reality,occlusion,shadows},
month = nov,
pages = {1--8},
publisher = {IEEE},
title = {{Retexturing in the Presence of Complex Illumination and Occlusions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538855},
year = {2007}
}
@article{Hoshang2012,
abstract = {In outdoor rendering, the sun's position, sky color, clouds, shadows, trees and grass are important factors to consider for effective realism. In this paper, three of these are combined. The sun's position is calculated based on specific longitude, latitude, date and time using Julian date; the sky color is created by Perez model and the shadows are generated using a technique called Hybrid Shadow Mapping (HSM). A new method is used to combine these important factors. Another contribution of this paper is a new algorithm to create shadows with higher quality and with higher frames per second when compared with other algorithms such as Layer Variance Shadow Maps and Cascade Shadow Maps. HSM has been tested to justify an increase in the quality and a decrease in the rendering time in a typical application. In this paper, real-time sky color and shadows, with effect of the sun's position are combined to create a realistic outdoor environment without worrying about components such as sky color and shadow position at different times of the day. Commercial games for outdoor rendering are possible beneficiaries of this technique. It could also be advantageous for teachers of physics who teach about earth orbit, and it could be applied in building and architectural designs.},
author = {Hoshang, K and Sunar, M},
journal = {International Journal of Innovative Computing, Information and Control},
keywords = {cascade shadow,shadow mapping,sky color,sun's position,variance shadow},
number = {19},
pages = {7168--7184},
title = {{Real-time outdoor rendering using hybrid shadow maps}},
url = {http://eprints.utm.my/33486/},
volume = {8},
year = {2012}
}
@inproceedings{Bimber2003,
abstract = {We present techniques which create a consistentillumination between real and virtual objects inside anapplication specific optical see-through display: theVirtual Showcase. We use projectors and cameras tocapture reflectance information from diffuse real objectsand to illuminate them under new synthetic lightingconditions. Matching direct and indirect lighting effects,such as shading, shadows, reflections and color bleedingcan be approximated at interactive rates in such acontrolled mixed environment.},
author = {Bimber, O and Grundh\"{o}fer, A},
booktitle = {Proceedings of the 2nd IEEE/ACM International Symposium on Mixed and Augmented Reality},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2nd IEEEACM International Symposium on Mixed and Augmented Reality/2003/Bimber, Grundh\"{o}fer/Bimber, Grundh\"{o}fer - 2003 - Consistent illumination within optical see-through augmented environments.pdf:pdf},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
pages = {198},
title = {{Consistent illumination within optical see-through augmented environments}},
url = {http://dl.acm.org/citation.cfm?id=946789},
year = {2003}
}
@inproceedings{Kinnunen2010,
abstract = {We propose a person authentication system using eye movement signals. In security scenarios, eye-tracking has earlier been used for gaze-based password entry. A few authors have also used physical features of eye movement signals for authentication in a task-dependent scenario with matched training and test samples. We propose and implement a task-independent scenario whereby the training and test samples can be arbitrary. We use short-term eye gaze direction to construct feature vectors which are modeled using Gaussian mixtures. The results suggest that there are personspecific features in the eye movements that can be modeled in a task-independent manner. The range of possible applications extends beyond the security-type of authentication to proactive and user-convenience systems.},
address = {New York, New York, USA},
annote = {- cited by: 13
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Kinnunen, Tomi and Sedlak, Filip and Bednarik, Roman},
booktitle = {Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10},
doi = {10.1145/1743666.1743712},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10/2010/Kinnunen, Sedlak, Bednarik/Kinnunen, Sedlak, Bednarik - 2010 - Towards task-independent person authentication using eye movement signals.pdf:pdf},
isbn = {9781605589947},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {187},
publisher = {ACM Press},
title = {{Towards task-independent person authentication using eye movement signals}},
url = {http://dl.acm.org/citation.cfm?id=1743712 http://portal.acm.org/citation.cfm?doid=1743666.1743712},
year = {2010}
}
@book{Ashby1999,
address = {London},
author = {Ashby, W Ross},
booktitle = {Director},
edition = {2},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Director/1999/Ashby/Ashby - 1999 - An Introduction to Cybernetics.pdf:pdf},
isbn = {0416683002},
pages = {312},
publisher = {Chapman \& Hall Ltd},
title = {{An Introduction to Cybernetics}},
url = {http://pcp.vub.ac.be/books/IntroCyb.pdf},
year = {1999}
}
@article{Regenbrecht2006,
author = {Regenbrecht, H. and Ott, C. and Wagner, M. and Lum, T. and Kohler, P. and Wilke, W. and Mueller, E.},
doi = {10.1109/ISMAR.2003.1240725},
isbn = {0-7695-2006-5},
journal = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
pages = {290--291},
publisher = {IEEE Comput. Soc},
title = {{An augmented virtuality approach to 3D videoconferencing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240725},
year = {2006}
}
@inproceedings{Komogortsev2010,
abstract = {This paper presents a set of qualitative and quantitative scores designed to assess performance of any eye movement classification algorithm. The scores are designed to provide a foundation for the eye tracking researchers to communicate about the performance validity of various eye movement classification algorithms. The paper concentrates on the five algorithms in particular: Velocity Threshold Identification (I-VT), Dispersion Threshold Identification (I-DT), Minimum Spanning Tree Identification (MST), Hidden Markov Model Identification (I-HMM) and Kalman Filter Identification (I-KF). The paper presents an evaluation of the classification performance of each algorithm in the case when values of the input parameters are varied. Advantages provided by the new scores are discussed. Discussion on what is the "best" classification algorithm is provided for several applications. General recommendations for the selection of the input parameters for each algorithm are provided.},
address = {New York, New York, USA},
annote = {- cited by: 15
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Komogortsev, Oleg V and Jayarathna, Sampath and Koh, Do Hyong and Gowda, Sandeep Munikrishne},
booktitle = {Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10},
doi = {10.1145/1743666.1743682},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2010 Symposium on Eye-Tracking Research \& Applications - ETRA '10/2010/Komogortsev et al/Komogortsev et al. - 2010 - Qualitative and quantitative scoring and evaluation of the eye movement classification algorithms.pdf:pdf},
isbn = {9781605589947},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
pages = {65},
publisher = {ACM Press},
title = {{Qualitative and quantitative scoring and evaluation of the eye movement classification algorithms}},
url = {http://dl.acm.org/citation.cfm?id=1743682 http://portal.acm.org/citation.cfm?doid=1743666.1743682},
year = {2010}
}
@book{Birta2013,
author = {Birta, Louis G. and Arbez, Gilbert},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2013/Birta, Arbez/Birta, Arbez - 2013 - Modelling and Simulation Exploring Dynamic System Behaviour (Simulation Foundations, Methods and Applications).pdf:pdf},
isbn = {144712782X},
keywords = {livro,modeling and simulation},
mendeley-tags = {livro,modeling and simulation},
pages = {551},
publisher = {Springer},
title = {{Modelling and Simulation: Exploring Dynamic System Behaviour (Simulation Foundations, Methods and Applications)}},
url = {http://www.amazon.com/Modelling-Simulation-Exploring-Foundations-Applications/dp/144712782X},
year = {2013}
}
@inproceedings{Behzadan2007,
abstract = {Visualization is a powerful method for verifying, validating, and communicating the results of a simulated model. Lack of visual understanding about a simulated model is one of the major reasons inhibiting contractors and engineers from using results obtained from discrete-event simulation to plan and design their construction processes and commit real resources on the job site. The fast emerging information technology makes the use of modern visualization applications more appealing to engineers and scientists in different domains. This paper presents the design and implementation of an augmented reality (AR) visualization application together with an authoring language that allows the creation of outdoor AR animated scenes of simulated operations while featuring complete user involvement and interaction. The application is based on the concept of scene graphs. It also uses a unique general purpose data transmission method to communicate with hardware components of the system.},
author = {Behzadan, Amir H and Kamat, Vineet R},
booktitle = {2007 Winter Simulation Conference},
doi = {10.1109/WSC.2007.4419851},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2007 Winter Simulation Conference/2007/Behzadan, Kamat/Behzadan, Kamat - 2007 - Enabling smooth and scalable dynamic 3D visualization of discrete-event construction simulations in outdoor aug.pdf:pdf},
isbn = {978-1-4244-1305-8},
month = dec,
pages = {2168--2176},
publisher = {IEEE},
title = {{Enabling smooth and scalable dynamic 3D visualization of discrete-event construction simulations in outdoor augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4419851},
year = {2007}
}
@inproceedings{Li2006a,
abstract = {Eye tracking has long held the promise of being a useful methodology for human computer interaction. However, a number of barriers have stood in the way of the integration of eye tracking into everyday applications, including the intrusiveness, robustness, availability, and price of eye-tracking systems. To lower these barriers, we have developed the openEyes system. The system consists of an open-hardware design for a digital eye tracker that can be built from low-cost off-the-shelf components, and a set of open-source software tools for digital image capture, manipulation, and analysis in eye-tracking applications. We expect that the availability of this system will facilitate the development of eye-tracking applications and the eventual integration of eye tracking into the next generation of everyday human computer interfaces. We discuss the methods and technical challenges of low-cost eye tracking as well as the design decisions that produced our current system.},
address = {New York, New York, USA},
author = {Li, Dongheng and Babcock, Jason and Parkhurst, Derrick J.},
booktitle = {Proceedings of the 2006 symposium on Eye tracking research \& applications - ETRA '06},
doi = {10.1145/1117309.1117350},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2006 symposium on Eye tracking research \& applications - ETRA '06/2006/Li, Babcock, Parkhurst/Li, Babcock, Parkhurst - 2006 - openEyes a low-cost head-mounted eye-tracking solution.pdf:pdf},
isbn = {1595933050},
keywords = {eye tracker},
mendeley-tags = {eye tracker},
pages = {95},
publisher = {ACM Press},
title = {{openEyes: a low-cost head-mounted eye-tracking solution}},
url = {http://dl.acm.org/citation.cfm?id=1117350 http://portal.acm.org/citation.cfm?doid=1117309.1117350},
year = {2006}
}
@inproceedings{Legg2011,
abstract = {In this paper we investigate the challenge of 3D reconstruction from Snooker video data. We propose a system pipeline for intelligent filtering based on semantic importance in Snooker. The system can be divided into table detection and correction, followed by ball detection, classification and tracking. It is apparent from previous work that there are several challenges presented here. Firstly, previous methods tend to use a fixed top-down camera mounted above the table. To capture a full table view from this is challenging due to space limitations above the table. Instead, we capture video data from a tripod and correct the viewpoint through processing. Secondly, previous methods tend to simply detect the balls without considering other interfering objects such as player and cue. This becomes even more apparent when the player strikes the cue ball. Our intelligent filtering avoids such issues to give accurate 3D table reconstruction.},
author = {Legg, Philip A. and Parry, Matthew L. and Chung, David H. S. and Jiang, Richard M. and Morris, Adrian and Griffiths, Iwan W. and Marshall, David and Chen, Min},
booktitle = {2011 18th IEEE International Conference on Image Processing},
doi = {10.1109/ICIP.2011.6116122},
isbn = {978-1-4577-1303-3},
issn = {1522-4880},
keywords = {3D table reconstruction,Cameras,Computer vision,Conferences,Image color analysis,Image reconstruction,Snooker video,Three dimensional displays,ball classification,ball detection,ball tracking,filtering theory,fixed top-down camera,image analysis,image processing,intelligent filtering,morphological operations,object detection,object recognition,object tracking,reconstruction,semantic importance,single-view 3D reconstruction,table detection,video signal processing},
mendeley-tags = {reconstruction},
month = sep,
pages = {2385--2388},
publisher = {IEEE},
shorttitle = {Image Processing (ICIP), 2011 18th IEEE Internatio},
title = {{Intelligent filtering by semantic importance for single-view 3D reconstruction from Snooker video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6116122},
year = {2011}
}
@inproceedings{Marin2008,
abstract = {In this study we have compared the subjective effect of distortions simulated ophthalmic lenses in a virtual lens simulator to the equivalent real ophthalmic test lenses, in static monocular and dynamic, monocular and binocular conditions, taking care of matching as best as possible virtual and real conditions. Though visual perception was found to be similar in static condition, distortions were judged to be exaggerated by the virtual lenses in dynamic conditions.},
address = {New York, New York, USA},
author = {Marin, Gildas and Terrenoire, Edith and Hernandez, Martha},
booktitle = {Proceedings of the 2008 ACM symposium on Virtual reality software and technology - VRST '08},
doi = {10.1145/1450579.1450648},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the 2008 ACM symposium on Virtual reality software and technology - VRST '08/2008/Marin, Terrenoire, Hernandez/Marin, Terrenoire, Hernandez - 2008 - Compared distortion effects between real and virtual ophthalmic lenses with a simulator.pdf:pdf},
isbn = {9781595939517},
pages = {271},
publisher = {ACM Press},
title = {{Compared distortion effects between real and virtual ophthalmic lenses with a simulator}},
url = {http://portal.acm.org/citation.cfm?id=1450648 http://portal.acm.org/citation.cfm?doid=1450579.1450648},
year = {2008}
}
@article{Sko2013,
address = {New York, New York, USA},
author = {Sko, Torben and Gardner, Henry J. and Martin, Michael},
doi = {10.1145/2470654.2481288},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Classification,Decision Trees,Games,Non-parametric,Online Studies,Parametric,Regression},
pages = {2103},
publisher = {ACM Press},
title = {{Non-parametric decision trees and online HCI}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481288},
year = {2013}
}
@inproceedings{Akpan2005,
abstract = {This paper presents the results from surveying simulation practitioners from industry and academics who have used 2D or 3D software applications for discrete event simulation (DES) projects. The survey focused on the impacts of virtual reality (VR) on DES activities. The findings indicate the software used, the applications areas, the stages in the simulation modeling process where visual display is commonly used, and a comparative evaluation of the benefits and costs associated with modeling in 3D over 2D. Other results indicate possible influence of each of the two displays on simulation results, effects on users' understanding of the modeled system and any corresponding influence on decision making. The findings also incorporate the pitfalls to avoid when modeling in 3D, and speculations about the future of VR-based DES (VRSIM) practice.},
author = {Akpan, JI and Brooks, RJ},
booktitle = {Proceedings of the Winter Simulation Conference, 2005.},
doi = {10.1109/WSC.2005.1574476},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the Winter Simulation Conference, 2005/2005/Akpan, Brooks/Akpan, Brooks - 2005 - Practitioners' Perception of the Impacts of Virtual Reality on Discrete-event Simulation.pdf:pdf},
isbn = {0-7803-9519-0},
pages = {1976--1984},
publisher = {IEEE},
title = {{Practitioners' Perception of the Impacts of Virtual Reality on Discrete-event Simulation}},
url = {http://eprints.lancs.ac.uk/30641/ http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1574476},
year = {2005}
}
@book{Koch2011,
address = {Berlin, Heidelberg},
annote = {crossRef(Jarodzka2010)},
booktitle = {Computer Vision – ACCV 2010 Workshops},
doi = {10.1007/978-3-642-22822-3},
editor = {Koch, Reinhard and Huang, Fay},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer Vision – ACCV 2010 Workshops/2011/Unknown/Unknown - 2011 - Computer Vision – ACCV 2010 Workshops.pdf:pdf},
isbn = {978-3-642-22821-6},
keywords = {eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Computer Vision – ACCV 2010 Workshops}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-22822-3},
volume = {6468},
year = {2011}
}
@inproceedings{Drettakis1997,
abstract = {The advent of computer augmented reality (CAR), in which computer generated objects mix with real video images, has resulted in many interesting new application domains. Providing common illumination between the real and synthetic objects can be very beneficial, since the additional visual cues (shadows, interreflections etc.) are critical to seamless real-synthetic world integration. Building on recent advances in computer graphics and computer vision, we present a new framework to resolving this problem. We address three specific aspects of the common illumination problem for CAR: (a) simplification of camera calibration and modeling of the real scene; (b) efficient update of illumination for moving CG objects and (c) efficient rendering of the merged world. A first working system is presented for a limited sub-problem: a static real scene and camera with moving CG objects. Novel advances in computer vision are used for camera calibration and user-friendly modeling of the real scene, a recent interactive radiosity update algorithm is adapted to provide fast illumination update and finally textured polygons are used for display. This approach allows interactive update rates on mid-range graphics workstations. Our new framework will hopefully lead to CAR systems with interactive common illumination without restrictions on the movement of real or synthetic objects, lights and cameras.},
author = {Drettakis, G and Robert, L and Bougnoux, S},
booktitle = {Rendering Techniques' 97},
doi = {10.1007/978-3-7091-6858-5\_5},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Rendering Techniques' 97/1997/Drettakis, Robert, Bougnoux/Drettakis, Robert, Bougnoux - 1997 - Interactive common illumination for computer augmented reality.pdf:pdf},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
pages = {45--56},
publisher = {Springer},
title = {{Interactive common illumination for computer augmented reality}},
url = {http://link.springer.com/chapter/10.1007/978-3-7091-6858-5\_5},
year = {1997}
}
@article{Nancel2013,
address = {New York, New York, USA},
author = {Nancel, Mathieu and Chapuis, Olivier and Pietriga, Emmanuel and Yang, Xing-Dong and Irani, Pourang P. and Beaudouin-Lafon, Michel},
doi = {10.1145/2470654.2470773},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Handheld Devices,Pointing,Wall Displays},
pages = {831},
publisher = {ACM Press},
title = {{High-precision pointing on large wall displays using small handheld devices}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470773},
year = {2013}
}
@inproceedings{Bernstein2013,
abstract = {When you share content in an online social network, who is listening? Users have scarce information about who actually sees their content, making their audience seem invisible and difficult to estimate. However, understanding this invisible audience can impact both science and design, since perceived audiences influence content production and self-presentation online. In this paper, we combine survey and large-scale log data to examine how well users' perceptions of their audience match their actual audience on Facebook. We find that social media users consistently underestimate their audience size for their posts, guessing that their audience is just 27\% of its true size. Qualitative coding of survey responses reveals folk theories that attempt to reverse-engineer audience size using feedback and friend count, though none of these approaches are particularly accurate. We analyze audience logs for 222,000 Facebook users' posts over the course of one month and find that publicly visible signals --- friend count, likes, and comments --- vary widely and do not strongly indicate the audience of a single post. Despite the variation, users typically reach 61\% of their friends each month. Together, our results begin to reveal the invisible undercurrents of audience attention and behavior in online social networks.},
address = {New York, New York, USA},
author = {Bernstein, Michael S. and Bakshy, Eytan and Burke, Moira and Karrer, Brian},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470658},
isbn = {9781450318990},
keywords = {Social networks,audience,information distribution},
pages = {21--30},
publisher = {ACM Press},
title = {{Quantifying the invisible audience in social networks}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470658},
year = {2013}
}
@article{Munn2009,
abstract = {Video-based eye trackers produce an output video showing where a subject is looking, the subject's Point-of-Regard (POR), for each frame of a video of the scene. This information can be extremely valuable, but its analysis can be overwhelming. Analysis of eye-tracked data from portable (wearable) eye trackers is especially daunting, as the scene video may be constantly changing, rendering automatic analysis more difficult. A common way to begin analysis of POR data is to group these data into fixations. In a previous article, we compared the fixations identified (i.e., start and end marked) automatically by an algorithm to those identified manually by users (i.e., manual coders). Here, we extend this automatic identification of fixations to tagging each fixation to a Region-of-Interest (ROI). Our fixation tagging algorithm, FixTag, requires the relative 3D positions of the vertices of ROIs and calibration of the scene camera. Fixation tagging is performed by first calculating the camera projection matrices for keyframes of the scene video (captured by the eye tracker) via an iterative structure and motion recovery algorithm. These matrices are then used to project 3D ROI vertices into the keyframes. A POR for each fixation is matched to a point in the closest keyframe, which is then checked against the 2D projected ROI vertices for tagging. Our fixation tags were compared to those produced by three manual coders tagging the automatically identified fixations for two different scenarios. For each scenario, eight ROIs were defined along with the 3D positions of eight calibration points. Therefore, 17 tags were available for each fixation: 8 for ROIs, 8 for calibration points, and 1 for “other.” For the first scenario, a subject was tracked looking through products on four store shelves, resulting in 182 automatically identified fixations. Our automatic tagging algorithm produced tags that matched those produced by at least one manual coder for 181 out of the 182 fixations (99.5\% agreement). For the second scenario, a subject was tracked looking at two posters on adjoining walls of a room. Our algorithm matched at least one manual coder's tag for 169 fixations out of 172 automatically identified (98.3\% agreement).},
annote = {- cited by: 8
- kw: identification fixation eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Munn, Susan M. and Pelz, Jeff B.},
doi = {10.1145/1577755.1577759},
file = {:home/acmt/Dropbox/Documentos/Mendeley/ACM Transactions on Applied Perception/2009/Munn, Pelz/Munn, Pelz - 2009 - FixTag An algorithm for identifying and tagging fixations to simplify the analysis of data collected by portable eye.pdf:pdf},
issn = {15443558},
journal = {ACM Transactions on Applied Perception},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = aug,
number = {3},
pages = {1--25},
title = {{FixTag: An algorithm for identifying and tagging fixations to simplify the analysis of data collected by portable eye trackers}},
url = {http://dl.acm.org/citation.cfm?id=1577759 http://portal.acm.org/citation.cfm?doid=1577755.1577759},
volume = {6},
year = {2009}
}
@article{Blinn1982,
abstract = {The study of the physical process of light interacting with matter is an important part of computer image synthesis since it forms the basis for calculations of intensities in the picture. The simpler models used in the past are being augmented by more complex models gleaned from the physics literature. This paper is another step in the direction of assimilating such knowledge. It concerns the statistical simulation of light passing through and being reflected by clouds of similar small particles. (It does not, however, address the cloud structure modeling problem). By extension it can be applied to surfaces completely covered by dust and is therefore a physical basis for various theories of diffuse reflection.},
author = {Blinn, James F.},
isbn = {0897910761},
journal = {Computer Graphics},
keywords = {Computer Graphics,Graphics and Realism,Three-Dimensional},
number = {3},
pages = {21--29},
title = {{Light Reflection Functions for Simulation of Clouds and Dusty Surfaces}},
volume = {16},
year = {1982}
}
@article{Mohr_Triggs_1996,
author = {Mohr, Roger and Triggs, Bill and {Roger Mohr}, Bill Triggs},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Int Symp Photogrammetry and Remote Sensing/1996/Mohr, Triggs, Roger Mohr/Mohr, Triggs, Roger Mohr - 1996 - Projective Geometry for Image Analysis.pdf:pdf},
journal = {Int Symp Photogrammetry and Remote Sensing},
number = {July},
publisher = {Citeseer},
title = {{Projective Geometry for Image Analysis}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.3924 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3924\&rep=rep1\&type=pdf},
year = {1996}
}
@inproceedings{Peinecke2007,
annote = {        From Duplicate 1 (                           Generating Tactile Textures using Periodicity Analysis                         - Peinecke, Niklas; Allerkamp, Dennis; Wolter, Franz-erich )
                
        
        
      },
author = {Peinecke, Niklas and Allerkamp, Dennis and Wolter, Franz-erich},
booktitle = {2007 International Conference on Cyberworlds (CW'07)},
doi = {10.1109/CW.2007.50},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2007 International Conference on Cyberworlds (CW'07)/2007/Peinecke, Allerkamp, Wolter/Peinecke, Allerkamp, Wolter - 2007 - Generating Tactile Textures using Periodicity Analysis.pdf:pdf;:home/acmt/Dropbox/Documentos/Mendeley/2007 International Conference on Cyberworlds (CW'07)/2007/Peinecke, Allerkamp, Wolter/Peinecke, Allerkamp, Wolter - 2007 - Generating Tactile Textures using Periodicity Analysis(2).pdf:pdf},
isbn = {0-7695-3005-2},
month = oct,
pages = {308--313},
publisher = {IEEE},
title = {{Generating Tactile Textures using Periodicity Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4390934},
year = {2007}
}
@inproceedings{Kan2013,
abstract = {Fast and realistic synthesis of real videos with computer generated content has been a challenging problem in computer graphics. It involves computationally expensive light transport calculations. We present a novel and efficient algorithm for diffuse light transport calculation between virtual and real worlds called Differential Irradiance Caching. Our algorithm produces a high-quality result while preserving interactivity and allowing dynamic geometry, materials, lighting, and camera movement. The problem of expensive differential irradiance evaluation is solved by exploiting the spatial coherence in indirect illumination using irradiance caching. We enable multiple bounces of global illumination by using Monte Carlo integration in GPU ray-tracing to evaluate differential irradiance at irradiance cache records in one pass. The combination of ray-tracing and rasterization is used in an extended irradiance cache splatting algorithm to provide a fast GPU-based solution of indirect illumination. Limited information stored in the irradiance splat buffer causes errors for pixels on edges in case of depth of field rendering. We propose a solution to this problem using a reprojection technique to access the irradiance splat buffer. A novel cache miss detection technique is introduced which allows for a linear irradiance cache data structure. We demonstrate the integration of differential irradiance caching into a rendering framework for Mixed Reality applications capable of simulating complex global illumination effects.},
author = {Kan, Peter and Kaufmann, Hannes},
booktitle = {2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2013.6671773},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)/2013/Kan, Kaufmann/Kan, Kaufmann - 2013 - Differential Irradiance Caching for fast high-quality light transport between virtual and real worlds.pdf:pdf},
isbn = {978-1-4799-2869-9},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = oct,
pages = {133--141},
publisher = {IEEE},
title = {{Differential Irradiance Caching for fast high-quality light transport between virtual and real worlds}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6671773 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6671773},
year = {2013}
}
@article{Nicholson2013,
address = {New York, New York, USA},
author = {Nicholson, James and Coventry, Lynne and Briggs, Pam},
doi = {10.1145/2470654.2470701},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {323--332},
publisher = {ACM Press},
title = {{Age-related performance issues for PIN and face-based authentication systems}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470701},
year = {2013}
}
@inproceedings{Chabbi,
abstract = {The planar surfaces (3-D faces) of the objects of a polyhedral scene are built using a trinocular stereovision system. Before the reconstruction step and after the step of matching, a search is conducted among the set of triplets of matched 2-D faces (supposed to be the projection of 3-D faces) for the triplets which really correspond to planar 3-D surfaces. To accurately reconstruct the surface, a check is conducted in 2-D space for the classes of triplets of faces which correspond to 3-D coplanar structures. It is shown how these 3-D properties are checked in the 2-D space, using tools of projective geometry, allowing the constraint of the 3-D reconstruction in order to maintain these 3-D properties},
author = {Chabbi, H. and Berger, M.-O.},
booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.1993.341045},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of IEEE Conference on Computer Vision and Pattern Recognition/Unknown/Chabbi, Berger/Chabbi, Berger - Unknown - Recovering planar surfaces by stereovision based on projective geometry.pdf:pdf},
isbn = {0-8186-3880-X},
pages = {649--650},
publisher = {IEEE Comput. Soc. Press},
title = {{Recovering planar surfaces by stereovision based on projective geometry}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=341045\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=.QT.projective+geometry.QT.}
}
@article{Bohme2006,
author = {Bohme, M and Meyer, A},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proc. of the 2006 \ldots/2006/Bohme, Meyer/Bohme, Meyer - 2006 - Remote eye tracking State of the art and directions for future development.pdf:pdf},
journal = {Proc. of the 2006  \ldots},
keywords = {eye tracker},
mendeley-tags = {eye tracker},
title = {{Remote eye tracking: State of the art and directions for future development}},
url = {http://www.inb.uni-luebeck.de/publikationen/pdfs/BoMeMaBa06.pdf},
year = {2006}
}
@inproceedings{Gunduz2013,
abstract = {Extraction of crowd dynamics from video is the fundamental step for automatic detection of abnormal events. However, it is difficult to obtain sufficient performance with object tracking due to occlusions and insufficient resolution of the objects in the scene. As a result, optical flow or feature tracking methods are preferred in crowd videos. These applications also require algorithms to work in real-time. In this work, we investigated the applicability and performance of feature detection and tracking algorithms in crowd videos. The algorithms that were tested in this paper include Scale Invariant Feature Transform (SIFT) and Speeded-Up Robust Features (SURF) as well as relatively newer approaches Binary Robust Independent Elementary Features (BRIEF) and Oriented Fast and Rotated Brief (ORB). These algorithms have been tested with videos having different crowd densities and comparative results of their accuracy and computational performance have been reported. The results show that BRIEF is computationally faster than the others, allowing real-time operation, and comparable with other algorithms regarding matching accuracy.},
author = {Gunduz, Ayse Elvan and Temizel, Alptekin and {Taskaya Temizel}, Tugba},
booktitle = {2013 21st Signal Processing and Communications Applications Conference (SIU)},
doi = {10.1109/SIU.2013.6531572},
isbn = {978-1-4673-5563-6},
keywords = {BRIEF,ORB,SIFT,SURF,Video surveillance,abnormal events,binary robust independent elementary features,computer vision,crowd density,crowd dynamics,crowd videos,feature detection,feature extraction,feature matching,feature tracking methods,hidden feature removal,image matching,object tracking,occlusions,optical flow,oriented fast and rotated brief,scale invariant feature transform,speeded-up robust features,video signal processing,video surveillance},
mendeley-tags = {crowd dynamics,feature detection},
month = apr,
pages = {1--4},
publisher = {IEEE},
shorttitle = {Signal Processing and Communications Applications },
title = {{Feature detection and tracking for extraction of crowd dynamics}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6531572},
year = {2013}
}
@inproceedings{Samaras2002,
abstract = {We present a new method for the detection and estimation of multiple directional illuminants, using a single image of any object with known geometry and Lambertian reflectance. We use the resulting highly accurate estimates to modify virtually the illumination and geometry of a real scene and produce correctly illuminated mixed reality images. Our method obviates the need to modify the imaged scene by inserting calibration objects of any particular geometry, relying instead on partial knowledge of the geometry of the scene. Thus, the recovered multiple illuminants can be used both for image-based rendering and for shape reconstruction. Our method combines information both from the shading of the object and from shadows cast on the scene by the object. Initially we use a method based on shadows and a method based on shading independently. The shadow based method utilizes brightness variation inside the shadows cast by the object, whereas the shading based method utilizes brightness variation on the directly illuminated portions of the object. We demonstrate how the two sources of information complement each other in a number of occasions. We then describe an approach that integrates the two methods, with results superior to those obtained if the two methods are used separately. The resulting illumination information can be used (i) to render synthetic objects in a real photograph with correct illumination effects, and (ii) to virtually re-light the scene.},
author = {Samaras, D.},
booktitle = {10th Pacific Conference on Computer Graphics and Applications, 2002. Proceedings.},
doi = {10.1109/PCCGA.2002.1167837},
isbn = {0-7695-1784-6},
pages = {38--47},
publisher = {IEEE Comput. Soc},
title = {{Estimation of multiple directional light sources for synthesis of mixed reality images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1167837},
year = {2002}
}
@inproceedings{Stellmach2013,
abstract = {We investigate how to seamlessly bridge the gap between users and distant displays for basic interaction tasks, such as object selection and manipulation. For this, we take advantage of very fast and implicit, yet imprecise gaze- and head-directed input in combination with ubiquitous smartphones for additional manual touch control. We have carefully elaborated two novel and consistent sets of gaze-supported interaction techniques based on touch-enhanced gaze pointers and local magnification lenses. These conflict-free sets allow for fluently selecting and positioning distant targets. Both sets were evaluated in a user study with 16 participants. Overall, users were fastest with a touch-enhanced gaze pointer for selecting and positioning an object after some training. While the positive user feedback for both sets suggests that our proposed gaze- and head-directed interaction techniques are suitable for a convenient and fluent selection and manipulation of distant targets, further improvements are necessary for more precise cursor control.},
address = {New York, New York, USA},
author = {Stellmach, Sophie and Dachselt, Raimund},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470695},
isbn = {9781450318990},
pages = {285--294},
publisher = {ACM Press},
title = {{Still looking: investigating seamless gaze-supported selection, positioning, and manipulation of distant targets}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470695},
year = {2013}
}
@article{Sheng2011,
abstract = {We present an application of interactive global illumination and spatially augmented reality to architectural daylight modeling that allows designers to explore alternative designs and new technologies for improving the sustainability of their buildings. Images of a model in the real world, captured by a camera above the scene, are processed to construct a virtual 3D model. To achieve interactive rendering rates, we use a hybrid rendering technique, leveraging radiosity to simulate the interreflectance between diffuse patches and shadow volumes to generate per-pixel direct illumination. The rendered images are then projected on the real model by four calibrated projectors to help users study the daylighting illumination. The virtual heliodon is a physical design environment in which multiple designers, a designer and a client, or a teacher and students can gather to experience animated visualizations of the natural illumination within a proposed design by controlling the time of day, season, and climate. Furthermore, participants may interactively redesign the geometry and materials of the space by manipulating physical design elements and see the updated lighting simulation.},
author = {Sheng, Yu and Yapo, Theodore C and Young, Christopher and Cutler, Barbara},
doi = {10.1109/TVCG.2009.209},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE transactions on visualization and computer graphics/2011/Sheng et al/Sheng et al. - 2011 - A spatially augmented reality sketching interface for architectural daylighting design.pdf:pdf},
issn = {1941-0506},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Algorithms,Computer Graphics,Computer-Aided Design,Computer-Assisted,Computer-Assisted: methods,Environment Design,Humans,Image Interpretation,Image Processing,Imaging,Lighting,Three-Dimensional,Three-Dimensional: methods,User-Computer Interface,anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
month = jan,
number = {1},
pages = {38--50},
pmid = {21071786},
title = {{A spatially augmented reality sketching interface for architectural daylighting design.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5342415 http://www.ncbi.nlm.nih.gov/pubmed/21071786},
volume = {17},
year = {2011}
}
@article{Papadopoulos2013,
abstract = {In this paper, a gaze-based Relevance Feedback (RF) approach to region-based image retrieval is presented. Fundamental idea of the proposed method comprises the iterative estimation of the real-world objects (or their constituent parts) that are of interest to the user and the subsequent exploitation of this information for refining the image retrieval results. Primary novelties of this work are: a) the introduction of a new set of gaze features for realizing user’s relevance assessment prediction at region-level, and b) the design of a time-efficient and effective object-based RF framework for image retrieval. Regarding the interpretation of the gaze signal, a novel set of features is introduced by formalizing the problem under a mathematical perspective, contrary to the exclusive use of explicitly defined features that are in principle derived from the psychology domain. Apart from the temporal attributes, the proposed features also represent the spatial characteristics of the gaze signal, which have not been extensively studied in the literature so far. On the other hand, the developed object-based RF mechanism aims at overcoming the main limitation of region-based RF approaches, i.e. the frequently inaccurate estimation of the regions of interest in the retrieved images. Moreover, the incorporation of a single-camera image processing-based gaze tracker makes the overall system cost efficient and portable. As it is shown by the experimental evaluation, the proposed method outperforms representative global- and region-based explicit RF approaches, using a challenging general-purpose image dataset.},
annote = {- cited by: 0
- kw: identification I-DT eye
- engine: Google Scholar
- crossref: inversed Salvucci2000},
author = {Papadopoulos, G. and Apostolakis, K. and Daras, P.},
doi = {10.1109/TMM.2013.2291535},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Multimedia/2013/Papadopoulos, Apostolakis, Daras/Papadopoulos, Apostolakis, Daras - 2013 - Gaze-Based Relevance Feedback for Realizing Region-Based Image Retrieval.pdf:pdf},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {Relevance feedback,gaze analysis,gaze-tracking,image retrieval},
mendeley-tags = {gaze analysis},
number = {99},
pages = {1},
shorttitle = {Multimedia, IEEE Transactions on},
title = {{Gaze-Based Relevance Feedback for Realizing Region-Based Image Retrieval}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6671553\&tag=1},
volume = {PP},
year = {2013}
}
@article{Xu2013a,
address = {New York, New York, USA},
author = {Xu, Qianli and Li, Liyuan and Wang, Gang},
doi = {10.1145/2470654.2481308},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2233},
publisher = {ACM Press},
title = {{Designing engagement-aware agents for multiparty conversations}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481308},
year = {2013}
}
@inproceedings{Livingston2003,
abstract = {A useful function of augmented reality (AR) systems is their ability to visualize occluded infrastructure directly in a user's view of the environment. This is especially important for our application context, which utilizes mobile AR for navigation and other operations in an urban environment. A key problem in the AR field is how to best depict occluded objects in such a way that the viewer can correctly infer the depth relationships between different physical and virtual objects. Showing a single occluded object with no depth context presents an ambiguous picture to the user. But showing all occluded objects in the environments leads to the "Superman's X-ray vision" problem, in which the user sees too much information to make sense of the depth relationships of objects. Our efforts differ qualitatively from previous work in AR occlusion, because our application domain involves far-field occluded objects, which are tens of meters distant from the user. Previous work has focused on near-field occluded objects, which are within or just beyond arm's reach, and which use different perceptual cues. We designed and evaluated a number of sets of display attributes. We then conducted a user study to determine which representations best express occlusion relationships among far-field objects. We identify a drawing style and opacity settings that enable the user to accurately interpret three layers of occluded objects, even in the absence of perspective constraints.},
address = {Tokyo, Japan},
author = {Livingston, MA and Swan, J.E. and Gabbard, J.L. and Hollerer, T.H. and Hix, D. and Julier, S.J. and Baillot, Y. and Brown, D.},
booktitle = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
doi = {10.1109/ISMAR.2003.1240688},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings/2003/Livingston et al/Livingston et al. - 2003 - Resolving multiple occluded layers in augmented reality.pdf:pdf},
isbn = {0-7695-2006-5},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {56--65},
publisher = {IEEE Comput. Soc},
title = {{Resolving multiple occluded layers in augmented reality}},
url = {http://dl.acm.org/citation.cfm?id=946796 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240688},
year = {2003}
}
@inproceedings{Weber2001,
author = {Weber, M and Cipolla, R},
booktitle = {Procedings of the British Machine Vision Conference 2001},
doi = {10.5244/C.15.49},
isbn = {1-901725-16-2},
pages = {49.1--49.10},
publisher = {British Machine Vision Association},
title = {{A Practical Method for Estimation of Point Light-Sources}},
url = {http://www.bmva.org/bmvc/2001/papers/117/index.html},
year = {2001}
}
@inproceedings{Brito2013,
author = {Brito, Jose Henrique and Angst, Roland and Koser, Kevin and Pollefeys, Marc},
booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.180},
isbn = {978-0-7695-4989-7},
month = jun,
pages = {1368--1375},
publisher = {IEEE},
title = {{Radial Distortion Self-Calibration}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619024},
year = {2013}
}
@inproceedings{Leung2009,
abstract = {In this paper, we proposed a movable hand-held display system which uses a projector to project display content onto an ordinary cardboard which can move freely within the projection area. Such a system can give users greater freedom of control of the display such as the viewing angle and distance. At the same time, the size of the cardboard can be made to a size that fits one's application. A projector-camera pair is calibrated and used as the tracking and projection system. We present a vision based algorithm to detect an ordinary cardboard and track its subsequent motion. Display content is then pre-warped and projected onto the cardboard at the correct position. Experimental results show that our system can project onto the cardboard in reasonable precision.},
address = {Miami, FL},
author = {Chang, M.M.Y.},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206658},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2009 IEEE Conference on Computer Vision and Pattern Recognition/2009/Chang/Chang - 2009 - A projector-based movable hand-held display system.pdf:pdf},
isbn = {978-1-4244-3992-8},
keywords = {anamorphism,display content,keystone,movable handheld display system,ordinary cardboard,projector-camera pair},
mendeley-tags = {anamorphism,keystone},
month = jun,
pages = {1109--1114},
publisher = {IEEE},
title = {{A projector-based movable hand-held display system}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5206658 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206658},
year = {2009}
}
@inproceedings{Model2012,
abstract = {Stereo-camera Remote Eye-Gaze Tracking (REGT) systems can provide calibration-free estimation of gaze. However, such systems have a limited tracking range due to the requirement for the eye to be tracked in both cameras. This paper presents a general framework for extension of a tracking range of stereo-camera user-calibration-free REGT systems. The proposed method consists of two distinct phases. In the brief initial phase, estimates of eye-features [the center of the pupil and corneal reflections] in pairs of stereo-images are used to estimate automatically a set of subject-specific eye parameters. In the second phase, these subject-specific eye parameters are used with estimates of eye-features in images from any one of the systems' cameras to compute the Point-of-Gaze (PoG). Experiments were conducted with a system that includes two cameras in a horizontal plane. The experimental results demonstrate that the tracking range for horizontal gaze directions can be extended by more than 50\%: from ±23.2° when the two cameras are used as a stereo pair to ±35.5° when the two cameras are used independently to estimate the PoG. By adding more cameras to the system, the proposed framework allows further extension of the tracking range in both horizontal and vertical direction, while preserving a user-calibration-free status of a REGT system.},
address = {New York, New York, USA},
author = {Model, Dmitri and Eizenman, Moshe},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12},
doi = {10.1145/2168556.2168609},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12/2012/Model, Eizenman/Model, Eizenman - 2012 - A general framework for extension of a tracking range of user-calibration-free remote eye-gaze tracking systems.pdf:pdf},
isbn = {9781450312219},
keywords = {calibration-free,distributed eye-gaze tracker,extended range,eye tracker,eye tracking,remote gaze estimation,toshi},
mendeley-tags = {eye tracker,toshi},
month = mar,
pages = {253},
publisher = {ACM Press},
title = {{A general framework for extension of a tracking range of user-calibration-free remote eye-gaze tracking systems}},
url = {http://dl.acm.org/citation.cfm?id=2168556.2168609},
year = {2012}
}
@incollection{Bevington2002,
author = {Bevington, Philip},
booktitle = {Data Reduction and Error Analysis for the Physical Sciences},
chapter = {6},
title = {{Least-Squares Fit to a Straight Line}},
year = {2002}
}
@article{Butz1999,
author = {Butz, A and H\"{o}llerer, T and Feiner, S and MacIntyre, B and Beshers, C},
doi = {10.1109/IWAR.1999.803804},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality IWAR99/1999/Butz et al/Butz et al. - 1999 - Enveloping users and computers in a collaborative 3D augmented reality.pdf:pdf},
isbn = {0769503594},
journal = {Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality IWAR99},
pages = {35--44},
publisher = {IEEE Comput. Soc},
title = {{Enveloping users and computers in a collaborative 3D augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=803804},
volume = {99},
year = {1999}
}
@article{Nadeau1999,
abstract = {VRML makes it easy to create virtual worlds. The article reviews VRML's syntax and features as well as its world construction and animation abilities. In 1995, VRML 1.0 began primarily as a shape description language. The following year, VRML 2.0 redefined the language, extending it to support sound, fog, backgrounds, animation, and user interaction. Today, the designers of VRML are in the midst of designing the next generation of VRML, code-named “VRML NG”. A first draft specification should be ready in early 1999, with a final specification and VRML browser implementations by the end of the year},
author = {Nadeau, D.R.},
doi = {10.1109/38.749119},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Computer Graphics And Applications/1999/Nadeau/Nadeau - 1999 - Building virtual worlds with VRML.pdf:pdf},
isbn = {0471165077},
issn = {02721716},
journal = {IEEE Computer Graphics And Applications},
number = {2},
pages = {18--29},
title = {{Building virtual worlds with VRML}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=749119},
volume = {19},
year = {1999}
}
@book{Ali2013,
address = {New York},
annote = {Global Navigation:
- Grafos de Visibilidade
- M\'{e}todos Probabil\'{\i}sticos
- },
author = {Ali, Saad and Nishino, Ko},
editor = {Manocha, Dinesh and Shah, Murabak},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2013/Unknown/Unknown - 2013 - Modeling, Simulation and Visual Analysis of Crowds A Multidisciplinary Perspective (The International Series in Video C.pdf:pdf},
isbn = {1461484820},
keywords = {crowd simulation,livro},
mendeley-tags = {crowd simulation,livro},
pages = {430},
publisher = {Springer},
title = {{Modeling, Simulation and Visual Analysis of Crowds: A Multidisciplinary Perspective (The International Series in Video Computing)}},
url = {http://www.amazon.com/Modeling-Simulation-Visual-Analysis-Crowds/dp/1461484820},
year = {2013}
}
@inproceedings{Sun2009,
abstract = {This paper proposes a hierarchical perception model for simulation of crowd perception in different density. In this model, there are two tiers: the low density and high density perception modules. They are both composed of perceptual filters, short-term and long-term memory. However, considering the changes of the perception as the density increases, we adopt different methods to model them. We also take into account the impact of the human subjective factors on the selection of the object the virtual agent focuses. Our hierarchical perception model can well reflect the characteristics of the human perception; moreover, it can provide more plausible information for the virtual human among the crowd to respond properly to certain event.},
address = {Tianjin},
author = {Sun, Libo and Liu, Yan and Sun, Jizhou and Lu, Wenyu},
booktitle = {2009 Sixth International Conference on Computer Graphics, Imaging and Visualization},
doi = {10.1109/CGIV.2009.43},
isbn = {978-0-7695-3789-4},
keywords = {crowd density,crowd simulation,hierarchical perception model,long-term memory,perceptual filter,short-term memory},
month = aug,
pages = {106--111},
publisher = {IEEE},
title = {{The Hierarchical Perception Model for Crowd Simulation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5298317},
year = {2009}
}
@book{Norman1988,
author = {Norman, Donald},
isbn = {0465067093},
pages = {257},
publisher = {Basic Books},
title = {{The psychology of everyday things}},
url = {http://www.livrariacultura.com.br/scripts/cultura/resenha/resenha.asp\& nitem=255317},
year = {1988}
}
@article{Kim2007,
author = {Kim, Sehwan and Diverdi, Stephen and Iltis, Ronald and Tobias, H},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Online/2007/Kim et al/Kim et al. - 2007 - Implicit 3D Modeling and Tracking for Anywhere Augmentation.pdf:pdf},
isbn = {9781595938633},
journal = {Online},
keywords = {based tracking,camera pose estimation,feature-,online modeling,outdoor augmented reality,ukf},
pages = {19--28},
title = {{Implicit 3D Modeling and Tracking for Anywhere Augmentation}},
year = {2007}
}
@article{Rolland2002,
author = {Rolland, J. and Davis, L. and Meyer, C. and Shaoulov, V. and Akcay, A. and Banks, R. and {Del Vento}, B.},
doi = {10.1109/38.974512},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Computer Graphics and Applications/2002/Rolland et al/Rolland et al. - 2002 - 3D visualization and imaging in distributed collaborative environments.pdf:pdf},
issn = {02721716},
journal = {IEEE Computer Graphics And Applications},
number = {1},
pages = {11--13},
title = {{3D visualization and imaging in distributed collaborative environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=974512},
volume = {22},
year = {2002}
}
@article{Grindinger2010,
abstract = {A novel method for distinguishing classes of viewers from their aggregated eye movements is described. The probabilistic framework accumulates uniformly sampled gaze as Gaussian point spread functions (heatmaps), and measures the distance of unclassified scanpaths to a previously classified set (or sets). A similarity measure is then computed over the scanpath durations. The approach is used to compare human observers’s gaze over video to regions of interest (ROIs) automatically predicted by a computational saliency model. Results show consistent discrimination between human and artificial ROIs, regardless of either of two differing instructions given to human observers (free or tasked viewing).},
author = {Grindinger, Thomas J. and Murali, Vidya N. and Tetreault, Stephen and Duchowski, Andrew T. and Birchfield, Stan T. and Orero, Pilar},
doi = {10.1007/978-3-642-22822-3\_39},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computer Vision - ACCV/2010/Grindinger et al/Grindinger et al. - 2010 - Algorithm for Discriminating Aggregate Gaze Points Comparison with Salient Regions-Of-Interest.pdf:pdf},
journal = {Computer Vision - ACCV},
pages = {390--399},
title = {{Algorithm for Discriminating Aggregate Gaze Points: Comparison with Salient Regions-Of-Interest}},
volume = {6468},
year = {2010}
}
@inproceedings{Lu2010,
abstract = {Occlusion handling in augmented reality (AR) applications is challenging in synthesizing virtual objects correctly into the real scene with respect to existing foregrounds and shadows. Furthermore, outdoor environment makes the task more difficult due to the unpredictable illumination changes. This paper proposes novel outdoor illumination constraints for resolving the foreground occlusion problem in outdoor environment. The constraints can be also integrated into a probabilistic model of multiple cues for a better segmentation of the foreground. In addition, we introduce an effective method to resolve the shadow occlusion problem by using shadow detection and recasting with a spherical vision camera. We have applied the system in our digital cultural heritage project named Virtual Asuka (VA) and verified the effectiveness of the system.},
author = {Lu, Boun Vinh and Kakuta, Tetsuya and Kawakami, Rei and Oishi, Takeshi and Ikeuchi, Katsushi},
booktitle = {2010 IEEE International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2010.5643558},
isbn = {978-1-4244-9343-2},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = oct,
pages = {109--118},
publisher = {IEEE},
title = {{Foreground and shadow occlusion handling for outdoor augmented reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5643558 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5643558},
year = {2010}
}
@article{Larsson2013,
abstract = {A novel algorithm for detection of saccades and postsaccadic oscillations in the presence of smooth pursuit movements is proposed. The method combines saccade detection in the acceleration domain with specialized on- and offset criteria for saccades and postsaccadic oscillations. The performance of the algorithm is evaluated by comparing the detection results to those of an existing velocity-based adaptive algorithm and a manually annotated database. The results show that there is a good agreement between the events detected by the proposed algorithm and those in the annotated database with Cohen's kappa around 0.8 for both a development and a test database. In conclusion, the proposed algorithm accurately detects saccades and postsaccadic oscillations as well as intervals of disturbances.},
author = {Larsson, Linn\'{e}a and Nystr\"{o}m, Marcus and Stridh, Martin},
doi = {10.1109/TBME.2013.2258918},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE transactions on bio-medical engineering/2013/Larsson, Nystr\"{o}m, Stridh/Larsson, Nystr\"{o}m, Stridh - 2013 - Detection of saccades and postsaccadic oscillations in the presence of smooth pursuit.pdf:pdf},
issn = {1558-2531},
journal = {IEEE transactions on bio-medical engineering},
keywords = {gaze analysis},
mendeley-tags = {gaze analysis},
month = sep,
number = {9},
pages = {2484--93},
pmid = {23625350},
title = {{Detection of saccades and postsaccadic oscillations in the presence of smooth pursuit.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6504734 http://www.ncbi.nlm.nih.gov/pubmed/23625350},
volume = {60},
year = {2013}
}
@inproceedings{Braun2003,
abstract = {This paper presents a model for studying the impact of individual agent characteristics in emergent groups, based on the evacuation efficiency as a result of local interactions. We used the physically based model of crowd simulation proposed by Helbing et al. (2000) and generalized it in order to deal with different individualities for agent and group behaviors. In addition, we present a framework to visualize the virtual agents and discuss the obtained results. A variety of simulations with different parameter sets shows significant impact on the evacuation scenario.},
author = {Braun, A and Musse, SR and de Oliveira, L.P.L. and Bodmann, B.E.J.},
booktitle = {Proceedings 11th IEEE International Workshop on Program Comprehension},
doi = {10.1109/CASA.2003.1199317},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings 11th IEEE International Workshop on Program Comprehension/2003/Braun et al/Braun et al. - 2003 - Modeling individual behaviors in crowd simulation.pdf:pdf},
isbn = {0-7695-1934-2},
keywords = {crowd simulation},
mendeley-tags = {crowd simulation},
pages = {143--148},
publisher = {IEEE Comput. Soc},
title = {{Modeling individual behaviors in crowd simulation}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1199317 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1199317},
year = {2003}
}
@book{Tiffin2001,
author = {Tiffin, John and Terashima, Nobuyoshi},
isbn = {041526104X},
pages = {192},
publisher = {Routledge},
title = {{HyperReality: Paradigm for the Third Millennium}},
url = {http://www.amazon.com/HyperReality-Paradigm-Millenium-Nobuyoshi- Terashima/dp/041526104X},
year = {2001}
}
@article{Lozano2009,
abstract = {Crowd simulation requires both rendering visually plausible images and managing the behavior of autonomous agents. Therefore, these applications need an efficient design that allows them to simultaneously handle these two requirements. Although several proposals have focused on the software architectures for these systems, no proposals have focused on the computer systems supporting them. In this paper, we analyze the computer architectures used in the literature to support distributed virtual environments. Also, we propose a distributed computer architecture which is efficient enough to support simulations of thousand of autonomous agents. This proposal consists of a cluster of interconnected computers in order to improve flexibility and robustness, as well as a hierarchical software architecture that efficiently provides consistency. Performance evaluation results show that the trade-off between flexibility and consistency allows to efficiently manage thousands of autonomous agents. Therefore, this network-based system architecture can provide the required scalability for large-scale crowd simulations.},
author = {Lozano, M. and Morillo, P. and Ordu\~{n}a, J.M. and Cavero, V. and Vigueras, G.},
doi = {10.1016/j.jnca.2008.02.011},
isbn = {10848045},
issn = {10848045},
journal = {Journal of Network and Computer Applications},
month = mar,
number = {2},
pages = {474--482},
title = {{A new system architecture for crowd simulation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S108480450800060X},
volume = {32},
year = {2009}
}
@inproceedings{Kim2011,
abstract = {Estimation of human head position and orientation has become an increasingly important issue in human-computer interaction field. Over the last decade, many approaches have been introduced to achieve head pose estimation in both academical and industrial fields, but the low-cost and real-time application still proves to be a difficult task. Motivated by the past researches, we propose an automatic and monocular head pose estimation system. We applied a number of improvements to a direct linear transformation algorithm called Pose from Orthography and Scaling with ITerations (POSIT) and applied it for the head pose estimation. User's virtual head model is also recovered by analyzing laser scan database and corrected by the head pose. The entire process is completely automatic with no need for users to pre-register for identification or initialize their head position. Experiments on a public dataset show realtime performance with lower errors than previous head pose estimation methods.},
author = {Kim, Woo Won and Park, Sangheon and Hwang, Jinkyu and Lee, Sangyoun},
booktitle = {2011 8th International Conference on Information, Communications \& Signal Processing},
doi = {10.1109/ICICS.2011.6173539},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2011 8th International Conference on Information, Communications \& Signal Processing/2011/Kim et al/Kim et al. - 2011 - Automatic head pose estimation from a single camera using projective geometry.pdf:pdf},
isbn = {978-1-4577-0031-6},
month = dec,
pages = {1--5},
publisher = {IEEE},
title = {{Automatic head pose estimation from a single camera using projective geometry}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6173539\&contentType=Conference+Publications\&searchField=Search\_All\&queryText=.QT.projective+geometry.QT.},
year = {2011}
}
@article{Holland2011,
author = {Holland, C and Komogortsev, Oleg V},
file = {::},
journal = {Biometrics (IJCB), 2011 \ldots},
keywords = {Eye Tracking,Segmentation},
mendeley-tags = {Eye Tracking,Segmentation},
title = {{Biometric identification via eye movement scanpaths in reading}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6117536},
year = {2011}
}
@inproceedings{Delage2006,
abstract = {When we look at a picture, our prior knowledge about the world allows us to resolve some of the ambiguities that are inherent to monocular vision, and thereby infer 3d information about the scene. We also recognize different objects, decide on their orientations, and identify how they are connected to their environment. Focusing on the problem of autonomous 3d reconstruction of indoor scenes, in this paper we present a dynamic Bayesian network model capable of resolving some of these ambiguities and recovering 3d information for many images. Our model assumes a "floorwall" geometry on the scene and is trained to recognize the floor-wall boundary in each column of the image. When the image is produced under perspective geometry, we show that this model can be used for 3d reconstruction from a single image. To our knowledge, this was the first monocular approach to automatically recover 3d reconstructions from single indoor images.},
author = {Delage, E and Ng, AY},
booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR'06)},
doi = {10.1109/CVPR.2006.23},
isbn = {0-7695-2597-0},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
pages = {2418--2428},
publisher = {IEEE},
title = {{A Dynamic Bayesian Network Model for Autonomous 3D Reconstruction from a Single Indoor Image}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1641050 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1641050},
volume = {2},
year = {2006}
}
@article{Li2009,
abstract = {We present a framework and algorithms for robust geometry and motion reconstruction of complex deforming shapes. Our method makes use of a smooth template that provides a crude approximation of the scanned object and serves as a geometric and topological prior for reconstruction. Large-scale motion of the acquired object is recovered using a novel space-time adaptive, non-rigid registration method. Fine-scale details such as wrinkles and folds are synthesized with an efficient linear mesh deformation algorithm. Subsequent spatial and temporal filtering of detail coefficients allows transfer of persistent geometric detail to regions not observed by the scanner. We show how this two-scale process allows faithful recovery of small-scale shape and motion features leading to a high-quality reconstruction. We illustrate the robustness and generality of our algorithm on a variety of examples composed of different materials and exhibiting a large range of dynamic deformations.},
author = {Li, Hao and Adams, Bart and Guibas, Leonidas J. and Pauly, Mark},
doi = {10.1145/1618452.1618521},
isbn = {978-1-60558-858-2},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {3D scanning,animation reconstruction,geometry synthesis,non-rigid registration,partial scans,reconstruction,template tracking},
mendeley-tags = {reconstruction},
month = dec,
number = {5},
pages = {1},
publisher = {ACM},
title = {{Robust single-view geometry and motion reconstruction}},
url = {http://dl.acm.org/citation.cfm?id=1618452.1618521 http://portal.acm.org/citation.cfm?doid=1618452.1618521},
volume = {28},
year = {2009}
}
@article{Sung2004,
abstract = {Crowd simulation for virtual environments offers many challenges centered on the trade-offs between rich behavior, control and computational cost. In this paper we present a new approach to controlling the behavior of agents in a crowd. Our method is scalable in the sense that increasingly complex crowd behaviors can be created without a corresponding increase in the complexity of the agents. Our approach is also more authorable; users can dynamically specify which crowd behaviors happen in various parts of an environment. Finally, the character motion produced by our system is visually convincing. We achieve our aims with a situation-based control structure. Basic agents have very limited behaviors. As they enter new situations, additional, situation-specific behaviors are composed on the fly to enable agents to respond appropriately. The composition is done using a probabilistic mechanism. We demonstrate our system with three environments including a city street and a theater. Categories and Subject Descriptors (according to ACM CCS): I.3.7 Computer Graphics: Animation},
author = {Sung, Mankyu and Gleicher, Michael and Chenney, Stephen},
doi = {10.1111/j.1467-8659.2004.00783.x},
isbn = {1467-8659},
issn = {0167-7055},
journal = {Computer Graphics Forum},
month = sep,
number = {3},
pages = {519--528},
title = {{Scalable behaviors for crowd simulation}},
url = {http://doi.wiley.com/10.1111/j.1467-8659.2004.00783.x},
volume = {23},
year = {2004}
}
@inproceedings{Mardanbegi2012,
abstract = {A novel method for video-based head gesture recognition using eye information by an eye tracker has been proposed. The method uses a combination of gaze and eye movement to infer head gestures. Compared to other gesture-based methods a major advantage of the method is that the user keeps the gaze on the interaction object while interacting. This method has been implemented on a head-mounted eye tracker for detecting a set of predefined head gestures. The accuracy of the gesture classifier is evaluated and verified for gaze-based interaction in applications intended for both large public displays and small mobile phone screens. The user study shows that the method detects a set of defined gestures reliably.},
address = {New York, New York, USA},
author = {Mardanbegi, Diako and Hansen, Dan Witzner and Pederson, Thomas},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12},
doi = {10.1145/2168556.2168578},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12/2012/Mardanbegi, Hansen, Pederson/Mardanbegi, Hansen, Pederson - 2012 - Eye-based head gestures.pdf:pdf},
isbn = {9781450312219},
keywords = {eye tracker,gaze interaction,head gestures,interaction,toshi},
mendeley-tags = {eye tracker,toshi},
month = mar,
pages = {139},
publisher = {ACM Press},
title = {{Eye-based head gestures}},
url = {http://dl.acm.org/citation.cfm?id=2168556.2168578},
year = {2012}
}
@article{Pang2013,
address = {New York, New York, USA},
author = {Pang, Carolyn E. and Neustaedter, Carman and Riecke, Bernhard E. and Oduor, Erick and Hillman, Serena},
doi = {10.1145/2470654.2466232},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {Personal health informatics, families, social supp},
pages = {1759},
publisher = {ACM Press},
title = {{Technology preferences and routines for sharing health information during the treatment of a chronic illness}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466232},
year = {2013}
}
@inproceedings{Tien2007,
abstract = {In this paper, we propose a system that can automatically segment a basketball video into several clips on the basis of a GOP-based scene change detection method. The length of each clip and the number of dominant color pixels of each frame are used to classify shots into close-up view, medium view, and full court view. Full court view shots are chosen to do advanced analyses such as ball tracking and parameter extracting for the transformation from a 3D real-world court to a 2D image. After that, we map points in the 2D image to the corresponding coordinates in a real-world court by some physical properties of the 3D shooting trajectory, and compute the statistics of all shooting positions. Eventually we can obtain the information about the most possible shooting positions of a professional basketball team, which is useful for opponents to adopt appropriate defense tactics.},
author = {Tien, Min-Chun and Chen, Hua-Tsung and Chen, Yi-Wen and Hsiao, Ming-Ho and Lee, Suh-Yin},
booktitle = {2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07},
doi = {10.1109/ICASSP.2007.366100},
isbn = {1-4244-0727-3},
issn = {1520-6149},
keywords = {3D real-world court,3D shooting trajectory,Application software,Calibration,Cameras,Data mining,GOP-based scene change detection method,Games,Gunshot detection systems,Layout,Trajectory,Videos,Watches,ball tracking,basketball,basketball videos,camera calibration,color pixels,dominant color,image classification,image colour analysis,image resolution,parameter extracting,scene change detection,shooting position extraction,shot classification,tracking,video signal processing},
mendeley-tags = {basketball,tracking},
pages = {I--1085--I--1088},
publisher = {IEEE},
shorttitle = {Acoustics, Speech and Signal Processing, 2007. ICA},
title = {{Shot Classification of Basketball Videos and its Application in Shooting Position Extraction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4217272},
volume = {1},
year = {2007}
}
@article{McMillan2013,
address = {New York, New York, USA},
author = {McMillan, Donald and Morrison, Alistair and Chalmers, Matthew},
doi = {10.1145/2470654.2466245},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1853},
publisher = {ACM Press},
title = {{Categorised ethical guidelines for large scale mobile HCI}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466245},
year = {2013}
}
@inproceedings{Ban2013,
abstract = {The main contribution of this paper is to develop a method for alleviating fatigue during handling medium-weight objects and augmenting our endurance by affecting our weight perception with augmented reality technology. To assist people to lift medium-weight objects without a complex structure or various costs, we focus on the phenomenon that our weight perception during handling objects is affected by visual properties. Our hypothesis is that this illusionary effect in weight perception can be applied to reduce fatigue while handling medium-weight objects without mechatronics-based physical assistance. In this paper, we propose an augmented reality system that changes the brightness value of an object in order to reduce fatigue while handling the object. We conducted two fundamental experiments to investigate the effectiveness of the proposed system. Our results suggested that the system eliminates the need to use excess energy for handling objects and reduces fatigue during the handling task.},
address = {New York, New York, USA},
author = {Ban, Yuki and Narumi, Takuji and Fujii, Tatsuya and Sakurai, Sho and Imura, Jun and Tanikawa, Tomohiro and Hirose, Michitaka},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470665},
isbn = {9781450318990},
pages = {69--78},
publisher = {ACM Press},
title = {{Augmented endurance: controlling fatigue while handling objects by affecting weight perception using augmented reality}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470665},
year = {2013}
}
@inproceedings{Liu2013,
abstract = {Health video blogs (vlogs) allow individuals with chronic illnesses to share their stories, experiences, and knowledge with the general public. Furthermore, health vlogs help in creating a connection between the vlogger and the viewers. In this work, we present a qualitative study examining the various methods that health vloggers use to establish a connection with their viewers. We found that vloggers used genres to express specific messages to their viewers while using the uniqueness of video to establish a deeper connection with their viewers. Health vloggers also explicitly sought interaction with their viewers. Based on these results, we present design implications to help facilitate and build sustainable communities for vloggers.},
address = {New York, New York, USA},
author = {Liu, Leslie S. and Huh, Jina and Neogi, Tina and Inkpen, Kori and Pratt, Wanda},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470663},
isbn = {9781450318990},
pages = {49--58},
publisher = {ACM Press},
title = {{Health vlogger-viewer interaction in chronic illness management}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470663},
year = {2013}
}
@inproceedings{Moura2011a,
abstract = {This paper presents a solution for photorealistic rendering of virtual objects into dynamic real scenes. It consists of a toolkit for Augmented Reality, named RPR-SORS, which is composed of two artifacts: an API that handles various computer graphics techniques in a novel pipeline approaching illumination, reflectance model, shadowing, composition, and camera effects; and a material editor that accelerates the creation of complex photorealistic materials. An experimental application was developed with the proposed solution, where both the API and material editor were used to accomplish photorealistic effects in an application of Augmented Reality in architecture. The results obtained show that the API provides a flexible infrastructure for the photorealistic Augmented Reality pipeline, and that the material editor facilitates the creation of photorealistic materials and their use for Augmented Reality.},
address = {Uberlandia},
author = {Moura, Guilherme de S and Pessoa, Saulo A. and Lima, Jo\~{a}o Paulo S. do M. and Teichrieb, Veronica and Kelner, Judith},
booktitle = {2011 XIII Symposium on Virtual Reality},
doi = {10.1109/SVR.2011.14},
isbn = {978-1-4577-0661-5},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = may,
pages = {178--187},
publisher = {IEEE},
title = {{RPR-SORS: An Authoring Toolkit for Photorealistic AR}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5951850 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5951850},
year = {2011}
}
@article{Hu2011,
abstract = {We present a pattern-based approach for simulating the steering behaviour of pedestrians, which aims to imitate the way that real pedestrians perceive spatial-temporal information and make steering de- cisions in daily-life situations. Novel representations of spatial-temporal patterns are proposed that allow modellers to intuitively and naturally specify some prototypical patterns for various steering behaviours. Based on the spatial-temporal patterns, a hierarchical pattern matching pro- cess has been developed, which simulates how pedestrians process spatial temporal information and make steering decisions. Experimental results show that this new approach is quite promising and capable of producing human-like steering. We hope that the idea presented in this paper can direct researchers in this area with a fresh perspective.},
author = {Hu, Nan and Lees, Michael and Zhou, Suiping and T, Vaisagh Viswanathan},
doi = {10.1007/978-3-642-22639-7\_11},
journal = {Transactions on edutainment},
keywords = {crowd simu,lation,motion planning,spatial temporal patterns,steering behaviour},
pages = {99--110},
title = {{Pattern Based Motion for Crowd Simulation}},
volume = {VI},
year = {2011}
}
@article{Essig2006,
abstract = {Using eye tracking for the investigation of visual attention has become increasingly popular during the last few decades. Nevertheless, only a small number of eye tracking studies have employed 3D displays, although such displays would closely resemble our natural visual environment. Besides higher cost and effort for the experimental setup, the main reason for the avoidance of 3D displays is the problem of computing a subject's current 3D gaze position based on the measured binocular gaze angles. The geometrical approaches to this problem that have been studied so far involved substantial error in the measurement of 3D gaze trajectories. In order to tackle this problem, we developed an anaglyph-based 3D calibration procedure and used a well-suited type of artificial neural network—a parametrized self-organizing map (PSOM)—to estimate the 3D gaze point from a subject's binocular eye-position data. We report an experiment in which the accuracy of the PSOM gaze-point estimation is compared to a geometrical solution. The results show that the neural network approach produces more accurate results than the geometrical method, especially for the depth axis and for distant stimuli.},
author = {Essig, Kai and Pomplun, Marc and Ritter, Helge},
doi = {10.1080/17445760500354440},
issn = {1744-5760},
journal = {International Journal of Parallel, Emergent and Distributed Systems},
keywords = {3d gaze},
mendeley-tags = {3d gaze},
month = apr,
number = {2},
pages = {79--95},
title = {{A neural network for 3D gaze recording with binocular eye trackers}},
url = {http://www.tandfonline.com/doi/abs/10.1080/17445760500354440},
volume = {21},
year = {2006}
}
@article{Kolivand2009,
abstract = {Implementation of shadows is crucial to enhancement of images in AR environments. Without shadows, virtual objects would look floating over the scene resulting in unrealistic rendering of AR environments. Casting hard shadows would provide only spatial information while soft shadows help improve realism of AR environments. Several algorithms have been proposed to render realistic shadows which often incurred high computational costs. Little attention has been directed towards the balanced trade-off between shadow quality and computational costs. In this study, two approaches are proposed: Quadratic Spline Interpolation (QSI) to soften the outline of the shadow and Detail Multi-Layer (DML) technique to optimize the volume of computations for the generation of soft shadows based on real light sources. QSI estimates boarder hard shadow samples while DML involves three main phases: real light sources estimation, soft shadow production and reduction of the complexity of 3-Dimensional objects’ shadows. To be more precise, a reflective hemisphere is used to capture real light and to create an environment map. The Median Cut algorithm is implemented to locate the direction of real light sources on the environment map. Subsequently, the original hard shadows are retrieved and a sample of multilayer hard shadows is produced where each layer has its unique size and colour. These layers overlap to produce soft shadows based on the real light sources’ directions. Finally, the Level of Details (LOD) algorithm is implemented to increase the efficiency of soft shadows by decreasing the complexity of vertex transformations. The proposed technique is tested using three samples of multilayer hard shadows with varying numbers of light sources generated from the Median Cut algorithm. The experimental results show that the proposed technique successfully produces realistic soft shadows at low computational costs.},
author = {Kolivand, Hoshang and Noh, Zakiah and Sunar, Mohd Shahrizal},
doi = {10.1007/s11042-013-1630-6},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
title = {{A quadratic spline approximation using detail multi-layer for soft shadow generation in augmented reality}},
url = {http://link.springer.com/article/10.1007/s11042-013-1630-6 http://link.springer.com/10.1007/s11042-013-1630-6},
year = {2013}
}
@inproceedings{Findlater2013,
abstract = {Despite the apparent popularity of touchscreens for older adults, little is known about the psychomotor performance of these devices. We compared performance between older adults and younger adults on four desktop and touchscreen tasks: pointing, dragging, crossing and steering. On the touchscreen, we also examined pinch-to-zoom. Our results show that while older adults were significantly slower than younger adults in general, the touchscreen reduced this performance gap relative to the desktop and mouse. Indeed, the touchscreen resulted in a significant movement time reduction of 35\% over the mouse for older adults, compared to only 16\% for younger adults. Error rates also decreased.},
address = {New York, New York, USA},
author = {Findlater, Leah and Froehlich, Jon E. and Fattal, Kays and Wobbrock, Jacob O. and Dastyar, Tanya},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470703},
isbn = {9781450318990},
pages = {343--346},
publisher = {ACM Press},
title = {{Age-related differences in performance with touchscreens compared to traditional mouse input}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470703},
year = {2013}
}
@inproceedings{Andersen2006,
abstract = {The the work presented in this paper explores how dynamical light parameters in an outdoor environment can be estimated for use in a real-time augmented reality (AR) system. A method using existing inverse rendering techniques is used to acquire diffuse surface reflectances in an offline procedure. The reflectances are used in an on-line procedure to estimate the illumination parameters of an outdoor scene. The method presented reduces the light estimation problem of outdoor scenes to a modified Phong shading model with two unknown parameters, which can be determined through the use of a linear equations system. The work presented provides an elegant method for estimating dynamically changing illumination parameters without the need for a calibration object in the scene.},
author = {Andersen, M.S. and Jensen, Tommy and Madsen, C.B.},
booktitle = {18th International Conference on Pattern Recognition (ICPR'06)},
doi = {10.1109/ICPR.2006.504},
isbn = {0-7695-2521-0},
pages = {91--94},
publisher = {IEEE},
title = {{Estimation of Dynamic Light Changes in Outdoor Scenes Without the use of Calibration Objects}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1699790},
year = {2006}
}
@inproceedings{Haller2004,
abstract = {Actual graphic hardware becomes more and more powerful. Consequently, virtual scenes can be rendered in a very good quality integrating dynamic behavior, real-time shadows, bump mapped surfaces and other photorealistic rendering techniques. On the other hand, non-photorealistic rendering became popular as well, because of its artistic merits. An integration in an AR environment is the logical consequence. In this paper we present an AR framework that uses photorealistic rendering as well as non-photorealistic rendering techniques. The prototypes based on these techniques show the advantages and disadvantages of both technologies in combination with Augmented Reality.},
address = {New York, New York, USA},
author = {Haller, Michael},
booktitle = {Proceedings of the 2004 ACM SIGGRAPH international conference on Virtual Reality continuum and its applications in industry - VRCAI '04},
doi = {10.1145/1044588.1044627},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Current/2004/Haller/Haller - 2004 - Photorealism or and Non-Photorealism in Augmented Reality.pdf:pdf},
isbn = {1581138849},
keywords = {augmented reality,non-,photorealisic rendering,photorealistic rendering},
pages = {189},
publisher = {ACM Press},
title = {{Photorealism or/and non-photorealism in augmented reality}},
url = {http://portal.acm.org/citation.cfm?doid=1044588.1044627},
year = {2004}
}
@article{Oulasvirta2013,
address = {New York, New York, USA},
author = {Oulasvirta, Antti and Roos, Teemu and Modig, Arttu and Lepp\"{a}nen, Laura},
doi = {10.1145/2470654.2466169},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {1289},
publisher = {ACM Press},
title = {{Information capacity of full-body movements}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2466169},
year = {2013}
}
@article{LeMeunier2003,
author = {{Le Meunier}, L. and Mathy, F. and Fagret, D.},
doi = {10.1109/TNS.2003.818369},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Nuclear Science/2003/Le Meunier, Mathy, Fagret/Le Meunier, Mathy, Fagret - 2003 - Validation of a pet monte-carlo simulator with random events and dead time modeling.pdf:pdf},
isbn = {0780376366},
issn = {0018-9499},
journal = {IEEE Transactions on Nuclear Science},
month = oct,
number = {5},
pages = {1462--1468},
title = {{Validation of a pet monte-carlo simulator with random events and dead time modeling}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1236950},
volume = {50},
year = {2003}
}
@article{Teubl2012,
author = {Teubl, Fernando and Kurashima, Celso and Cabral, Marcio and Zuffo, Marcelo},
doi = {10.1109/SVR.2012.1},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2012 14th Symposium on Virtual and Augmented Reality/2012/Teubl et al/Teubl et al. - 2012 - FastFusion A Scalable Multi-projector System.pdf:pdf},
isbn = {978-1-4673-1929-4},
journal = {2012 14th Symposium on Virtual and Augmented Reality},
keywords = {-multi-projector system,virtual reality,visual},
month = may,
pages = {26--35},
publisher = {Ieee},
title = {{FastFusion: A Scalable Multi-projector System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6297557},
year = {2012}
}
@article{Smeets2003,
abstract = {We studied the variability in saccades by comparing the peak velocities of saccades with the same target amplitude made with different actual amplitudes. We tested three hypotheses: the pulse-height noise hypothesis (peak velocity and amplitude vary proportionally), the localization noise hypothesis (variability in amplitude and peak velocity lie along the main sequence), and the independent noise hypothesis (variability in amplitude and peak velocity are independent). We measured eye orientation in two experiments by a scleral coil and a video system. Surprisingly, the main source of variability of saccades depended on the measurement system used. A combination of localization noise and independent noise best describes the data obtained by the video system. The independent noise (e.g., measurement inaccuracy) was the main source of variability. For the scleral coils, the variability was considerably larger than for the less accurate video system. The pulse-height noise hypothesis best describes this additional variability. Therefore we conclude that pulse-height noise is the main source of variability in saccades measured with scleral coils. We discuss the influence of scleral coils on saccade generation and suggest that a change in motor strategy due to the discomfort of wearing the coils might be the cause of the increased variability.},
annote = {- cited by: 35
- kw: Nature of variability in saccades
- engine: Google Scholar
- crossref: Larsson2010},
author = {Smeets, JBJ and Hooge, ITC},
doi = {10.​1152/​jn.​01075.​2002},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of neurophysiology/2003/Smeets, Hooge/Smeets, Hooge - 2003 - Nature of variability in saccades.pdf:pdf},
journal = {Journal of neurophysiology},
keywords = {gaze analysis,saccade},
mendeley-tags = {gaze analysis,saccade},
number = {1},
pages = {12--20},
title = {{Nature of variability in saccades}},
url = {http://jn.physiology.org/content/90/1/12.short},
volume = {90},
year = {2003}
}
@inproceedings{Gibson2013,
abstract = {New mothers can experience social exclusion, particularly during the early weeks when infants are solely dependent on their mothers. We used ethnographic methods to investigate whether technology plays a role in supporting new mothers. Our research identified two core themes: (1) the need to improve confidence as a mother; and (2) the need to be more than ‘just’ a mother. We reflect on these findings both in terms of those interested in designing applications and services for motherhood and also the wider CHI community.},
address = {New York, New York, USA},
author = {Gibson, Lorna and Hanson, Vicki L},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2470700},
isbn = {9781450318990},
pages = {313--322},
publisher = {ACM Press},
title = {{Digital motherhood: how does technology help new mothers?}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470700},
year = {2013}
}
@article{Andersen2013,
address = {New York, New York, USA},
author = {Andersen, Erik and Gulwani, Sumit and Popovic, Zoran},
doi = {10.1145/2470654.2470764},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
keywords = {games, procedural content generation},
pages = {773},
publisher = {ACM Press},
title = {{A trace-based framework for analyzing and synthesizing educational progressions}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2470764},
year = {2013}
}
@article{Fuhrmann1999,
abstract = {Augmented environments superimpose computer enhancements on the real world. Such augmented environments are well suited for collaboration of multiple users. To improve the quality and consistency of the augmentation the occlusion of real objects by computer-generated objects and vice versa has to be implemented. We present methods how this can be done for a tracked user's body and other real objects and how irritating artifacts due to misalignments can be reduced. Our method is based on simulating the occlusion of virtual objects by a representation of the user modeled as kinematic chains of articulated solids. Smoothing the border between virtual world and occluding real reduces registration and modeling errors of this model. Finally, an implementation in our augmented environment and the resulting improvements are presented.},
author = {Fuhrmann, Anton and Hesina, Gerd and Faure, Fran\c{c}ois and Gervautz, Michael},
doi = {10.1016/S0097-8493(99)00107-7},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Computers \& Graphics/1999/Fuhrmann et al/Fuhrmann et al. - 1999 - Occlusion in collaborative augmented environments.pdf:pdf},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
month = dec,
number = {6},
pages = {809--819},
title = {{Occlusion in collaborative augmented environments}},
url = {http://www.sciencedirect.com/science/article/pii/S0097849399001077 http://linkinghub.elsevier.com/retrieve/pii/S0097849399001077},
volume = {23},
year = {1999}
}
@article{Cristino2010,
abstract = {We present a novel approach to comparing saccadic eye movement sequences based on the Needleman-Wunsch algorithm used in bioinformatics to compare DNA sequences. In the proposed method, the saccade sequence is spatially and temporally binned and then recoded to create a sequence of letters that retains fixation location, time, and order information. The comparison of two letter sequences is made by maximizing the similarity score computed from a substitution matrix that provides the score for all letter pair substitutions and a penalty gap. The substitution matrix provides a meaningful link between each location coded by the individual letters. This link could be distance but could also encode any useful dimension, including perceptual or semantic space. We show, by using synthetic and behavioral data, the benefits of this method over existing methods. The ScanMatch toolbox for MATLAB is freely available online (www.scanmatch.co.uk).},
author = {Cristino, Filipe and Math\^{o}t, Sebastiaan and Theeuwes, Jan and Gilchrist, Iain D},
doi = {10.3758/BRM.42.3.692},
issn = {1554-3528},
journal = {Behavior research methods},
keywords = {Algorithms,Computational Biology,Fixation,Humans,Ocular,Ocular: physiology,Photic Stimulation,Psychomotor Performance,Psychomotor Performance: physiology,Reading,Saccades,Saccades: physiology,Semantics,Software,User-Computer Interface,Visual Perception,Visual Perception: physiology,eye tracking,scanpath,scanpath similarity,similarity},
mendeley-tags = {eye tracking,scanpath,scanpath similarity,similarity},
month = aug,
number = {3},
pages = {692--700},
pmid = {20805591},
title = {{ScanMatch: a novel method for comparing fixation sequences.}},
url = {http://link.springer.com/article/10.3758/BRM.42.3.692 http://www.ncbi.nlm.nih.gov/pubmed/20805591},
volume = {42},
year = {2010}
}
@inproceedings{Morris2013,
abstract = {An interactive system, PIXEE, was developed to promote greater emotional expression in image-based social media. Images shared on social media were projected onto a large interactive display at public events. A multimodal interface displayed the sentiment analysis of images and invited viewers to express their emotional responses. Viewers could adjust the emotional classification and thereby change the color and sound associated with a picture, and experiment with emotion-based composition. An interdisciplinary team deployed this system around the world to explore new ways for technology to catalyze emotional connectedness. This paper describes the system, design iterations, and observations about how people used it for self-expression and connection.},
address = {New York, New York, USA},
author = {Morris, Margaret E and Labs, Intel and Corporation, Intel and Haj, Murad Al and Marshall, Carl S and Calix, Mira and Macdougall, James S and Carmean, Douglas M},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13},
doi = {10.1145/2468356.2468750},
isbn = {9781450319522},
pages = {2277--2286},
publisher = {ACM Press},
title = {{PIXEE: Pictures, Interaction and Emotional Expression}},
url = {http://dl.acm.org/citation.cfm?doid=2468356.2468750},
year = {2013}
}
@inproceedings{Lalonde2009,
abstract = {Given a single outdoor image, we present a method for estimating the likely illumination conditions of the scene. In particular, we compute the probability distribution over the sun position and visibility. The method relies on a combination of weak cues that can be extracted from different portions of the image: the sky, the vertical surfaces, and the ground. While no single cue can reliably estimate illumination by itself, each one can reinforce the others to yield a more robust estimate. This is combined with a data-driven prior computed over a dataset of 6 million Internet photos. We present quantitative results on a webcam dataset with annotated sun positions, as well as qualitative results on consumer-grade photographs downloaded from Internet. Based on the estimated illumination, we show how to realistically insert synthetic 3-D objects into the scene.},
address = {Kyoto},
author = {Lalonde, Jean-Francois and Efros, Alexei a and Narasimhan, Srinivasa G},
booktitle = {2009 IEEE 12th International Conference on Computer Vision},
doi = {10.1109/ICCV.2009.5459163},
isbn = {978-1-4244-4420-5},
month = sep,
number = {Iccv},
pages = {183--190},
publisher = {IEEE},
title = {{Estimating natural illumination from a single outdoor image}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459163},
year = {2009}
}
@inproceedings{Xu2003a,
abstract = {Video events detection or recognition is one of important tasks in semantic understanding of video content. Sports game video should be considered as a rule-based sequential signal, Therefore, it is reasonable to model sports events using Hidden Markov Models. In this paper, we present a generic. scalable and multi-layer framework based on HMMs. called SG-HMMs (Sports Conie HAZ$\backslash$I.T), for sports game event detection. At the bottom layer of this framewrork. event HMMs output basic hypotheses based on lo$\backslash$\%,-level features. The upper lays are composed of composition HMMs, which add constraints on those hypotheses of the lower layer. Instead of isolated event recognition, the hypotheses at different layers are optimized in a bottom-up manner and the optimal semantics are dcterniincd by top-down process. The experimental results on basketball and volleyball videos have dcmonstrated the en'ectiveness of the proposed framework for sports game analysis},
author = {Xu, Gu and Ma, Yu-Fei and Zhang, Hong-Juang and Yang, Shiqiang},
pages = {25--28},
publisher = {IEEE},
title = {{A HMM Based Semantic Analysis Framework for Sports Game Event Detection}},
year = {2003}
}
@article{Kisacanin2005,
author = {Kisa\v{c}anin, B and Pavlovi\'{c}, V and Huang, TS},
title = {{Real-time vision for human-computer interaction}},
url = {http://books.google.com.br/books?hl=pt-BR\&lr=\&id=82A4\_vJWxWsC\&oi=fnd\&pg=PR15\&dq=Real-Time+Vision+for+Human-Computer+Interaction\&ots=gq0rdcJgdU\&sig=FMAG6v4GZAakYcSsKs47nnjo0OU},
year = {2005}
}
@article{Rutherford2008,
abstract = {Typical adults use predictable scan patterns while observing faces. Some research suggests that people with autism spectrum disorders (ASD) instead attend to eyes less, and perhaps to the mouth more. The current experiment was designed as a direct measure of scan paths that people with and without ASD use when identifying simple and complex emotions. Participants saw photos of emotions and chose emotion labels. Scan paths were measured via infrared corneal reflectance. Both groups looked significantly longer at eyes than mouth, and neither overall looking time at eyes nor first fixations distinguished the groups. These results are contrary to suggestions that those with ASD attend preferentially to the mouth and avoid the eyes. Furthermore, there was no interaction between group and area of the face: the ratio of attention between eyes and mouth did not differ between the ASD and control groups. However, those with ASD looked at the eyes less than the control group when viewing complex emotions.},
author = {Rutherford, M D and Towns, Ashley M},
doi = {10.1007/s10803-007-0525-7},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Journal of autism and developmental disorders/2008/Rutherford, Towns/Rutherford, Towns - 2008 - Scan path differences and similarities during emotion perception in those with and without autism spectrum di.pdf:pdf},
issn = {0162-3257},
journal = {Journal of autism and developmental disorders},
keywords = {Adult,Attention,Autistic Disorder,Autistic Disorder: diagnosis,Autistic Disorder: psychology,Emotions,Eye Movements,Facial Expression,Female,Fixation,Humans,Male,Ocular,Pattern Recognition,Reference Values,Visual,Young Adult,eye tracking,scanpath,similarity},
mendeley-tags = {eye tracking,scanpath,similarity},
month = aug,
number = {7},
pages = {1371--81},
pmid = {18297386},
title = {{Scan path differences and similarities during emotion perception in those with and without autism spectrum disorders.}},
url = {http://link.springer.com/article/10.1007/s10803-007-0525-7 http://www.ncbi.nlm.nih.gov/pubmed/18297386},
volume = {38},
year = {2008}
}
@phdthesis{Larsson2010a,
abstract = {There are three different movements of the eye that are of main interest to detect and investigate by eye movement researchers; fixations, saccades and smooth pursuits. At the moment only two of them, fixations and saccades, can be de- tected by commercial detection algorithms. There are several different types of eye-trackers, each having its own properties and its own detection algorithm. In the first part of this thesis, filters used in different detection algorithms for com- putation of velocity and acceleration are compared and their impact on the pa- rameters describing the properties of the eye movement are investigated. It was shown that the filter used in the algorithm for detection of the eye movement, will have effects on the properties of the detected eye movements. In the second part of the thesis, a detection algorithm that is able to detect the three types of eye movements, is developed. Three different methods for detection of smooth pursuits are evaluated and the results show that a method using a Rayleigh test performs well in the detection of smooth pursuits.},
author = {Larsson, Linn\'{e}a},
booktitle = {Unpublished master's thesis). Lund University, Lund, \ldots},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unpublished master's thesis). Lund University, Lund, \ldots/2010/Larsson/Larsson - 2010 - Event detection in eye-tracking data.pdf:pdf},
keywords = {event detection,fixation,saccade,smooth pursuit},
mendeley-tags = {event detection,fixation,saccade,smooth pursuit},
school = {Lund University},
title = {{Event detection in eye-tracking data}},
url = {http://scholar.google.com.br/scholar?q="Event+detection+in+eye-tracking+data"\&btnG=\&hl=pt-BR\&as\_sdt=0,5\#0},
year = {2010}
}
@article{Kumar2013a,
address = {New York, New York, USA},
author = {Kumar, Neha and Parikh, Tapan S.},
doi = {10.1145/2470654.2481396},
isbn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {2863},
publisher = {ACM Press},
title = {{Mobiles, music, and materiality}},
url = {http://dl.acm.org/citation.cfm?doid=2470654.2481396},
year = {2013}
}
@article{Rodriguez2005,
author = {a.a. Rodriguez and Metzger, R.P. and Cifdaloz, O. and Dhirasakdanon, T.},
doi = {10.1109/TE.2004.842915},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Education/2005/Rodriguez et al/Rodriguez et al. - 2005 - Description of a Modeling, Simulation, Animation, and Real-Time Control (MoSART) Environment for a Class of El.pdf:pdf},
issn = {0018-9359},
journal = {IEEE Transactions on Education},
month = aug,
number = {3},
pages = {359--374},
title = {{Description of a Modeling, Simulation, Animation, and Real-Time Control (MoSART) Environment for a Class of Electromechanical Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1495643},
volume = {48},
year = {2005}
}
@article{Kolivand2013,
abstract = {Outdoor rendering is an attractive topic in computer graphics. In this paper our main concern is to reveal the interaction between sky color and virtual objects in Mixed Reality environments. Although registration and tracking are two of the main issues in building effective Augmented Reality (AR) systems the creation of more realistic virtual objects indistinguishable from their real-world counterparts is our target which is currently the ultimate goal in AR. Two classes of sky color generation are employed to reveal the outdoor-element interaction. Virtual Sky Modelling (VSM) based on the Perez Model is capable of generating the sky color in a specific location, date and time. The second technique is to generate a virtual model based on the real image of the sky which is called in this paper Real Sky Modelling (RSM). Subsequently, preprocessing of the sky color bleeding is based on the radiosity technique to give the sky color effect to the virtual objects as well as the real ones. Through designing a test AR set-up and applying software and hardware the goal of a robust generation of realistic virtual objects with effect of sky color is achieved.},
author = {Kolivand, Hoshang and Sunar, Mohd Shahrizal},
doi = {10.1007/s11042-013-1494-9},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Multimedia Tools and Applications/2013/Kolivand, Sunar/Kolivand, Sunar - 2013 - Covering photo-realistic properties of outdoor components with the effects of sky color in mixed reality.pdf:pdf},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
keywords = {augmented reality,illumination},
mendeley-tags = {augmented reality,illumination},
month = may,
title = {{Covering photo-realistic properties of outdoor components with the effects of sky color in mixed reality}},
url = {http://link.springer.com/article/10.1007/s11042-013-1494-9 http://link.springer.com/10.1007/s11042-013-1494-9},
year = {2013}
}
@article{Helbing1995,
abstract = {It is suggested that the motion of pedestrians can be described as if they would be subject to ‘‘social forces.’’ These ‘‘forces’’ are not directly exerted by the pedestrians’ personal environment, but they are a measure for the internal motivations of the individuals to perform certain actions (movements). The corresponding force concept is discussed in more detail and can also be applied to the description of other behaviors. In the presented model of pedestrian behavior several force terms are essential: first, a term describing the acceleration towards the desired velocity of motion; second, terms reflecting that a pedestrian keeps a certain distance from other pedestrians and borders; and third, a term modeling attractive effects. The resulting equations of motion of nonlinearly coupled Langevin equations. Computer simulations of crowds of interacting pedestrians show that the social force model is capable of describing the self-organization of several observed collective effects of pedestrian behavior very realistically.},
author = {Helbing, Dirk and Moln\'{a}r, P\'{e}ter},
doi = {10.1103/PhysRevE.51.4282},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Physical Review E/1995/Helbing, Moln\'{a}r/Helbing, Moln\'{a}r - 1995 - Social force model for pedestrian dynamics.pdf:pdf},
issn = {1063-651X},
journal = {Physical Review E},
keywords = {crowd simulation,path planning},
mendeley-tags = {crowd simulation,path planning},
month = may,
number = {5},
pages = {4282--4286},
publisher = {American Physical Society},
shorttitle = {Phys. Rev. E},
title = {{Social force model for pedestrian dynamics}},
url = {http://link.aps.org/doi/10.1103/PhysRevE.51.4282},
volume = {51},
year = {1995}
}
@article{Juhola2006,
abstract = {Eye movements have been investigated in several areas of medicine and also elsewhere, such as in psychology or even in the development of human-computer interfaces. In the last few years we have designed a technique to stimulate, measure and analyze vestibulo-ocular reflex eye movements. In the otoneurological literature these are seen as a novel and promising means of revealing certain disorders and diseases associated with vertigo. Vestibulo-ocular reflex is stimulated by impulsive head movements. We developed the present pattern recognition technique to detect the stimulus (impulsive head movements) and the vestibulo-ocular reflex (response eye movements) generated from signals and to compute the latency and the gain values between them. Using our technique to calculate these attributes, we obtained clearly different results for a group of 22 dizzy patients than for a group of 30 healthy subjects.},
author = {Juhola, Martti and Aalto, Heikki and Hirvonen, Timo},
doi = {10.1007/s10439-006-9129-1},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Annals of biomedical engineering/2006/Juhola, Aalto, Hirvonen/Juhola, Aalto, Hirvonen - 2006 - A signal analysis technique of vestibulo-ocular reflex stimulated with impulsive head movements.pdf:pdf},
issn = {0090-6964},
journal = {Annals of biomedical engineering},
keywords = {Automated,Automated: methods,Computer-Assisted,Computer-Assisted: instrumentation,Dizziness,Dizziness: diagnosis,Eye Movements,Female,Head Movements,Humans,Male,Pattern Recognition,Reflex,Signal Processing,Vertigo,Vertigo: diagnosis,Vestibulo-Ocular,gaze analysis,nystagmus},
mendeley-tags = {gaze analysis,nystagmus},
month = jul,
number = {7},
pages = {1213--25},
pmid = {16786396},
title = {{A signal analysis technique of vestibulo-ocular reflex stimulated with impulsive head movements.}},
url = {http://link.springer.com/article/10.1007/s10439-006-9129-1 http://www.ncbi.nlm.nih.gov/pubmed/16786396},
volume = {34},
year = {2006}
}
@book{Thalmann2012,
annote = {Aspectos interessantes sobre a Simula\c{c}\~{a}o de Pessoas
- Desempenho, atrasos (se tempo real)
- Qualidade
- Escalabilidade
- Usabilidade
-- Intera\c{c}\~{a}o com indiv\'{\i}duo
-- Intera\c{c}\~{a}o com grupo
-- Intera\c{c}\~{a}o com popula\c{c}\~{a}o
-- Intera\c{c}\~{a}o com ferramentas de autoria
--- Modelagem de pessoas
--- Inser\c{c}\~{a}o no ambiente (espacial e temporal)
-- Extra\c{c}\~{a}o de caracter\'{\i}sticas em v\'{\i}deos
-- Treinamento: como objeto de simula\c{c}\~{a}o ou sobre simula\c{c}\~{a}o
-- Emerg\^{e}ncia
-- Cibern\'{e}tica
-- Vida Artificial
-- Intera\c{c}\~{a}o Humano-Computador
--- Percep\c{c}\~{a}o
--- Mem\'{o}ria
--- Aten\c{c}\~{a}o
--- Intera\c{c}\~{a}o Espacial (Aura, Nimbo, Foco e Consci\^{e}ncia)},
author = {Thalmann, Daniel and Musse, Soraia Raupp},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2012/Thalmann, Musse/Thalmann, Musse - 2012 - Crowd Simulation.pdf:pdf},
isbn = {144714449X},
keywords = {crowd simulation},
mendeley-tags = {crowd simulation},
pages = {338},
publisher = {Springer},
title = {{Crowd Simulation}},
url = {http://www.amazon.com/Crowd-Simulation-Daniel-Thalmann/dp/144714449X},
year = {2012}
}
@inproceedings{Heo2013,
abstract = {The possibility of using shear forces is being explored recently as a method to enrich touch screen interaction. However, most of the related studies are restricted to the case of single-point shear forces, possibly owing to the difficulty of independently sensing shear forces at multiple touch points. In this paper, we propose indirect methods to estimate shear forces using the movement of contact areas. These methods enable multi-point shear force estimation, where the estimation is done for each finger independently. We show the feasibility of these methods through an informal user study with a demo application utilizing these methods.},
address = {New York, New York, USA},
author = {Heo, Seongkook and Lee, Geehyuk},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13},
doi = {10.1145/2468356.2479536},
file = {:home/acmt/Dropbox/Documentos/Mendeley/CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13/2013/Heo, Lee/Heo, Lee - 2013 - Indirect shear force estimation for multi-point shear force operations.pdf:pdf},
isbn = {9781450319522},
pages = {2835},
publisher = {ACM Press},
title = {{Indirect shear force estimation for multi-point shear force operations}},
url = {http://dl.acm.org/citation.cfm?doid=2468356.2479536},
year = {2013}
}
@techreport{Wangenheim2005,
address = {Florian\'{o}polis},
author = {Wangenheim, A.V. and Wagner, H.M.},
institution = {UFSC},
pages = {87--114},
title = {{Curvas Param\'{e}tricas}},
url = {http://www.inf.ufsc.br/~grafica/CG4.pdf},
year = {2005}
}
@article{Harma2004,
author = {H\"{a}rm\"{a}, A and Jakka, J and Tikander, M},
file = {::},
journal = {Journal of the Audio  \ldots},
keywords = {augmented reality,sound},
mendeley-tags = {augmented reality,sound},
title = {{Augmented reality audio for mobile and wearable appliances}},
url = {http://www.aes.org/e-lib/browse.cfm?elib=13010},
year = {2004}
}
@inproceedings{VanToll2011,
abstract = {Virtual characters often need to plan visually convincing paths through a complicated environment. For example, a traveler may need to walk from an airport entrance to a staircase, descend the staircase, walk to a shuttle, ride the shuttle to a destination, ride an elevator back to the ground floor, and finally move on the ground floor again to reach the desired airplane. Most previous research only supports path planning in a single plane because the underlying data structures are two-dimensional. The goal of this paper is to permit visually convincing paths to be efficiently computed in a multi-layered environment such as an airport or a multi-storey building. We describe an algorithm to create a navigation mesh, and our implementation demonstrates the feasibility of the approach. A multi-layered environment is represented by a set of two dimensional layers and a set of connections. Each layer is a collection of two-dimensional polygons that all lie in a single plane, and each connection provides a means of moving between layers. We first compute the traditional medial axis of each two dimensional layer in the environment. The connections are then used to iteratively merge this collection of medial axes into a single data structure. By adding a linear number of line segments to this structure, we obtain a navigation mesh that mathematically describes the walkable areas in a multi layered environment. This mesh can easily be input into existing planners to generate visually convincing paths for thousands of virtual characters in real-time.},
address = {San Francisco, CA},
author = {van Toll, W. and Cook, a. F. and Geraerts, R.},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6094790},
isbn = {978-1-61284-456-5},
month = sep,
pages = {3526--3532},
publisher = {IEEE},
title = {{Navigation meshes for realistic multi-layered environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6094790},
year = {2011}
}
@inproceedings{Koyama2003,
abstract = {This paper proposes a method to realize a 3D video display system that can capture video from multiple cameras, reconstruct 3D models and transmit 3D video data in real time. We represent a target object with a simplified 3D model consisting of a single plane and a 2D texture extracted from multiple cameras. This 3D model is simple enough to be transmitted via a network. We have developed a prototype system that can capture multiple videos, reconstruct 3D models, transmit the models via a network, and display 3D video in real time. A 3D video of a typical soccer scene that includes a dozen players was processed at 26 frames per second.},
address = {Tokyo},
annote = {        From Duplicate 1 (                           Live mixed-reality 3D video in soccer stadium                         - Koyama, T; Kitahara, I; Ohta, Y )
                
        
        
      },
author = {Koyama, T. and Kitahara, I. and Ohta, Y.},
booktitle = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
doi = {10.1109/ISMAR.2003.1240701},
isbn = {0-7695-2006-5},
keywords = {2D texture,3D model,3D reconstruction,3D video data,3D video display system,Cameras,Data engineering,Image reconstruction,Intelligent robots,Layout,Prototypes,Real time systems,Space technology,Three dimensional displays,Virtual reality,computer vision,image media,image representation,mixed reality,multiple cameras,prototype system,real time system,reconstruction,soccer scene,soccer stadium,sport,target object,three-dimensional displays,video capture,visual information},
mendeley-tags = {reconstruction},
pages = {178--186},
publisher = {IEEE Comput. Soc},
shorttitle = {Mixed and Augmented Reality, 2003. Proceedings. Th},
title = {{Live mixed-reality 3D video in soccer stadium}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1240701 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240701},
year = {2003}
}
@inproceedings{Mao2010,
abstract = {In this paper, we present a novel parallelizing method for crowd simulators constructed with a continuum model rather than an agent-based model. The basic idea is to partition a crowded virtual environment into some districts, each of which keeps its own dynamic continuum fields and has several transitional blocks to make individuals keep continuum motion from one district to another. Our method makes continuum models to be parallelizable while preserving their existing superiority of generating smooth motion. Moreover, for most of large-scale applications, our partitioning method effectively simplifies the complexity of simulation. Experiments show that our method has achieved super-linear speedup and could employ more than one hundred worker processors to simulate 1 million people in an area of 672,400m2.},
address = {New York, New York, USA},
author = {Mao, Tianlu and Jiang, Hao and Li, Jian and Zhang, Yanfeng and Xia, Shihong and Wang, Zhaoqi},
booktitle = {Proceedings of the 17th ACM Symposium on Virtual Reality Software and Technology - VRST '10},
doi = {10.1145/1889863.1889914},
isbn = {9781450304412},
pages = {231},
publisher = {ACM Press},
title = {{Parallelizing continuum crowds}},
url = {http://portal.acm.org/citation.cfm?doid=1889863.1889914},
year = {2010}
}
@inproceedings{Park2010,
abstract = {This paper presents a simple and active calibration technique of camera-projector systems based on planar homography. From the camera image of a planar calibration pattern, we generate a projector image of the pattern through the homography between the camera and the projector. To determine the coordinates of the pattern corners from the view of the projector, we actively project a corner marker from the projector to align the marker with the printed pattern corners. Calibration is done in two steps. First, four outer corners of the pattern are identified. Second, all other inner corners are identified. The pattern image from the projector is then used to calibrate the projector. Experimental results of two types of camera-projector systems show that the projection errors of both camera and projector are less than 1 pixel.},
author = {Park, Soon-Yong and Park, Go Gwang},
booktitle = {2010 20th International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2010.87},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2010 20th International Conference on Pattern Recognition/2010/Park, Park/Park, Park - 2010 - Active Calibration of Camera-Projector Systems Based on Planar Homography.pdf:pdf},
isbn = {978-1-4244-7542-1},
keywords = {anamorphism,keystone},
mendeley-tags = {anamorphism,keystone},
month = aug,
pages = {320--323},
publisher = {IEEE},
title = {{Active Calibration of Camera-Projector Systems Based on Planar Homography}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5597796 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5597796},
year = {2010}
}
@inproceedings{Lepetit2000,
abstract = {Realistic merging of virtual and real objects requires that the augmented patterns be correctly occluded by foreground objects. In this paper we propose a semi-automatic method for resolving occlusion in augmented reality which makes use of key-views. Once the user has outlined the occluding objects in the key-views, our system detects automatically these occluding objects in the intermediate views. A region of interest that contains the occluding objects is first computed from the outlined silhouettes. One of the main contribution of this paper is that this region takes into account the uncertainty on the computed interframe motion. Then a deformable region-based approach is used to recover the actual occluding boundary within the region of interest from this prediction},
address = {Hilton Head Island, SC},
author = {Lepetit, V and Berger, MO},
booktitle = {Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)},
doi = {10.1109/CVPR.2000.854794},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)/2000/Lepetit, Berger/Lepetit, Berger - 2000 - A semi-automatic method for resolving occlusion in augmented reality.pdf:pdf},
isbn = {0-7695-0662-3},
keywords = {augmented reality,occlusion},
mendeley-tags = {augmented reality,occlusion},
pages = {225--230},
publisher = {IEEE Comput. Soc},
title = {{A semi-automatic method for resolving occlusion in augmented reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=854794 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=854794},
volume = {2},
year = {2000}
}
@article{Delage2007,
abstract = {3d reconstruction from a single image is inherently an ambiguous problem. Yet when we look at a picture, we can often infer 3d information about the scene. Humans perform single-image 3d reconstructions by using a variety of single-image depth cues, for example, by recognizing objects and surfaces, and reasoning about how these surfaces are connected to each other. In this paper, we focus on the problem of automatic 3d reconstruction of indoor scenes, specifically ones (sometimes called “Manhattan worlds”) that consist mainly of orthogonal planes. We use a Markov random field (MRF) model to identify the different planes and edges in the scene, as well as their orientations. Then, an iterative optimization algorithm is applied to infer the most probable position of all the planes, and thereby obtain a 3d reconstruction. Our approach is fully automatic—given an input image, no human intervention is necessary to obtain an approximate 3d reconstruction.},
author = {Delage, E and Lee, H and Ng, AY},
doi = {10.1007/978-3-540-48113-3\_28},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Robotics Research/2007/Delage, Lee, Ng/Delage, Lee, Ng - 2007 - Automatic single-image 3d reconstructions of indoor manhattan world scenes.pdf:pdf},
journal = {Robotics Research},
keywords = {perspective,reconstruction,single view},
mendeley-tags = {perspective,reconstruction,single view},
title = {{Automatic single-image 3d reconstructions of indoor manhattan world scenes}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-48113-3\_28},
year = {2007}
}
@inproceedings{Song2008,
address = {New York, New York, USA},
author = {Song, Peng and Yu, Hang and Winkler, Stefan},
booktitle = {Proceedings of The 7th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry - VRCAI '08},
doi = {10.1145/1477862.1477871},
isbn = {9781605583358},
keywords = {finger interaction,finger tracking,mixed reality},
number = {June 2008},
pages = {1},
publisher = {ACM Press},
title = {{Vision-based 3D finger interactions for mixed reality games with physics simulation}},
url = {http://portal.acm.org/citation.cfm?doid=1477862.1477871},
volume = {8},
year = {2008}
}
@article{Hermens2010,
abstract = {When making a saccadic eye movement to a peripheral target, a simultaneous stimulus onset at central fixation generally increases saccadic latency, while offsets reduce latency ('gap effect'). Visual onsets remote from fixation also increase latency ('remote distractor effect'); however, the influence of remote visual offsets is less clear. Previous studies, which used a search task, found that remote offsets either facilitated, inhibited, or did nothing to saccade latencies towards a peripheral target. It cannot be excluded, however, that the target selection process in such search tasks influenced the results. We therefore simplified the task and asked participants to make eye movements to a predictable target. Simultaneously with target onset, either one or multiple remote stimulus onsets and offsets were presented. It was found that peripheral onsets increased saccade latencies, but offsets did not influence the initiation of a saccade to the target. Moreover, the number of onsets and offsets did not affect the results. These results suggest that earlier effects of remote stimulus offsets and of the number of remote distractor onsets reside in the target identification process of the visual search task rather than the competition between possible saccade goals. The results are discussed in the context of models of saccade target selection.},
annote = {Ver o que est\'{a} relacionado \`{a} segmenta\c{c}\~{a}o de dados do olhar},
author = {Hermens, Frouke and Walker, Robin},
doi = {10.1068/i0392},
file = {:home/acmt/Dropbox/Documentos/Mendeley/i-Perception/2010/Hermens, Walker/Hermens, Walker - 2010 - The influence of onsets and offsets on saccade programming.pdf:pdf},
issn = {2041-6695},
journal = {i-Perception},
keywords = {eye tracking,segmentation},
mendeley-tags = {eye tracking,segmentation},
month = jan,
number = {2},
pages = {83--94},
pmid = {23397028},
publisher = {Pion Ltd},
title = {{The influence of onsets and offsets on saccade programming.}},
url = {http://i-perception.perceptionweb.com/journal/I/article/i0392 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3563056\&tool=pmcentrez\&rendertype=abstract},
volume = {1},
year = {2010}
}
@book{Hart1999,
author = {Hart, Chris},
isbn = {0761959750},
keywords = {rbs},
mendeley-tags = {rbs},
pages = {230},
publisher = {SAGE Publications Ltd},
title = {{Doing a Literature Review: Releasing the Social Science Research Imagination (SAGE Study Skills Series)}},
url = {http://www.amazon.com/Doing-Literature-Review-Releasing-Imagination/dp/0761959750},
year = {1999}
}
@article{Gustavson2005a,
author = {Gustavson, Stefan},
file = {::},
title = {{Simplex noise demystified}},
year = {2005}
}
@inproceedings{Mensmann2008a,
author = {Mensmann, J\"{o}rg and Hinrichs, Klaus},
booktitle = {16th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision},
file = {::},
keywords = {Volume Rendering,illustration,volume cutting,volume deformation,volume rendering,voreen},
mendeley-tags = {Volume Rendering,voreen},
pages = {89--96},
title = {{Interactive Cutting Operations for Generating Anatomical Illustrations from Volumetric Data Sets}},
volume = {d},
year = {2008}
}
@article{Bajaj2001a,
author = {Bajaj, C. and Blanke, W.},
file = {::},
journal = {Proceedings IEEE 2001 Symposium on Parallel and Large-Data Visualization and Graphics (Cat. No.01EX520)},
keywords = {Parallel Rendering,gressive mesh,metabuffer,multi-resolution,parallel and out-of-core isocontouring,parallel rendering,pro-},
mendeley-tags = {Parallel Rendering},
pages = {51--150},
publisher = {Ieee},
title = {{Scalable isosurface visualization of massive datasets on COTS clusters}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=964404},
year = {2001}
}
@article{McGuire2006a,
address = {New York, New York, USA},
author = {McGuire, Morgan and Stathis, George and Pfister, Hanspeter and Krishnamurthi, Shriram},
file = {::},
journal = {Proceedings of the 2006 symposium on Interactive 3D graphics and games - SI3D '06},
pages = {79},
publisher = {ACM Press},
title = {{Abstract shade trees}},
url = {http://portal.acm.org/citation.cfm?doid=1111411.1111425},
year = {2006}
}
@article{Dachsbacher2003a,
author = {Dachsbacher, Carsten and Vogelgsang, Christian and Stamminger, Marc},
file = {::},
journal = {ACM Transactions on Graphics},
keywords = {Point-Based Rendering,hardware accelerated graphics,level,point rendering},
mendeley-tags = {Point-Based Rendering},
month = jul,
number = {3},
pages = {657},
title = {{Sequential point trees}},
url = {http://portal.acm.org/citation.cfm?doid=882262.882321},
volume = {22},
year = {2003}
}
@article{Bierens2004,
address = {Cambridge},
author = {Bierens, Herman J.},
doi = {10.1017/CBO9780511754012},
isbn = {9780511754012},
publisher = {Cambridge University Press},
title = {{Introduction to the Mathematical and Satistical Foundations of Econometrics}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511754012},
year = {2004}
}
@article{Mehra2009a,
author = {Mehra, Ravish and Zhou, Qingnan and Long, Jeremy and Sheffer, Alla and Gooch, Amy and Mitra, Niloy J.},
file = {::},
journal = {Journal of the Electrochemical Society},
keywords = {communities have resulted in,curve network,geometry,good,growth of online modeling,in such collections do,large col-,lections of such models,most models of man-made,not satisfy the notion,npr,objects present,of,perception,shape analysis},
title = {{Abstraction of Man-Made Shapes}},
year = {2009}
}
@inproceedings{Crassin2009a,
author = {Crassin, Cyril and Neyret, Fabrice and Lefebvre, Sylvain and Eisemann, Elmar},
booktitle = {ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D)},
file = {::},
title = {{GigaVoxels: Ray-Guided Streaming for Efficient and Detailed Voxel Rendering}},
url = {ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D)},
volume = {d},
year = {2009}
}
@article{Irving2007a,
author = {Irving, Geoffrey and Schroeder, Craig and Fedkiw, Ronald},
file = {::},
journal = {ACM Transactions on Graphics},
keywords = {collisions,deformable solids,incompressibility},
month = jul,
number = {3},
pages = {13},
title = {{Volume conserving finite element simulations of deformable models}},
url = {http://portal.acm.org/citation.cfm?doid=1276377.1276394 http://naml.us/~irving/},
volume = {26},
year = {2007}
}
@inproceedings{Liang2009a,
author = {Liang, XiaoHui and Zhao, QinPing and He, ZhiYing and Xie, Ke and Liu, YuBo},
file = {::},
keywords = {Point-Based Rendering},
mendeley-tags = {Point-Based Rendering},
number = {8},
title = {{A point-based rendering approach for real-time interaction on mobile devices}},
volume = {3},
year = {2009}
}
@incollection{Card2001b,
author = {Card, Drew and Mitchell, Jason L},
booktitle = {ShaderX},
file = {::},
keywords = {NPR,Non-Photorealistic Rendering,Real-Time Rendering,Shaders},
mendeley-tags = {NPR,Non-Photorealistic Rendering,Real-Time Rendering,Shaders},
title = {{Non-photorealistic rendering with pixel and vertex shaders}},
year = {2001}
}
@article{Thiruvathukal2008b,
author = {Thiruvathukal, Editors George K and L\"{a}ufer, Konstantin and Messmer, By Peter and Mullowney, Paul J and Granger, Brian E},
file = {::},
journal = {Compare A Journal Of Comparative Education},
keywords = {gpu,python},
mendeley-tags = {gpu,python},
title = {{GPULib: GPU Computing in High-Level Languages}},
year = {2008}
}
@article{Hunt2006,
author = {Hunt, Warren and Mark, William and Stoll, Gordon},
doi = {10.1109/RT.2006.280218},
file = {::},
isbn = {1-4244-0693-5},
journal = {2006 IEEE Symposium on Interactive Ray Tracing},
keywords = {approximation,error,heuristic,kd-tree},
month = sep,
number = {v},
pages = {81--88},
publisher = {Ieee},
title = {{Fast kd-tree Construction with an Adaptive Error-Bounded Heuristic}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4061549},
year = {2006}
}
@article{McGuire2004b,
address = {New York, New York, USA},
author = {McGuire, Morgan and Hughes, John F.},
file = {::},
journal = {Proceedings of the 3rd international symposium on Non-photorealistic animation and rendering - NPAR '04},
keywords = {GPU,NPR,contour,shadow volume,silhouette},
pages = {35},
publisher = {ACM Press},
title = {{Hardware-determined feature edges}},
url = {http://portal.acm.org/citation.cfm?doid=987657.987663},
year = {2004}
}
@article{Sugerman2009a,
author = {Sugerman, Jeremy and Fatahalian, Kayvon and Boulos, Solomon and Akeley, Kurt and Hanrahan, Pat},
file = {::},
journal = {ACM Transactions on Graphics},
keywords = {3D Graphics},
mendeley-tags = {3D Graphics},
number = {1},
pages = {1--11},
title = {{Gramps: A Programming Model for Graphics Pipelines}},
url = {http://portal.acm.org/citation.cfm?doid=1477926.1477930},
volume = {28},
year = {2009}
}
@article{Zhaia,
author = {Zhai, Yun and Liu, Jingen and Cao, Xiaochun and Basharat, Arslan and Hakeem, Asaad and Ali, Saad and Shah, Mubarak and Grana, Costantino and Cucchiara, Rita},
file = {::},
journal = {Search},
number = {1},
title = {{Video Understanding and Content-Based Retrieval}}
}
@inproceedings{Zafar2010a,
address = {Saarbrucken, Germany},
author = {Zafar, Fahad and Olano, Marc and Curtis, Aaron},
booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on High Performance Graphics},
file = {::},
keywords = {GPU,Noise,Real-Time Rendering,Shaders,cryptographic hash,noise,shadows},
mendeley-tags = {GPU,Noise,Real-Time Rendering,Shaders},
pages = {25--27},
title = {{GPU Random Numbers via the Tiny Encryption Algorithm}},
url = {http://www.csee.umbc.edu/~olano/papers/},
volume = {xx},
year = {2010}
}
@article{Ren2002a,
author = {Ren, Liu and Pfister, Hanspeter and Zwicker, Matthias},
file = {::},
journal = {Computer Graphics Forum},
keywords = {Point-Based Rendering},
mendeley-tags = {Point-Based Rendering},
month = sep,
number = {3},
pages = {461--470},
title = {{Object Space EWA Surface Splatting: A Hardware Accelerated Approach to High Quality Point Rendering}},
url = {http://blackwell-synergy.com/doi/abs/10.1111/1467-8659.00606},
volume = {21},
year = {2002}
}
@article{Bigler2006a,
author = {Bigler, J. and Stephens, A. and Parker, S.G.},
file = {::},
journal = {Proceedings of the IEEE Symposium on Interactive Ray Tracing},
keywords = {interactive ray tracing,ray tracing,software architec-},
pages = {187--195},
publisher = {Citeseer},
title = {{Design for parallel interactive ray tracing systems}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Design+for+Parallel+Interactive+Ray+Tracing+Systems\#0},
year = {2006}
}
@article{Hecker2008a,
author = {Hecker, Chris and Raabe, Bernd and Enslow, Ryan W. and DeWeese, John and Maynard, Jordan and van Prooijen, Kees},
file = {::},
journal = {ACM Transactions on Graphics},
keywords = {ated content,character animation,games,inverse kinematics,motion retargeting,procedural animation,user gene},
month = aug,
number = {3},
pages = {1},
title = {{Real-time motion retargeting to highly varied user-created morphologies}},
url = {http://portal.acm.org/citation.cfm?doid=1360612.1360626},
volume = {27},
year = {2008}
}
@article{Maim2009,
abstract = {A solution that allows each individual to look unique in a real-time large crowd simulation. First, it provides a simple, efficient method for attaching accessories to individuals to modify their look. Second, it provides a new, generic technique based on segmentation maps for adding detailed color variety and patterns to human meshes as well as accessories. Both methods are scalable to suit all human levels of detail exploited in crowd simulations; that is, impostors and rigid and deformable meshes. Tests and comparisons show that the algorithm provides the crowd with an appealing visual aspect and is adequate for real-time simulations of thousands of unique characters.},
author = {Ma\"{\i} m, Jonathan and Yersin, Barbara and Thalmann, Daniel},
journal = {Computer Graphics and Applications, IEEE},
number = {6},
pages = {82 -- 90},
title = {{Unique Character Instances for Crowds}},
url = {http://ieeexplore.ieee.org/xpl/freeabs\_all.jsp?isnumber=5307631\&arnumber=5307646},
volume = {29},
year = {2009}
}
@inproceedings{Rusinkiewicz2000a,
author = {Rusinkiewicz, S. and Levoy, Marc},
booktitle = {Proceedings of the 27th annual conference on Computer graphics and interactive techniques},
file = {::},
keywords = {3d scan-,Point-Based Rendering,algorithms,and software components of,compression algorithms,data they produce,improvements in the hardware,level of detail,over the past several,rendering systems,spatial data structures,the large amounts of,years},
mendeley-tags = {Point-Based Rendering},
pages = {343--352},
publisher = {ACM Press/Addison-Wesley Publishing Co. New York, NY, USA},
title = {{QSplat: A multiresolution point rendering system for large meshes}},
url = {http://portal.acm.org/citation.cfm?id=344779.344940},
year = {2000}
}
@inproceedings{Lagae2010a,
author = {Lagae, A. and Lefebvre, S. and Cook, R. and DeRose, T. and Drettakis, G. and Ebert, D.S. and Lewis, J.P. and Perlin, K. and Zwicker, M.},
booktitle = {EG 2010 Proceedings},
file = {::},
keywords = {analysis,anisotropic noise,anti-aliasing,filtering,gabor noise,noise,perlin noise,power spectrum estimation,procedural,procedural modeling,procedural noise function,procedural texture,solid noise,solid texture,sparse convolution noise,spectral,spot noise,stochastic modeling,stochastic process,surface noise,texture synthesis,wavelet noise},
title = {{State of the Art in Procedural Noise Functions}},
url = {http://www.cs.kuleuven.be/~ares/publications/LLCDDELPZ10STARPNF/LLCDDELPZ10STARPNF.pdf},
year = {2010}
}
@article{Linsen2007a,
author = {Linsen, Lars and Muller, Karsten and Rosenthal, Paul},
file = {::},
journal = {Journal of WSCG},
keywords = {Point-Based Rendering,photo-realistic rendering,point-based rendering,ray tracing,splatting},
mendeley-tags = {Point-Based Rendering},
number = {1-3},
pages = {51--58},
title = {{Splat-based ray tracing of point clouds}},
url = {http://www.cse.iitb.ac.in/~kashyap/seminar/final/Splat-based Ray Tracing of Point Clouds.pdf},
volume = {15},
year = {2007}
}
@inproceedings{Deines2007a,
author = {Deines, Eduard and Michel, Frank and Hering-Bertram, Martin and Mohring, Jan and Hagen, Hans},
booktitle = {19th International Congress on Acoustics (ICA) 2007},
file = {::},
number = {September},
pages = {2--7},
title = {{Simulation, visualization, and virtual reality based modeling of room acoustics}},
url = {http://www-hagen.informatik.uni-kl.de/~bertram/papers/},
year = {2007}
}
@article{Reniers2009a,
author = {Reniers, Dennie and Telea, A.},
file = {::},
journal = {Computing and Visualization in Science},
keywords = {Point-Based Rendering,algebraic multigrid,extreme model simplification,point set models,real-time rendering},
mendeley-tags = {Point-Based Rendering},
number = {1},
pages = {9--22},
publisher = {Springer},
title = {{Extreme simplification and rendering of point sets using algebraic multigrid}},
url = {http://www.springerlink.com/index/rkv5571752q38wr8.pdf},
volume = {12},
year = {2009}
}
@article{Murnaghan1961,
author = {Murnaghan, F. D. and Hoffman, Kenneth and Kunze, Ray},
doi = {10.2307/2002915},
issn = {00255718},
journal = {Mathematics of Computation},
month = jul,
number = {75},
pages = {303},
title = {{Linear Algebra.}},
url = {http://www.jstor.org/stable/2002915?origin=crossref},
volume = {15},
year = {1961}
}
@article{Reshetov2009a,
address = {New York, New York, USA},
author = {Reshetov, Alexander},
file = {::},
journal = {Proceedings of the 1st ACM conference on High Performance Graphics - HPG '09},
keywords = {Antialiasing,Real-Time Rendering,antialiasing,image enhancement,morphological analysis},
mendeley-tags = {Antialiasing,Real-Time Rendering},
pages = {109},
publisher = {ACM Press},
title = {{Morphological antialiasing}},
url = {http://portal.acm.org/citation.cfm?doid=1572769.1572787},
year = {2009}
}
@article{Haseman2008,
address = {Berkeley, CA},
author = {Haseman, Chris},
doi = {10.1007/978-1-4302-1063-4},
isbn = {978-1-4302-1064-1},
publisher = {Apress},
title = {{Android Essentials}},
url = {http://www.springerlink.com/index/10.1007/978-1-4302-1063-4},
year = {2008}
}
@article{Benard2010,
address = {New York, New York, USA},
author = {B\'{e}nard, Pierre and Cole, Forrester and Golovinskiy, Aleksey and Finkelstein, Adam},
file = {::},
journal = {Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering - NPAR '10},
keywords = {line drawing,non-photorealistic rendering,temporal},
pages = {91},
publisher = {ACM Press},
title = {{Self-similar texture for coherent line stylization}},
url = {http://portal.acm.org/citation.cfm?doid=1809939.1809950 http://artis.imag.fr/Publications/2010/BCGF10/},
year = {2010}
}
@article{Everitta,
author = {Everitt, Cass},
file = {::},
journal = {Engineering},
keywords = {GPU,OIT,Transparency,graphics,nvidia},
mendeley-tags = {GPU,OIT,Transparency,graphics,nvidia},
title = {{Interactive Order-Independent Transparency}}
}
@article{Gaiazov2005a,
author = {Gaiazov, Leonid},
file = {::},
journal = {Science},
keywords = {gpu,graphics,npr,shaders},
mendeley-tags = {gpu,graphics,npr,shaders},
pages = {1--9},
title = {{Real-Time NPR System with Multiple Styles}},
year = {2005}
}
@article{E.Kee2011,
author = {{E. Kee}, M. K. Johnson and H. Farid},
journal = {IEEE Transactions on Information Forensics and Security},
keywords = {authentication verification jpg forensics},
number = {3},
pages = {1066--1075},
title = {{Digital image Authentication from JPEG headers}},
url = {http://www.cs.dartmouth.edu/~farid/Hany\_Farid/Papers/Entries/2011/6/4\_Digital\_image\_Authentication\_from\_JPEG\_headers.html},
volume = {6},
year = {2011}
}
@book{Mason2009,
author = {Mason, Charlie},
booktitle = {American journal of orthodontics and dentofacial orthopedics : official publication of the American Association of Orthodontists, its constituent societies, and the American Board of Orthodontics},
doi = {10.1016/j.ajodo.2009.10.012},
isbn = {0821807854},
issn = {1097-6752},
keywords = {Clinical Competence,General Practice, Dental,General Practice, Dental: standards,Humans,Orthodontic Appliances, Functional,Orthodontic Appliances, Removable,Orthodontics,Orthodontics: instrumentation,Orthodontics: standards,Tooth Movement,Tooth Movement: instrumentation,Tooth Movement: standards,Treatment Outcome},
month = dec,
number = {6},
pages = {759},
pmid = {19962587},
title = {{Mixed motives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21614721},
volume = {136},
year = {2009}
}
@article{Speakers2012,
author = {Speakers, Invited},
doi = {10.1093/carcin/bgs054},
issn = {1460-2180},
journal = {Carcinogenesis},
month = mar,
number = {3},
pages = {NP},
pmid = {22383472},
title = {{Table of contents.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22383472},
volume = {33},
year = {2012}
}
@book{Yang2005a,
address = {Hoboken, NJ, USA},
author = {Yang, Won Young and Cao, Wenwu and Chung, Tae-Sang and Morris, John},
doi = {10.1002/0471705195},
isbn = {9780471705192},
month = jan,
publisher = {John Wiley \& Sons, Inc.},
title = {{Applied Numerical Methods Using MATLAB®}},
url = {http://doi.wiley.com/10.1002/0471705195},
year = {2005}
}
@article{Gooch1998a,
author = {Gooch, Amy and Gooch, Bruce and Shirley, Peter and Cohen, Elaine},
file = {::},
title = {{A Non-Photorealistic Lighting Model For Automatic Technical Illustration.}},
year = {1998}
}
@article{Pfister2002a,
author = {Pfister, Hanspeter and Gross, Markus},
file = {::},
journal = {IEEE computer graphics and applications},
keywords = {Algorithms,Art,Computer Graphics,Computer-Aided Design,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Point-Based Rendering,Signal Processing, Computer-Assisted,Software,User-Computer Interface},
mendeley-tags = {Point-Based Rendering},
number = {4},
pages = {22--3},
title = {{Point-based computer graphics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15628082},
volume = {24},
year = {2002}
}
@inproceedings{Germs2001a,
author = {Germs, Rick and Jansen, F.W.},
booktitle = {Proc. of WSCG},
file = {::},
keywords = {geometric simplification,occlusion culling,virtual occluders,walkthrough visualization},
publisher = {Citeseer},
title = {{Geometric simplification for efficient occlusion culling in urban scenes}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.8189\&amp;rep=rep1\&amp;type=pdf},
volume = {2001},
year = {2001}
}
@phdthesis{Cole2009c,
address = {New York, New York, USA},
author = {Cole, Forrester},
booktitle = {ACM SIGGRAPH 2005 Courses on - SIGGRAPH '05},
file = {::},
keywords = {3D Graphics,NPR,Non-Photorealistic Rendering},
mendeley-tags = {3D Graphics,NPR,Non-Photorealistic Rendering},
number = {June},
pages = {1},
publisher = {ACM Press},
school = {Princeton},
title = {{Line drawings of 3D models}},
url = {http://www.cs.princeton.edu/gfx/pubs/\_2009\_LDO/index.php},
year = {2009}
}
@article{Cook2007a,
author = {Cook, Robert L. and Halstead, John and Planck, Maxwell and Ryu, David},
file = {::},
journal = {ACM Transactions on Graphics},
keywords = {000 leaves,Pixar,SIGGRAPH07,a plant with 320,figure 1,level of detail,simpli cation,stochastic sampling},
mendeley-tags = {Pixar,SIGGRAPH07},
month = jul,
number = {3},
pages = {79},
title = {{Stochastic simplification of aggregate detail}},
url = {http://portal.acm.org/citation.cfm?doid=1276377.1276476 http://graphics.pixar.com/library/},
volume = {26},
year = {2007}
}
@article{Zeng2009b,
author = {Zeng, Kun and Zhao, Mingtian and Xiong, Caiming and Zhu, Song-Chun},
file = {::},
journal = {ACM Transactions on Graphics},
number = {1},
pages = {1--11},
title = {{From image parsing to painterly rendering}},
url = {http://portal.acm.org/citation.cfm?doid=1640443.1640445},
volume = {29},
year = {2009}
}
@article{Sainz2004a,
author = {Sainz, Miguel and Parajola, Renato},
file = {::},
journal = {Computers \& Graphics},
keywords = {Point-Based Rendering,hardware acceleration,level-of-detail,point-based rendering},
mendeley-tags = {Point-Based Rendering},
pages = {869--879},
title = {{Point-based rendering techniques}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849304001530},
volume = {28},
year = {2004}
}
@inproceedings{Kircher2009a,
author = {Kircher, S. and Lawrance, A.},
booktitle = {Proceedings of the 2009 ACM SIGGRAPH Symposium on Video Games},
file = {::},
keywords = {Lighting,Real-Time Rendering,SIGGRAPH09: GPU,Shaders,alpha-,deferred shading,fast dynamic lighting,polygon shadows,real-time rendering},
mendeley-tags = {Lighting,Real-Time Rendering,SIGGRAPH09: GPU,Shaders},
pages = {39--45},
publisher = {ACM},
title = {{Inferred lighting: fast dynamic lighting and shadows for opaque and translucent objects}},
url = {http://portal.acm.org/citation.cfm?id=1581080 http://graphics.cs.uiuc.edu/~kircher/publications.html},
year = {2009}
}
@article{Filion2008c,
address = {New York, New York, USA},
author = {Filion, Dominic and McNaughton, Rob},
file = {::},
journal = {ACM SIGGRAPH 2008 classes on - SIGGRAPH '08},
pages = {133},
publisher = {ACM Press},
title = {{Starcraft 2 : Effects \& techniques}},
url = {http://portal.acm.org/citation.cfm?doid=1404435.1404441},
year = {2008}
}
@article{Habel2007a,
abstract = {This paper introduces a technique for rendering animated grass in real time. The technique uses front-to-back compositing of implicitly defined grass slices in a fragment shader and therefore significantly reduces the overhead associated with common vegetation rendering systems. We also introduce a texture-based animation scheme that combines global wind movements with local turbulences. Since the technique is confined to a fragment shader, it can be easily integrated into any rendering system and used as a material in existing scenes.},
author = {Habel, Ralf and Wimmer, Michael and Jeschke, Stefan},
file = {::},
journal = {Journal of WSCG},
keywords = {3D Graphics,Grass,OGRE,Real-Time Rendering,gpu programming,natural phenomena,natural scene rendering,real-time rendering},
mendeley-tags = {3D Graphics,Grass,OGRE,Real-Time Rendering},
pages = {123----128},
title = {{Instant Animated Grass}},
url = {http://www.cg.tuwien.ac.at/research/publications/2007/Habel\_2007\_IAG/},
volume = {15},
year = {2007}
}
@article{Banks1997,
abstract = {This is an expanded version of talks given by the author at the Trieste Spring School on Supergravity and Superstrings in April of 1997 and at the accompanying workshop. The manuscript is intended to be a mini-review of Matrix Theory. The motivations and some of the evidence for the theory are presented, as well as a clear statement of the current puzzles about compactification to low dimensions.},
archivePrefix = {arXiv},
arxivId = {hep-th/9710231},
author = {Banks, Tom},
doi = {10.1016/S0920-5632(98)00130-3},
eprint = {9710231},
journal = {Matrix},
month = oct,
number = {September},
pages = {72},
primaryClass = {hep-th},
title = {{Matrix Theory}},
url = {http://arxiv.org/abs/hep-th/9710231},
year = {1997}
}
@article{Leitmann1982,
author = {Leitmann, George},
doi = {10.1115/1.3139697},
journal = {Journal of Dynamic Systems, Measurement, and Control},
number = {2},
pages = {202},
title = {{The Calculus of Variations and Optimal Control}},
volume = {104},
year = {1982}
}
@article{Case2009a,
author = {Case, Colleen and Cunningham, Steve},
file = {::},
pages = {1--6},
title = {{Teaching Computer Graphics in Context Computer Graphics Education 09 Workshop Munich, Germany, March 31-April 1, 2009}},
year = {2009}
}
@inproceedings{Ropinski2005a,
author = {Ropinski, Timo and Steinicke, Frank and Hinrichs, KH},
booktitle = {proceedings of the 10th International Fall Workshop on Vision, Modeling, and Visualization},
file = {::},
keywords = {Volume Rendering,voreen},
mendeley-tags = {Volume Rendering,voreen},
pages = {273--280},
publisher = {Citeseer},
title = {{Interactive importance-driven visualization techniques for medical volume data}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.4332\&amp;rep=rep1\&amp;type=pdf},
volume = {D},
year = {2005}
}
@article{Bridson2007a,
author = {Bridson, Robert and Houriham, Jim and Nordenstam, Marcus},
doi = {10.1145/1276377.1276435},
file = {::},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {a of distance from,a smoothed step function,by a modu-,figure 1,fluids,incompressible 2d noise with,lation function,n,noise,procedural animation,pute the potential $\psi$,solid boundaries,t,the,to com-,turbulence,we multiply scaled noise,x},
month = jul,
number = {3},
pages = {46},
title = {{Curl-noise for procedural fluid flow}},
url = {http://portal.acm.org/citation.cfm?doid=1276377.1276435 http://www.cs.ubc.ca/~rbridson/},
volume = {26},
year = {2007}
}
@article{Mikkelsen2010a,
author = {Mikkelsen, Morten S},
file = {::},
keywords = {GPU,Real-Time Rendering},
mendeley-tags = {GPU,Real-Time Rendering},
title = {{Bump Mapping Unparametrized Surfaces on the GPU}},
year = {2010}
}
@inproceedings{Stegmaier2005a,
author = {Stegmaier, Simon and Klein, Thomas},
booktitle = {Volume Graphics 2005},
file = {::},
title = {{A Simple and Flexible Volume Rendering Framework for Graphics-Hardware-based Raycasting}},
year = {2005}
}
@inproceedings{McGuire2010a,
author = {McGuire, M.},
booktitle = {ACM SIGGRAPH/Eurographics High Performance Graphics 2009},
file = {::},
keywords = {AO,Ambient Occlusion,Real-Time Rendering},
mendeley-tags = {AO,Ambient Occlusion,Real-Time Rendering},
pages = {1},
publisher = {ACM},
title = {{Ambient occlusion volumes}},
url = {http://graphics.cs.williams.edu/papers/AOVHPG10/},
year = {2010}
}
@article{Shanmugam2007a,
address = {New York, New York, USA},
author = {Shanmugam, Perumaal and Arikan, Okan},
doi = {10.1145/1230100.1230113},
file = {::},
isbn = {9781595936288},
journal = {Proceedings of the 2007 symposium on Interactive 3D graphics and games - I3D '07},
keywords = {Computer Graphics,GPU,I3D07,Real-Time Rendering,Shaders,ambient occlusion,gpu,real-time render-,soft shadows},
mendeley-tags = {Computer Graphics,GPU,I3D07,Real-Time Rendering,Shaders},
pages = {73},
publisher = {ACM Press},
title = {{Hardware accelerated ambient occlusion techniques on GPUs}},
url = {http://portal.acm.org/citation.cfm?doid=1230100.1230113},
year = {2007}
}
@inproceedings{Roßler2008,
author = {R\"{o}\ss ler, F. and Botchen, RP and Ertl, Thomas},
booktitle = {Proceedings of IEEE Pacific Visualization Symposium},
file = {::},
pages = {17--24},
title = {{Dynamic shader generation for flexible multi-volume visualization}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Dynamic+Shader+Generation+for+Flexible+Multi-Volume+Visualization\#0},
volume = {2008},
year = {2008}
}
@article{Palubicki2009a,
author = {Palubicki, Wojciech and Horel, Kipp and Longay, Steven and Runions, Adam and Lane, Brendan},
file = {::},
journal = {ACM Transactions on Graphics},
keywords = {3D Graphics,Procedural Modeling,apical control,bud fate,emergence,generative tree model,interactive-procedural modeling,tree development},
mendeley-tags = {3D Graphics,Procedural Modeling},
month = apr,
number = {2},
pages = {1--10},
publisher = {ACM New York, NY, USA},
title = {{Self-organizing tree models for image synthesis}},
url = {http://portal.acm.org/citation.cfm?doid=1516522.1516530},
volume = {28},
year = {2009}
}
@article{Krugera,
author = {Kruger, J. and Westermann, R.},
file = {::},
journal = {IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency Control},
keywords = {programmable graphics hard-,volume rendering},
pages = {287--292},
publisher = {Ieee},
title = {{Acceleration techniques for GPU-based volume rendering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1250384}
}
@inproceedings{Parker2010a,
author = {Parker, Steven G and Bigler, James and Dietrich, Andreas and Friedrich, Heiko and Hoberock, Jared and Luebke, David and Mcallister, David and Stich, Martin},
booktitle = {SIGGRAPH 2010},
file = {::},
keywords = {graphics hardware,graphics systems,ray tracing},
title = {{OptiX : A General Purpose Ray Tracing Engine}},
year = {2010}
}
@article{Ritter2007a,
author = {Ritter, Justin and King, Chris and Gronsky, Stefan},
file = {::},
journal = {ACM SIGGRAPH 2007 sketches},
keywords = {Lighting,Pixar,SIGGRAPH07},
mendeley-tags = {Lighting,Pixar,SIGGRAPH07},
pages = {2007--2007},
title = {{Fast, Soft Reflections Using Radiance Caches}},
url = {http://portal.acm.org/citation.cfm?id=1278780.1278843 http://graphics.pixar.com/library/SoftReflections/},
year = {2007}
}
@article{Oliveira2000a,
address = {New York, New York, USA},
author = {Oliveira, Manuel M. and Bishop, Gary and McAllister, David},
file = {::},
journal = {Proceedings of the 27th annual conference on Computer graphics and interactive techniques - SIGGRAPH '00},
keywords = {Computer Graphics},
mendeley-tags = {Computer Graphics},
pages = {359--368},
publisher = {ACM Press},
title = {{Relief texture mapping}},
url = {http://portal.acm.org/citation.cfm?doid=344779.344947},
year = {2000}
}
@article{Nichols2009c,
author = {Nichols, Greg and Shopf, Jeremy and Wyman, Chris},
file = {::},
journal = {Computer Graphics Forum},
keywords = {3D Graphics,EG09,Global Illumination,Real-Time Rendering,Shaders},
mendeley-tags = {3D Graphics,EG09,Global Illumination,Real-Time Rendering,Shaders},
month = jun,
number = {4},
pages = {1141--1149},
title = {{Hierarchical Image-Space Radiosity for Interactive Global Illumination}},
url = {http://blackwell-synergy.com/doi/abs/10.1111/j.1467-8659.2009.01491.x},
volume = {28},
year = {2009}
}
@article{Perlin2002a,
address = {New York, New York, USA},
author = {Perlin, Ken},
file = {::},
journal = {Proceedings of the 29th annual conference on Computer graphics and interactive techniques - SIGGRAPH '02},
pages = {681},
publisher = {ACM Press},
title = {{Improving noise}},
url = {http://portal.acm.org/citation.cfm?doid=566570.566636},
year = {2002}
}
@article{Bailey2009,
author = {Bailey, Mike},
file = {::},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
number = {5},
pages = {96--100},
publisher = {IEEE},
title = {{Using GPU shaders for visualization}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5230472},
volume = {29},
year = {2009}
}
@article{Cole2009b,
address = {New York, New York, USA},
author = {Cole, Forrester and Finkelstein, Adam},
file = {::},
journal = {Proceedings of the 2009 symposium on Interactive 3D graphics and games - I3D '09},
keywords = {3D Graphics,NPR,Non-Photorealistic Rendering,a major dif culty,broad strokes,conventional,figure 2,hidden line removal,in drawing strokes is,insuf cient for drawing,item buffer intro-,line drawing,npr,per-fragment depth testing is,techniques such as the,visibility,visibility computation},
mendeley-tags = {3D Graphics,NPR,Non-Photorealistic Rendering},
pages = {115},
publisher = {ACM Press},
title = {{Fast high-quality line visibility}},
url = {http://portal.acm.org/citation.cfm?doid=1507149.1507168},
year = {2009}
}
@article{Alexa2003a,
author = {Alexa, M. and Behr, J. and Cohen-Or, D. and Fleishman, S. and Levin, D. and Silva, C.T.},
file = {::},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {3d acquisition,Point-Based Rendering,least squares,moving,point sample rendering,surface representation and reconstruction},
mendeley-tags = {Point-Based Rendering},
month = jan,
number = {1},
pages = {3--15},
title = {{Computing and rendering point set surfaces}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1175093},
volume = {9},
year = {2003}
}
@article{Kirchera,
author = {Kircher, Scott},
file = {::},
keywords = {alpha-,deferred shading,fast dynamic lighting,polygon shadows,real-time rendering},
title = {{Inferred Lighting: Fast dynamic lighting and shadows for opaque and translucent objects}}
}
@article{Burns2008a,
author = {Burns, Michael and Finkelstein, Adam},
file = {::},
journal = {ACM Transactions on Graphics},
keywords = {cutaway diagram,distance transform,npr,visibility},
month = dec,
number = {5},
pages = {1},
title = {{Adaptive cutaways for comprehensible rendering of polygonal scenes}},
url = {http://portal.acm.org/citation.cfm?doid=1409060.1409107},
volume = {27},
year = {2008}
}
@inproceedings{Sigg2006a,
author = {Sigg, Christian and Weyrich, Tim and Botsch, Mario and Gross, Markus},
booktitle = {Eurographics Symposium on Point-Based Graphics (2006)},
editor = {Botsch, M. and Chen, B.},
file = {::},
title = {{GPU-Based Ray-Casting of Quadratic Surfaces}},
year = {2006}
}
@article{Machines2012,
author = {Machines, Business},
doi = {10.1093/carcin/bgs054},
issn = {1460-2180},
journal = {Carcinogenesis},
month = mar,
number = {3},
pages = {NP},
pmid = {22383472},
title = {{Table of contents.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22383472},
volume = {33},
year = {2012}
}
@inproceedings{Botsch2005a,
author = {Botsch, M. and Hornung, A. and Zwicker, M. and Kobbelt, L.},
booktitle = {Proceedings Eurographics/IEEE VGTC Symposium Point-Based Graphics, 2005.},
file = {::},
keywords = {Point-Based Rendering},
mendeley-tags = {Point-Based Rendering},
pages = {17--141},
publisher = {Ieee},
title = {{High-quality surface splatting on today's GPUs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1500313},
volume = {5},
year = {2005}
}
@phdthesis{Stone1971b,
author = {Stone, John Edward},
file = {::},
school = {University of Missouri-Rolla},
title = {{An Efficient Library for Parallel Ray Tracing and Animation}},
year = {1971}
}
@article{Blass1984,
author = {Blass, Andreas and Burris, Stanley and Sankappanavar, H. P.},
doi = {10.2307/2322184},
issn = {00029890},
journal = {The American Mathematical Monthly},
month = jan,
number = {1},
pages = {64},
title = {{A Course in Universal Algebra.}},
url = {http://www.jstor.org/stable/2322184?origin=crossref},
volume = {91},
year = {1984}
}
@article{Shanmugam2007,
address = {New York, New York, USA},
author = {Shanmugam, Perumaal and Arikan, Okan},
doi = {10.1145/1230100.1230113},
file = {::},
isbn = {9781595936288},
journal = {Proceedings of the 2007 symposium on Interactive 3D graphics and games - I3D '07},
keywords = {Computer Graphics,GPU,I3D07,Real-Time Rendering,Shaders,ambient occlusion,gpu,real-time render-,soft shadows},
mendeley-tags = {Computer Graphics,GPU,I3D07,Real-Time Rendering,Shaders},
pages = {73},
publisher = {ACM Press},
title = {{Hardware accelerated ambient occlusion techniques on GPUs}},
url = {http://portal.acm.org/citation.cfm?doid=1230100.1230113},
year = {2007}
}
@article{McGuire2004c,
address = {New York, New York, USA},
author = {McGuire, Morgan and Hughes, John F.},
file = {::},
journal = {Proceedings of the 3rd international symposium on Non-photorealistic animation and rendering - NPAR '04},
keywords = {NPR,Non-Photorealistic Rendering,a bottleneck,and,buffer to discern edges,components of the frame,computing the,contour,cpu and transmitting it,edge list on the,for the first class,gpu,npr,of algorithms,reading back the frame-buffer,shadow volume,silhouette,to the gpu is},
mendeley-tags = {NPR,Non-Photorealistic Rendering},
pages = {35},
publisher = {ACM Press},
title = {{Hardware-determined feature edges}},
url = {http://portal.acm.org/citation.cfm?doid=987657.987663},
year = {2004}
}
@inproceedings{Lagae2009a,
author = {Lagae, Ares and Lefebvre, Sylvain and Drettakis, George and Dutr\'{e}, Philip},
booktitle = {ACM Transactions on Graphics},
file = {::},
keywords = {noise,procedural texture,rendering,shading},
month = jul,
number = {3},
pages = {1},
publisher = {ACM},
title = {{Procedural noise using sparse Gabor convolution}},
url = {http://portal.acm.org/citation.cfm?doid=1531326.1531360},
volume = {28},
year = {2009}
}
@inproceedings{Mcguire2009a,
author = {Mcguire, Morgan and Luebke, David},
booktitle = {Proceedings of HPG'09},
file = {::},
keywords = {global illumination,photon mapping,photon volumes},
title = {{Hardware-Accelerated Global Illumination by Image Space Photon Mapping}},
year = {2009}
}
@article{Schollmeyer2009a,
author = {Schollmeyer, Andre and Fr\"{o}hlich, Bernd},
file = {::},
journal = {ACM Transactions on Graphics},
keywords = {3D Graphics,Real-Time Rendering,cation,figure 1,ne sampling of the,parametric surfaces,per-pixel accuracy for arbitrary,point classi -,programmable graphics hardware,ray casting,recent on-the- y tessellation,root nding,trim boundaries,trimmed nurbs,zoom levels},
mendeley-tags = {3D Graphics,Real-Time Rendering},
month = jul,
number = {3},
pages = {1},
title = {{Direct trimming of NURBS surfaces on the GPU}},
url = {http://portal.acm.org/citation.cfm?doid=1531326.1531353},
volume = {28},
year = {2009}
}
@article{Ziegel1994,
author = {Ziegel, Eric R. and Khuri, Andre},
doi = {10.2307/1269402},
issn = {00401706},
journal = {Technometrics},
keywords = {Calculus, Probability, Statistics, Mulivariable Ca},
month = aug,
number = {3},
pages = {333},
title = {{Advanced Calculus with Applications in Statistics}},
url = {http://www.jstor.org/stable/1269402?origin=crossref},
volume = {36},
year = {1994}
}
@article{Hasselgren2009a,
author = {Hasselgren, Jon and Munkberg, Jacob and Akenine-M\"{o}ller, Tomas},
file = {::},
journal = {ACM Transactions on Graphics},
keywords = {3D Graphics,Real-Time Rendering},
mendeley-tags = {3D Graphics,Real-Time Rendering},
number = {2},
pages = {1--10},
title = {{Automatic pre-tessellation culling}},
url = {http://portal.acm.org/citation.cfm?doid=1516522.1516530},
volume = {28},
year = {2009}
}
@article{Nichols2009b,
author = {Nichols, Greg and Shopf, Jeremy and Wyman, Chris},
file = {::},
journal = {Computer Graphics Forum},
month = jun,
number = {4},
pages = {1141--1149},
title = {{Hierarchical Image-Space Radiosity for Interactive Global Illumination}},
url = {http://blackwell-synergy.com/doi/abs/10.1111/j.1467-8659.2009.01491.x},
volume = {28},
year = {2009}
}
@incollection{Bunnell2005b,
author = {Bunnell, M.},
booktitle = {GPU Gems},
file = {::},
keywords = {GPU Gems,ambient occlusion,graphics,nvidia},
mendeley-tags = {graphics,nvidia},
pages = {223--233},
title = {{Dynamic ambient occlusion and indirect lighting}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Dynamic+Ambient+Occlusion+and+Indirect+Lighting\#0},
volume = {2},
year = {2005}
}
@article{Card2001c,
author = {Card, Drew and Mitchell, Jason L},
file = {::},
journal = {ATI Research},
title = {{Non-photorealistic rendering with pixel and vertex shaders}},
year = {2001}
}
@phdthesis{Stone1971c,
author = {Stone, J.},
booktitle = {Computer Science Department, University of Missouri-Rolla Masters Thesis},
file = {::},
publisher = {Citeseer},
title = {{An efficient library for parallel ray tracing and animation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.7193\&amp;rep=rep1\&amp;type=pdf},
year = {1971}
}
@article{Fauser2002,
abstract = {Quantum Clifford Algebras (QCA), i.e. Clifford Hopf gebras based on bilinear forms of arbitrary symmetry, are treated in a broad sense. Five alternative constructions of QCAs are exhibited. Grade free Hopf gebraic product formulas are derived for meet and join of Grassmann-Cayley algebras including co-meet and co-join for Grassmann-Cayley co-gebras which are very efficient and may be used in Robotics, left and right contractions, left and right co-contractions, Clifford and co-Clifford products, etc. The Chevalley deformation, using a Clifford map, arises as a special case. We discuss Hopf algebra versus Hopf gebra, the latter emerging naturally from a bi-convolution. Antipode and crossing are consequences of the product and co-product structure tensors and not subjectable to a choice. A frequently used Kuperberg lemma is revisited necessitating the definition of non-local products and interacting Hopf gebras which are generically non-perturbative. A `spinorial' generalization of the antipode is given. The non-existence of non-trivial integrals in low-dimensional Clifford co-gebras is shown. Generalized cliffordization is discussed which is based on non-exponentially generated bilinear forms in general resulting in non unital, non-associative products. Reasonable assumptions lead to bilinear forms based on 2-cocycles. Cliffordization is used to derive time- and normal-ordered generating functionals for the Schwinger-Dyson hierarchies of non-linear spinor field theory and spinor electrodynamics. The relation between the vacuum structure, the operator ordering, and the Hopf gebraic counit is discussed. QCAs are proposed as the natural language for (fermionic) quantum field theory.},
archivePrefix = {arXiv},
arxivId = {math.QA/0202059},
author = {Fauser, Bertfried},
eprint = {0202059},
keywords = {Mathematical Physics,Quantum Algebra},
month = feb,
primaryClass = {math.QA},
title = {{A Treatise on Quantum Clifford Algebras}},
url = {http://arxiv.org/abs/math.QA/0202059},
year = {2002}
}
@article{Luft2006,
author = {Luft, Thomas and Colditz, Carsten and Deussen, Oliver},
file = {::},
journal = {ACM Transactions on Graphics (TOG)},
keywords = {Real-Time Rendering,SIGGRAPH06,SSAO,a dull appearance,ambient occlusion,artistic tone reproduction,complex scenes,complex spatial arrange-,gpu,image enhancement,ing,ments sometimes suffers from,non-photorealistic render-,shaders,since the rendering of,the desired quality,this is especially},
mendeley-tags = {Real-Time Rendering,SIGGRAPH06,SSAO,ambient occlusion,gpu,shaders},
title = {{Image enhancement by unsharp masking the depth buffer}},
url = {http://portal.acm.org/citation.cfm?id=1141911.1142016 http://graphics.uni-konstanz.de/publikationen/2006/unsharp\_masking/webseite/},
year = {2006}
}
@book{Yang2005,
address = {Hoboken, NJ, USA},
author = {Yang, Won Young and Cao, Wenwu and Chung, Tae-Sang and Morris, John},
doi = {10.1002/0471705195},
isbn = {9780471705192},
month = jan,
publisher = {John Wiley \& Sons, Inc.},
title = {{Applied Numerical Methods Using MATLAB®}},
url = {http://doi.wiley.com/10.1002/0471705195},
year = {2005}
}
@article{Kala2010,
abstract = {Handwriting Recognition enables a person to scribble something on a piece of paper and then convert it into text. If we look into the practical reality there are enumerable styles in which a character may be written. These styles can be self combined to generate more styles. Even if a small child knows the basic styles a character can be written, he would be able to recognize characters written in styles intermediate between them or formed by their mixture. This motivates the use of Genetic Algorithms for the problem. In order to prove this, we made a pool of images of characters. We converted them to graphs. The graph of every character was intermixed to generate styles intermediate between the styles of parent character. Character recognition involved the matching of the graph generated from the unknown character image with the graphs generated by mixing. Using this method we received an accuracy of 98.44\%.},
author = {Kala, Rahul and Vazirani, Harsh and Shukla, Anupam and Tiwari, Ritu},
file = {::},
journal = {International Journal of Computer Science},
number = {2},
pages = {16--25},
title = {{Offline Handwriting Recognition using Genetic Algorithm}},
url = {http://arxiv.org/abs/1004.3257},
volume = {7},
year = {2010}
}
@article{Wang2009a,
author = {Wang, Xiangyu and Chen, Rui (Irene)},
doi = {10.1080/15710880903320020},
file = {::},
issn = {1571-0882},
journal = {CoDesign},
keywords = {augmented reality,collaborative design,urban design,virtual reality},
month = dec,
number = {4},
pages = {229--244},
title = {{An experimental study on collaborative effectiveness of augmented reality potentials in urban design}},
url = {http://www.tandfonline.com/doi/abs/10.1080/15710880903320020},
volume = {5},
year = {2009}
}
@article{Rekimoto1995a,
author = {Rekimoto, Jun and Nagao, Katashi},
file = {::},
journal = {Proc. 8th Ann. ACM Symp. User Interface and Software Technology (UIST), ACM Press},
pages = {29--36},
title = {{The World through the Computer: Computer Augmented Interaction with Real World Environments}},
year = {1995}
}
@article{Capin2008b,
author = {Capin, Tolga and Pulli, Kari and Akenine-M\"{o}ller, Tomas},
file = {::},
journal = {IEEE Computer Graphics and Applications},
month = jul,
number = {4},
pages = {74--84},
title = {{The State of the Art in Mobile Graphics Research}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4557959},
volume = {28},
year = {2008}
}
@article{Barakonyi2003,
author = {Barakonyi, I. and Fahmy, T. and Schmalstieg, Dieter},
file = {::},
isbn = {0769520065},
journal = {and Augmented Reality},
keywords = {augmented reality,computer supported col-,laborative work,videoconferencing,volume rendering},
pages = {333},
publisher = {IEEE Computer Society},
title = {{Collaborative work with volumetric data using augmented reality videoconferencing}},
url = {http://portal.acm.org/citation.cfm?id=946833},
year = {2003}
}
@article{Malkawi2005,
author = {Malkawi, a and Srinivasan, R},
doi = {10.1016/j.autcon.2004.08.001},
file = {::},
issn = {09265805},
journal = {Automation in Construction},
keywords = {augmented reality,building interaction,cfd,gesture recognition,hci,human,speech recognition,visualization},
month = jan,
number = {1},
pages = {71--84},
title = {{A new paradigm for Human-Building Interaction: the use of CFD and Augmented Reality}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0926580504000998},
volume = {14},
year = {2005}
}
@article{Ziaei2011,
author = {Ziaei, Z. and Hahto, a. and Mattila, J. and Siuko, M. and Semeraro, L.},
doi = {10.1016/j.fusengdes.2010.12.082},
file = {::},
issn = {09203796},
journal = {Fusion Engineering and Design},
month = feb,
publisher = {Elsevier B.V.},
title = {{Real-time markerless Augmented Reality for Remote Handling system in bad viewing conditions}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0920379611000160},
year = {2011}
}
@article{Klinec1988a,
author = {Klinec, Darko and Leonhardi, Alexander},
file = {::},
number = {19-20},
pages = {xiii},
title = {{Positioning and Location Services for Infoor Areas in neXus}},
volume = {7},
year = {1988}
}
@article{Pan2006,
author = {Pan, Z and Cheok, a and Yang, H and Zhu, J and Shi, J},
doi = {10.1016/j.cag.2005.10.004},
file = {::},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {collaborative,cooperative,edutainment,mixed reality,virtual learning environment,virtual reality,vle},
month = feb,
number = {1},
pages = {20--28},
title = {{Virtual reality and mixed reality for virtual learning environments}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849305002025},
volume = {30},
year = {2006}
}
@article{Cheok2002,
abstract = {This paper presents an interactive theatre based on an embodied mixed reality space and wearable computers. Embodied computing mixed reality spaces integrate ubiquitous computing, tangible interaction and social computing within a mixed reality space, which enables intuitive interaction with physical world and virtual world. We believe it has potential advantages to support novel interactive theatre experiences. Therefore, we explored the novel interactive theatre experience supported in the embodied mixed reality space, and implemented live 3D characters to interact with user in such a system.},
author = {Cheok, A D and Weihua, Wang and Yang, Xubo and Prince, S and Wan, Fong Siew and Billinghurst, M and Kato, H},
doi = {10.1109/ISMAR.2002.1115073},
file = {::},
isbn = {0769517811},
journal = {Proceedings International Symposium on Mixed and Augmented Reality},
pages = {59--317},
publisher = {IEEE Computer Society},
title = {{Interactive theatre experience in embodied + wearable mixed reality space}},
url = {http://portal.acm.org/citation.cfm?id=850976.854978},
year = {2002}
}
@article{Platonov2006,
author = {Platonov, Juri and Heibel, Hauke and Meier, Peter},
doi = {10.1109/ISMAR.2006.297800},
file = {:home/acmt/Dropbox/Documentos/Mendeley/of the 5th IEEE and ACM/2006/Platonov, Heibel, Meier/Platonov, Heibel, Meier - 2006 - A mobile markerless AR system for maintenance and repair.pdf:pdf},
isbn = {1-4244-0650-1},
journal = {of the 5th IEEE and ACM},
keywords = {17,a laptop,and the user,augmented reality,augmented video stream are,computer,e,g,has been inspired by,lessly transmitted between a,maintenance,markerless tracking,our markerless tracking algorithm,solution both,the raw and the,wire-},
month = oct,
pages = {105--108},
publisher = {Ieee},
title = {{A mobile markerless AR system for maintenance and repair}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4079262 http://portal.acm.org/citation.cfm?id=1514222},
year = {2006}
}
@article{Nurminen2008c,
author = {Nurminen, Antti},
journal = {IEEE Computer Graphics and Applications},
month = jul,
number = {4},
pages = {20--31},
title = {{Mobile 3D City Maps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4557952},
volume = {28},
year = {2008}
}
@article{Baudin2004,
abstract = {This paper presents'LabFuture', an advanced e-learning platform that uses novel Information and Communication Technologies to support and expand laboratory teaching practices. For this purpose, LabFuture uses real and computer-generated objects that are interfaced using mechatronic systems,augmented reality, mobile technologies and 3D multi user environments. The main aim is to develop and demonstrate technological support for practical experiments in the following focused subjects namely: Fluid Dynamics -Science subject in Germany, Geometry - Mathematics subject in Austria,History and Environmental Awareness - Arts and Humanities subjects in Greece and Slovenia. In order to pedagogically enhance the design and functional aspects of this e-learning technology, we are investigating the dialogical operationalisation of learning theories so as to leverage our understanding of teaching and learning practices in the targeted context of deployment.},
author = {Baudin, Veronique and Faust, Martin and Kaufmann, Hannes and Litsa, Vivian and Mwanza, Daisy and Pierre, Arnaud and Totter, Alexandra},
file = {::},
publisher = {Springer Boston},
title = {{The Lab@Future Project: ‘Moving towards the future of e-Learning}},
url = {http://www.ims.tuwien.ac.at/publication\_detail.php?ims\_id=152},
year = {2004}
}
@article{Hile2008a,
author = {Hile, Harlan and Borriello, Gaetano},
file = {::},
journal = {IEEE Computer Graphics and Applications},
month = jul,
number = {4},
pages = {32--39},
title = {{Positioning and Orientation in Indoor Environments Using Camera Phones}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4557953},
volume = {28},
year = {2008}
}
@article{Anastassova2009,
abstract = {The paper presents an ergonomic analysis carried out in the early phases of an R\&D project. The purpose was to investigate the functioning of today's Automotive Service Technicians (ASTs) training in order to inform the design of an Augmented Reality (AR) teaching aid. The first part of the paper presents a literature review of some major problems encountered by ASTs today. The benefits of AR as technological aid are also introduced. Then, the methodology and the results of two case studies are presented. The first study is based on interviews with trainers and trainees; the second one on observations in real training settings. The results support the assumption that today's ASTs' training could be regarded as a community-of-practice (CoP). Therefore, AR could be useful as a collaboration tool, offering a shared virtual representation of real vehicle's parts, which are normally invisible unless dismantled (e.g. the parts of a hydraulic automatic transmission). We conclude on the methods and the technologies to support the automotive CoP.},
author = {Anastassova, Margarita and Burkhardt, Jean-Marie},
doi = {10.1016/j.apergo.2008.06.008},
file = {::},
issn = {1872-9126},
journal = {Applied ergonomics},
keywords = {Adult,Automobiles,Human Engineering,Humans,Male,Middle Aged,Task Performance and Analysis,Teaching,Teaching: methods,Technology,User-Computer Interface,Workplace,Young Adult},
month = jul,
number = {4},
pages = {713--21},
pmid = {18703179},
title = {{Automotive technicians' training as a community-of-practice: implications for the design of an augmented reality teaching aid.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18703179},
volume = {40},
year = {2009}
}
@article{Conole2004a,
abstract = {This paper considers the increasing impact of Information and Communication Technologies (ICT) and the associated rise in e-learning as a recognised and respected research area. The paper provides a summary of some of the current research areas under investigation and provides a list of characteristics of the area. The paper goes on to consider the professional identities of researchers in the area and the tensions which have resulted in terms of aligning with this new emergent group of professionals within existing institutional structures.},
author = {Conole, Grainne},
file = {::},
journal = {Journal of Interactive Media in Education},
pages = {1--18},
title = {{E-Learning: The Hype and the Reality}},
url = {http://www-jime.open.ac.uk/2004/12/},
volume = {2004},
year = {2004}
}
@article{Reitmayr2005,
author = {Reitmayr, G. and Eade, E. and Drummond, T.},
doi = {10.1109/ISMAR.2005.39},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)/2005/Reitmayr, Eade, Drummond/Reitmayr, Eade, Drummond - 2005 - Localisation and interaction for augmented maps.pdf:pdf},
isbn = {0-7695-2459-1},
journal = {Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)},
keywords = {a control,a user interacting with,augmented,device to pick up,figure 1,maps using a pda,optical tracking,plays,projection dis-,spatially augmented reality,tangible user interfaces},
pages = {120--129},
publisher = {Ieee},
title = {{Localisation and interaction for augmented maps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544673},
year = {2005}
}
@article{Schmalstieg2000,
abstract = {Studierstube is an experimental user integace system, which uses collaborative augmented reality to incorporate true 30 interaction into a productivity environment. This concept is extended to bridge multiple user integace dimensions by including multiple users, multiple host platforms, multiple display types, multiple concurrent applications, and a multi-context (i. e., 30 document) integace into a heterogeneous distributed environment. With this architecture, we can explore the user integace design space between pure augmented reality and the popular ubiquitous computing paradigm. We report on our design philosophy centered around the notion of contexts and locales, as well as the underlying sofhare and hardware architecture. Contexts encapsulate a live application together with 30 (visual) and other data, while locales are used to organize geometric reference systems. By separating geometric relationships (locales) from semantic relationships (contexts), we achieve a great amount of flexibility in the configuration of displays. To illustrate our claims, we present several applications including a cinematographic design tool which showcases many features of our system},
author = {Schmalstieg, D and Fuhrmann, A and Hesina, G},
doi = {10.1109/ISAR.2000.880919},
file = {::},
isbn = {0769508464},
journal = {Proceedings IEEE and ACM International Symposium on Augmented Reality ISAR 2000},
pages = {20--29},
publisher = {Ieee},
title = {{Bridging multiple user interface dimensions with augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=880919},
year = {2000}
}
@incollection{Christian2007,
abstract = {There is evidence that recent developments in Augmented Reality (AR) technology has the potential to be applied as pervasive media on multiple devices in different ways and contexts, especially with low-cost devices including Mobile Augmented Reality (MAR) applications on smart phones or Pocket-PCs. In this paper we present a framework in order to combine the pervasive e-education concept with augmented reality content for e-training. We analyze current research, discuss some examples from ultralight light sport aircraft maintenance and show how to apply this framework generically. We present a learning engine to deliver this special type of content and provide a further outlook of future research. A user-centered approach must ensure that the developments can stimulate motivation and enhance performance of the end users in different training sessions. The main benefit is, that the end users are enabled to better perceive complex, technical facts, systems and components.},
author = {Christian, Johannes and Krieger, Horst and Holzinger, Andreas and Behringer, Reinhold},
booktitle = {Universal Access to Applications and Services Lecture Notes in Computer Science LNCS 4556},
editor = {Stephanidis, C},
file = {::},
isbn = {9783540732822},
keywords = {augmented reality,learning performance,performance},
pages = {520--529},
publisher = {Springer},
title = {{Virtual and mixed reality interface for e-training: examples of applications in ultralight / light sport aircraft maintenance}},
year = {2007}
}
@article{Bradley2007,
author = {Bradley, Derek and Roth, Gerhard and Bose, Prosenjit},
doi = {10.1007/s00138-007-0108-9},
file = {::},
isbn = {0013800701},
issn = {0932-8092},
journal = {Machine Vision and Applications},
keywords = {augmented reality,common illumination,marker systems,non-rigid object,tracking},
month = nov,
number = {2},
pages = {85--92},
title = {{Augmented reality on cloth with realistic illumination}},
url = {http://www.springerlink.com/index/10.1007/s00138-007-0108-9},
volume = {20},
year = {2007}
}
@inproceedings{Buchmann2004,
author = {Buchmann, Volkert and Violich, S. and Billinghurst, M. and Cockburn, A.},
booktitle = {Proceedings of the 2nd international conference on Computer graphics and interactive techniques in Australasia and South East Asia},
file = {::},
keywords = {augmented reality,gesture interaction,occlusion},
pages = {212--221},
publisher = {ACM},
title = {{FingARtips: gesture based direct manipulation in Augmented Reality}},
url = {http://portal.acm.org/citation.cfm?id=988871},
year = {2004}
}
@article{Comport2006,
abstract = {Tracking is a very important research subject in a real-time augmented reality context. The main requirements for trackers are high accuracy and little latency at a reasonable cost. In order to address these issues, a real-time, robust, and efficient 3D model-based tracking algorithm is proposed for a "video see through" monocular vision system. The tracking of objects in the scene amounts to calculating the pose between the camera and the objects. Virtual objects can then be projected into the scene using the pose. Here, nonlinear pose estimation is formulated by means of a virtual visual servoing approach. In this context, the derivation of point-to-curves interaction matrices are given for different 3D geometrical primitives including straight lines, circles, cylinders, and spheres. A local moving edges tracker is used in order to provide real-time tracking of points normal to the object contours. Robustness is obtained by integrating an M-estimator into the visual control law via an iteratively reweighted least squares implementation. This approach is then extended to address the 3D model-free augmented reality problem. The method presented in this paper has been validated on several complex image sequences including outdoor environments. Results show the method to be robust to occlusion, changes in illumination, and mistracking.},
author = {Comport, A.I. and Marchand, Eric and Pressigout, Muriel and Chaumette, Fran\c{c}ois},
doi = {10.1109/TVCG.2006.78},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Transactions on Visualization and Computer Graphics/2006/Comport et al/Comport et al. - 2006 - Real-time markerless tracking for augmented reality the virtual visual servoing framework.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Algorithms,Computer Graphics,Computer Systems,Computer-Assisted,Computer-Assisted: methods,Feedback,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Information Storage and Retrieval,Information Storage and Retrieval: methods,Signal Processing,Three-Dimensional,Three-Dimensional: methods,User-Computer Interface},
number = {4},
pages = {615--628},
pmid = {16805268},
publisher = {IEEE Computer Society},
title = {{Real-time markerless tracking for augmented reality: the virtual visual servoing framework}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16805268 http://www.computer.org/portal/web/csdl/doi/10.1109/tvcg.2006.78},
volume = {12},
year = {2006}
}
@article{Bencina2005c,
author = {Bencina, R. and Kaltenbrunner, M. and Jorda, S.},
file = {::},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops},
pages = {99--99},
publisher = {Ieee},
title = {{Improved Topological Fiducial Tracking in the reacTIVision System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1565409},
year = {2005}
}
@article{Juan2011,
author = {Juan, M. Carmen and Joele, Dennis},
doi = {10.1016/j.ijhcs.2011.03.002},
file = {::},
issn = {10715819},
journal = {International Journal of Human-Computer Studies},
keywords = {augmented reality,invisible markers,phobia towards small animals},
month = jun,
number = {6},
pages = {440--453},
publisher = {Elsevier},
title = {{A comparative study of the sense of presence and anxiety in an invisible marker versus a marker augmented reality system for the treatment of phobia towards small animals}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S107158191100036X},
volume = {69},
year = {2011}
}
@article{Thomas2010,
abstract = {The use of Virtual Environments has been widely reported as a method of teaching anatomy. Generally such environments only convey the shape of the anatomy to the student. We present the Bangor Augmented Reality Education Tool for Anatomy (BARETA), a system that combines Augmented Reality (AR) technology with models produced using Rapid Prototyping (RP) technology, to provide the student with stimulation for touch as well as sight. The principal aims of this work were to provide an interface more intuitive than a mouse and keyboard, and to evaluate such a system as a viable supplement to traditional cadaver based education.},
author = {Thomas, Rhys Gethin and John, Nigel William and Delieu, John Michael},
doi = {10.3109/17453050903557359},
file = {::},
issn = {1745-3062},
journal = {Journal of visual communication in medicine},
keywords = {Anatomy,Anatomy: education,Cadaver,Computer Simulation,Consumer Satisfaction,Female,Humans,Magnetic Resonance Imaging,Male,Models, Biological,Pilot Projects,Sex Factors,User-Computer Interface},
month = mar,
number = {1},
pages = {6--15},
pmid = {20297908},
title = {{Augmented reality for anatomical education.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20297908},
volume = {33},
year = {2010}
}
@article{Jiang2004a,
author = {Jiang, B. and Neumann, U. and You, S.},
file = {::},
journal = {IEEE Virtual Reality, 2004. Proceedings},
pages = {3--275},
title = {{A robust hybrid tracking system for outdoor augmented reality}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1310049},
year = {2004}
}
@article{Liarokapis2002a,
abstract = {An interactive Multimedia Augmented Reality Interface for E-Learning (MARIE) is presented in the article. Its application for engineering education is discussed in order to enhance traditional teaching and learning methods; however, it is equally applicable to other areas. The authors have developed and implemented a user-friendly interface to experimentally explore the potential of augmented reality by superimposing Virtual Multimedia Content (VMC) information in an Augmented Reality (AR) tabletop environment, such as a student desk workspace. The user can interact with the VMC, which is composed of threedimensional objects, images, animations, text (ASCII or three-dimensional) and sound. To prove the feasibility of the system only a small part of the teaching material was digitised and some experimental results are presented in the article.},
author = {Liarokapis, F and Petridis, P and Lister, P F and White, M},
file = {::},
journal = {World Transactions on Engineering and Technology Education},
keywords = {l education (general),qa75 electronic computers. computer science,qa76 computer software},
number = {2},
pages = {173--176},
publisher = {Citeseer},
title = {{Multimedia Augmented Reality Interface for E-Learning (MARIE)}},
url = {http://eprints.sussex.ac.uk/1088/},
volume = {1},
year = {2002}
}
@inproceedings{Buscher2000,
address = {New York, New York, USA},
author = {B\"{u}scher, Monika and Christensen, Michael and Gr\o nb\ae k, Kaj and Krogh, Peter and Mogensen, Preben and Shapiro, Dan and \O rb\ae k, Peter},
booktitle = {Proceedings of the third international conference on Collaborative virtual environments - CVE '00},
doi = {10.1145/351006.351012},
file = {::},
isbn = {1581133030},
pages = {47--56},
publisher = {ACM Press},
title = {{Collaborative augmented reality environments}},
url = {http://portal.acm.org/citation.cfm?id=351012 http://portal.acm.org/citation.cfm?doid=351006.351012},
year = {2000}
}
@article{Schmalstieg2007c,
author = {Schmalstieg, Dieter and Schall, Gerhard and Wagner, Daniel and Barakonyi, Istv\'{a}n and Reitmayr, Gerhard and Newman, Joseph and Ledermann, Florian},
journal = {IEEE Computer Graphics and Applications},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Computer Graphics,Computer-Assisted,Computer-Assisted: methods,Database Management Systems,Databases,Ecosystem,Factual,Geographic Information Systems,Image Interpretation,Imaging,Information Storage and Retrieval,Information Storage and Retrieval: methods,Models,Pattern Recognition,Theoretical,Three-Dimensional,Three-Dimensional: methods,User-Computer Interface},
number = {4},
pages = {48--57},
publisher = {IEEE Computer Society},
title = {{Managing complex augmented reality models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17713234 http://doi.ieeecomputersociety.org/10.110910.1109/MCG.2007.85},
volume = {27},
year = {2007}
}
@article{Watsen1999a,
author = {Watsen, K and Darken, R and Capps, M},
file = {::},
journal = {3rd International Immersive Projection Technology Workshop (IPTW'},
publisher = {Citeseer},
title = {{A Handheld Computer as an Interaction Device to a Virtual Environment}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:A+Handheld+Computer+as+an+Interaction+Device+to+a+Virtual+Environment\#0},
volume = {99},
year = {1999}
}
@article{Regenbrecht2002a,
author = {Regenbrecht, H and Wagner, M and Baratoff, G},
doi = {10.1007/s100550200016},
file = {::},
issn = {13594338},
journal = {Virtual Reality},
number = {3},
pages = {151--166},
publisher = {Springer},
title = {{Magicmeeting: A collaborative tangible augmented reality system}},
url = {http://www.springerlink.com/openurl.asp?genre=article\&id=doi:10.1007/s100550200016},
volume = {6},
year = {2002}
}
@inproceedings{Starner2000,
abstract = {Computer gaming offers a unique test-bed and market for advanced concepts in computer science, such as Human Computer Interaction (HCI), computer-supported collaborative work (CSCW), intelligent agents, graphics, and sensing technology. In addition, computer gaming is especially wellsuited for explorations in the relatively young fields of wearable computing and augmented reality (AR). This paper presents a developing multi-player augmented reality game, patterned as a cross between a martial arts fighting game and an agent controller, as implemented using the Wearable Augmented Reality for Personal, Intelligent, and Networked Gaming (WARPING) system. Through interactions based on gesture, voice, and head movement input and audio and graphical output, the WARPING system demonstrates how computer vision techniques can be exploited for advanced, intelligent interfaces. Keywords Augmented reality, wearable computing, computer vision 1. INTRODUCTION: WHY GAMES? Computer gaming provides...},
author = {Starner, Thad and Leibe, Bastian and Singletary, Brad and Pair, Jarrell},
booktitle = {Interface},
doi = {10.1145/325737.325864},
file = {::},
isbn = {1581131348},
keywords = {augmented reality,computer vision,wearable computing},
pages = {256--259},
publisher = {ACM},
title = {{MIND-WARPING : Towards Creating a Compelling Collaborative Augmented Reality Game}},
url = {http://portal.acm.org/citation.cfm?id=325737.325864},
year = {2000}
}
@inproceedings{Jin2007,
author = {Jin, Yoon-suk and Kim, Yang-wook and Park, Jun},
booktitle = {Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2007.4538863},
file = {::},
isbn = {978-1-4244-1749-0},
keywords = {augmented reality,design evaluation,mock-up},
month = nov,
pages = {1--2},
publisher = {IEEE Computer Society},
title = {{ARMO: Augmented Reality based Reconfigurable MOck-up}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538863 http://portal.acm.org/citation.cfm?id=1514374},
year = {2007}
}
@article{Courtiat2004,
abstract = {This paper presents LabFuture, an advanced e-learning platform that uses novel Information and Communication Technologies to support and expand laboratory teaching practices. For this purpose, LabFuture uses real and computer generated objects that are interfaced using mechatronic systems, augmented reality, mobile technologies and 3D multi user environments. The main aim is to develop and demonstrate technological support for practical experiments in the following focused disciplines namely: Fluid Dynamics - Science subject in Germany, Geometry - Mathematics subject in Austria, History and Environmental Awareness Arts and Humanities subjects in Greece and Slovenia. In order to pedagogically enhance the design and functional aspects of this e-learning technology, we are investigating the dialogical operationalisation of learning theories so as to leverage our understanding of teaching and learning practices in the targeted context of deployment. To be able to evaluate the labfuture system in its entire complexity an evaluation methodology including several phases has been developed, performing formative as well as summative evaluations.},
author = {Courtiat, Jean-Pierre and Davarakis, Costas and Totter, Alexandra and Mwanza, Daisy and Faust, Martin and Kaufmann, Hannes},
file = {::},
journal = {Learning},
title = {{Evaluating LAB@FUTURE, a Collaborative E-Learning Laboratory Experiments Platform}},
url = {http://www.eden-online.org/eden.php?menuId=222\&contentId=285},
year = {2004}
}
@article{Gee2011,
author = {Gee, Andrew P. and Webb, Matthew and Escamilla-Ambrosio, Jorge and Mayol-Cuevas, Walterio and Calway, Andrew},
doi = {10.1016/j.cag.2011.04.006},
file = {::},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {Augmented reality,GPS,Topometric,UWB,Visual SLAM},
month = aug,
number = {4},
pages = {854--868},
publisher = {Elsevier},
title = {{A topometric system for wide area augmented reality}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849311001099},
volume = {35},
year = {2011}
}
@article{Adcock2004,
author = {Adcock, Matt and Hutchins, Matthew and Gunn, Chris},
doi = {10.1145/1186415.1186463},
file = {::},
isbn = {1581138962},
journal = {ACM SIGGRAPH 2004 Posters on SIGGRAPH 04},
pages = {41},
publisher = {ACM Press},
title = {{Haptic collaboration with augmented reality}},
url = {http://portal.acm.org/citation.cfm?doid=1186415.1186463},
year = {2004}
}
@article{Martin-Gutierrez2010,
author = {Mart\'{\i}n-Guti\'{e}rrez, Jorge and {Lu\'{\i}s Saor\'{\i}n}, Jos\'{e} and Contero, Manuel and Alca\~{n}iz, Mariano and P\'{e}rez-L\'{o}pez, David C. and Ortega, Mario},
doi = {10.1016/j.cag.2009.11.003},
file = {::},
issn = {00978493},
journal = {Computers \& Graphics},
month = feb,
number = {1},
pages = {77--91},
title = {{Design and validation of an augmented book for spatial abilities development in engineering students}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849309001514},
volume = {34},
year = {2010}
}
@phdthesis{Flintham2009,
abstract = {Mobile mixed-reality experiences mix physical and digital spaces, enabling participants to simultaneously inhabit a shared environment online and on the streets. These experiences take the form of games, educational applications and new forms of performance and art, and engender new opportunities for interaction, collaboration and play. As mobile mixed-reality experiences move out of the laboratory and into more public settings they raise new challenges concerning how to support these experiences in the wild. This thesis argues that mobile mixed-reality experiences in which artists retain creative control over the content and operation of each experience, particularly those that are deployed as theatrical performances, require dedicated support for content authoring and reactive orchestration tools and paradigms in order to be successfully and robustly operated in public settings. These requirements are examined in detail, drawing on the experience of supporting four publicly toured mobile mixed-reality experiences; Can You See Me Now?, Uncle Roy All Around You, I Like Frank in Adelaide and Savannah, which have provided a platform to practically develop, refine and evaluate new solutions to answer these challenges in the face of presenting the experiences to many thousands of participants over a four year period. This thesis presents two significant supporting frameworks. The ColourMaps system enables designers to author location-based content by directly colouring over maps; providing a simple, familiar and yet highly flexible approach to matching location-triggers to complex physical spaces. It provides support for multiple and specialised content layers, and the ability to configure and manage other aspects of an experience, including filtering inaccurate position data and underpinning orchestration tools. Second, the Orchestration framework supports the day-to-day operation of public experiences; providing dedicated control-room tools for monitoring that reveal the content landscape and historical events, intervention and improvisation techniques for steering and shaping each participant's experience as it unfolds both physically and virtually, and processes to manage a constant flow of participants.},
author = {Flintham, Martin},
booktitle = {Computing Systems},
file = {::},
keywords = {qa 75 electronic computers. computer science},
number = {December},
pages = {234},
school = {University of Nottingham},
title = {{Supporting mobile mixed-reality experiences}},
url = {http://etheses.nottingham.ac.uk/632/},
year = {2009}
}
@article{Klineca,
author = {Klinec, Darko and Leonhardi, Alexander},
file = {::},
journal = {ifp.uni-stuttgart.de},
pages = {1--12},
title = {{POSITIONING AND LOCATION SERVICES}},
url = {http://www.ifp.uni-stuttgart.de/publications/2001/Klinec\_Indoornav2001.pdf}
}
@article{Shin2009,
author = {Shin, Do Hyoung and Jang, Won-Suk},
doi = {10.1016/j.autcon.2009.06.001},
file = {::},
issn = {09265805},
journal = {Automation in Construction},
month = dec,
number = {8},
pages = {1063--1069},
publisher = {Elsevier B.V.},
title = {{Utilization of ubiquitous computing for construction AR technology}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0926580509000922},
volume = {18},
year = {2009}
}
@article{Castle2011,
abstract = {We show how a system for video-rate parallel camera tracking and 3D map-building can be readily extended to allow one or more cameras to work in several maps, separately or simultaneously. The ability to handle several thousand features per map at video-rate, and for the cameras to switch automatically between maps, allows spatially localized AR workcells to be constructed and used with very little intervention from the user of a wearable vision system. The user can explore an environment in a natural way, acquiring local maps in real-time. When revisiting those areas the camera will select the correct local map from store and continue tracking and structural acquisition, while the user views relevant AR constructs registered to that map. The method is shown working in a progressively larger environments, from desktop to large building.},
author = {Castle, Robert O and Klein, Georg and Murray, David W},
doi = {10.1016/j.cviu.2011.02.007},
file = {::},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {augmented reality},
number = {6},
pages = {854--867},
publisher = {Elsevier Inc.},
title = {{Wide-area Augmented Reality using Camera Tracking and Mapping in Multiple Regions}},
url = {http://www.sciencedirect.com/science/article/B6WCX-528YXFJ-1/2/34212f30c9fe494b6317046adfc20777},
volume = {In Press,},
year = {2011}
}
@article{Gelb2010,
author = {Gelb, Dan and Subramanian, A},
file = {::},
isbn = {9781612840352},
journal = {Person-Oriented Vision (POV),},
pages = {1--6},
title = {{Augmented reality for immersive remote collaboration}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5712368},
year = {2010}
}
@article{Kiyokawa2002,
author = {Kiyokawa, K. and Billinghurst, M. and Hayes, S.E. and Gupta, a. and Sannohe, Y. and Kato, H.},
doi = {10.1109/ISMAR.2002.1115083},
file = {::},
isbn = {0-7695-1781-1},
journal = {Proceedings. International Symposium on Mixed and Augmented Reality},
pages = {139--148},
publisher = {IEEE Comput. Soc},
title = {{Communication behaviors of co-located users in collaborative AR interfaces}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1115083},
year = {2002}
}
@article{Dunston2003,
abstract = {Design visualization is key to the communication and shared perception of designs and is essential for meaningful design development and collaborations. The initial development of an Augmented Reality Computer Aided Drawing (AR CAD) system for enhancing visualization of models created in standard CAD was presented at the 17th ISARC. AR CAD features a more natural mode for changing views of the model and completely understanding the model content. Expected benefits are improved efficiency in the design detailing function, both for the individual detailer and for design collaborations where maintaining an accurate shared understanding of the design model is critical. An experimental program is under way to examine the impact of AR CAD upon a users perception and recall of a design model. Related experiments with desktop and immersive virtual environments have found that motion cues can indeed markedly improve spatial cognition. It is expected that we will see the same benefits in our AR CAD system, although until now such studies have not been conducted in an AR environment. This paper presents the rationale for experiments to measure the impact of AR CAD in terms of cognition cost, and it lays the foundation for further application of Mixed Reality (MR) technology to the design, construction, and maintenance phases of a facilitys life cycle. MR applications may prove promising for effective communication of designs for prefabrication, site installation, and the planning and excecution of maintenance operations.},
author = {Dunston, P and Wang, X and Billinghurst, M and Hampson, B},
file = {::},
journal = {NIST SPECIAL PUBLICATION SP},
keywords = {3d cad,augmented reality,mixed reality,spatial cognition,visualization},
pages = {1--6},
publisher = {NATIONAL INSTIUTE OF STANDARDS \& TECHNOLOGY},
title = {{Mixed Reality benefits for design perception}},
url = {http://www.hitlabnz.org/publications/2002-ISARC-MixedReality.pdf},
year = {2003}
}
@inproceedings{Kurz2011,
author = {Kurz, Daniel and Benhimane, Selim},
booktitle = {IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR2011)},
keywords = {Handheld Augmented Reality Inertial Sensors Gravit},
pages = {111--120},
title = {{Gravity-Aware Handheld Augmented Reality}},
year = {2011}
}
@article{Nam2009,
author = {Nam, Tek-Jin and Sakong, Kyung},
file = {::},
journal = {International Journal of Design},
keywords = {augmented reality,collaborative design,interaction,shared 3d workspace,tangible interaction,tele presence},
number = {1},
pages = {43--55},
title = {{Collaborative 3D Workspace and Interaction Techniques for Synchronous Distributed Product Design Reviews}},
url = {http://www.ijdesign.org/ojs/index.php/IJDesign/article/view/387/240},
volume = {3},
year = {2009}
}
@article{Bajura1995a,
author = {Bajura, M. and Neumann, U.},
file = {::},
journal = {IEEE Computer Graphics and Applications},
keywords = {augmented reality,reality,registration,virtual},
number = {5},
pages = {52--60},
title = {{Dynamic registration correction in video-based augmented reality systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=403828},
volume = {15},
year = {1995}
}
@article{Teichrieb2007,
author = {Teichrieb, Veronica and Lima, Monte and Lourenc, Eduardo and Bueno, Silva and Kelner, Judith and Santos, Ismael H F},
file = {::},
journal = {International Journal of Modeling and Simulation for the Petroleum Industry},
number = {1},
pages = {1--7},
title = {{A Survey of Online Monocular Markerless Augmented Reality}},
url = {http://rpcmod.ganer.ex-br.com/revista/articles/1.pdf},
volume = {1},
year = {2007}
}
@inproceedings{Kurz2011a,
author = {Kurz, Daniel and Benhimane, Selim},
booktitle = {IEEE and ACM International Symposium on Mixed and Augmented Reality},
keywords = {Handheld Augmented Reality Inertial Sensors Gravit},
pages = {111--120},
title = {{Gravity-Aware Handheld Augmented Reality}},
url = {http://da.nielkurz.de/content/Gravity-Aware\_Handheld\_Augmented\_Reality},
year = {2011}
}
@article{DeBoer2011,
author = {de Boer, Jelle and a.M. Kommers, Piet and de Brock, Bert},
doi = {10.1016/j.compedu.2010.10.015},
file = {::},
issn = {03601315},
journal = {Computers \& Education},
month = apr,
number = {3},
pages = {727--735},
publisher = {Elsevier Ltd},
title = {{Using learning styles and viewing styles in streaming video}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0360131510003003},
volume = {56},
year = {2011}
}
@article{Pasman2006a,
author = {Woodward, Charles and Pasman, W.},
file = {:home/acmt/Dropbox/Documentos/Mendeley/The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings/2006/Woodward, Pasman/Woodward, Pasman - 2006 - Implementation of an Augmented Reality System on a PDA.pdf:pdf},
journal = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
pages = {276--277},
publisher = {IEEE Comput. Soc},
title = {{Implementation of an Augmented Reality System on a PDA}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240718},
year = {2006}
}
@article{Gjosaeter2009,
author = {Gjosaeter, Tor},
doi = {10.1109/SocInfo.2009.21},
file = {::},
isbn = {978-0-7695-3706-1},
journal = {2009 International Workshop on Social Informatics},
keywords = {-component,augmented reality,cscd,cscw},
month = jun,
pages = {35--40},
publisher = {Ieee},
title = {{Computer Supported Collaborative Design Using Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5230716},
year = {2009}
}
@article{Rashid2006,
abstract = {RFID is often cited as the next big evolution in computing as it effectively enables everyday objects to be connected to the Internet. RFID readers are now available on mobile phones and in this paper we present an example of their use in a location based mobile game. Location based games are a new entertainment genre that allow users to play games in mixed reality in that they incorporate knowledge of their physical location and then provide them with the ability to interact with both real and virtual objects within that location. The game presented in this paper is the first of its kind and shows the potential for using RFID with mobile phones.},
author = {Rashid, Omer and Coulton, Paul and Edwards, Reuben and Bamford, Will},
file = {::},
journal = {Consumer Electronics 2006 ICCE 06 2006 Digest of Technical Papers International Conference on},
keywords = {qa76 computer software},
pages = {459--460},
title = {{Utilising RFID for mixed reality mobile games.}},
url = {http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=1598509},
year = {2006}
}
@article{Attfield2005,
author = {Attfield, S and Blandford, A and Mottram, C and Penn, A and {Fatah Gen Schieck}, A},
file = {::},
publisher = {Key Centre of Design Computing and Cognition, University of Sydney},
title = {{Exploring the effects of introducing real-time simulation on collaborative urban design in augmented reality}},
url = {http://discovery.ucl.ac.uk/1472/},
year = {2005}
}
@article{Pilet2005,
author = {Pilet, J. and Lepetit, V. and Fua, P.},
doi = {10.1109/ISMAR.2005.18},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)/2005/Pilet, Lepetit, Fua/Pilet, Lepetit, Fua - 2005 - Augmenting deformable objects in real-time.pdf:pdf},
isbn = {0-7695-2459-1},
journal = {Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)},
pages = {134--137},
publisher = {Ieee},
title = {{Augmenting deformable objects in real-time}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544675},
volume = {1},
year = {2005}
}
@phdthesis{Milvich2004b,
author = {Milvich, Michael Lazar},
number = {July},
title = {{JavaCave: A 3D Immersive Environment in Java}},
year = {2004}
}
@article{Schall2008,
abstract = {In this paper we present a natural feature tracking algorithm based on on-line boosting used for localizing a mobile computer. Mobile augmented reality requires highly accurate and fast six degrees of freedom tracking in order to provide registered graphical overlays to a mobile user. With advances in mobile computer hardware, vision-based tracking approaches have the potential to provide efficient solutions that are non-invasive in contrast to the currently dominating marker-based approaches. We propose to use a tracking approach which can use in an unknown environment, i.e. the target has not be known beforehand. The core of the tracker is an on-line learning algorithm, which updates the tracker as new data becomes available. This is suitable in many mobile augmented reality applications. We demonstrate the applicability of our approach on tasks where the target objects are not known beforehand, i.e. interactive planing.},
author = {Schall, Gerhard and Grabner, Helmut and Grabner, Michael and Wohlhart, Paul and Schmalstieg, Dieter and Bischof, Horst},
doi = {10.1109/CVPRW.2008.4563134},
file = {::},
isbn = {9781424423392},
journal = {2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
number = {c},
pages = {1--8},
publisher = {Ieee},
title = {{3D tracking in unknown environments using on-line keypoint learning for mobile augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4563134},
year = {2008}
}
@article{Wang2008,
author = {Wang, X and Dunston, P},
doi = {10.1016/j.autcon.2007.07.002},
file = {::},
issn = {09265805},
journal = {Automation in Construction},
keywords = {3d models,computer supported cooperative work,cscw,design review,mixed reality,usability},
month = may,
number = {4},
pages = {399--412},
title = {{User perspectives on mixed reality tabletop visualization for face-to-face collaborative design review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0926580507000933},
volume = {17},
year = {2008}
}
@article{Chen2008,
author = {Chen, R and Wang, X},
doi = {10.1016/S1007-0214(08)70120-2},
file = {::},
issn = {10070214},
journal = {Tsinghua Science \& Technology},
keywords = {augmented reality,design learning,physicality,tangible augmented reality,tangible interface},
month = oct,
number = {October},
pages = {13--18},
publisher = {Tsinghua University Press},
title = {{An Empirical Study on Tangible Augmented Reality Learning Space for Design Skill Transfer}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1007021408701202},
volume = {13},
year = {2008}
}
@article{Fuge2011,
author = {Fuge, Mark and Yumer, Mehmet Ersin and Orbay, Gunay and Kara, Levent Burak},
doi = {10.1016/j.cad.2011.05.009},
file = {::},
issn = {00104485},
journal = {Computer-Aided Design},
month = jun,
pages = {1--13},
publisher = {Elsevier Ltd},
title = {{Conceptual design and modification of freeform surfaces using dual shape representations in augmented reality environments}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S001044851100128X},
year = {2011}
}
@article{Hirakawa2004,
author = {Hirakawa, Masahito and Koike, Satoshi},
file = {::},
journal = {Proceedings of the IEEE},
keywords = {augmented reality,collaboration,single camera tracking,transparent},
title = {{A Collaborative Augmented Reality System using Transparent Display}},
year = {2004}
}
@article{Fjeld2004,
author = {Fjeld, Morten},
doi = {10.1145/1029036.1029044},
file = {::},
issn = {10725520},
journal = {Interactions},
month = nov,
number = {6},
pages = {11},
title = {{Usability and collaborative aspects of augmented reality}},
url = {http://portal.acm.org/citation.cfm?doid=1029036.1029044},
volume = {11},
year = {2004}
}
@article{Pape1999a,
author = {Pape, Dave},
file = {::},
journal = {Proceedings of SPIE},
keywords = {application framework,stereo perspective},
number = {Figure 1},
pages = {346--353},
publisher = {Spie},
title = {{Transparently supporting a wide range of VR and stereoscopic display devices}},
url = {http://link.aip.org/link/?PSI/3639/346/1\&Agg=doi},
year = {1999}
}
@article{Juan2010,
author = {Juan, Carmen M. and Llop, Edith and Abad, Francisco and Lluch, Javier},
doi = {10.1109/ICALT.2010.123},
file = {::},
isbn = {978-1-4244-7144-7},
journal = {2010 10th IEEE International Conference on Advanced Learning Technologies},
keywords = {augmented reality,edutainment,learning words},
month = jul,
pages = {422--426},
publisher = {Ieee},
title = {{Learning Words Using Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5572407},
year = {2010}
}
@article{Carmigniani2010,
author = {Carmigniani, Julie and Furht, Borko and Anisetti, Marco and Ceravolo, Paolo and Damiani, Ernesto and Ivkovic, Misa},
doi = {10.1007/s11042-010-0660-6},
file = {::},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
keywords = {ar,augmented,augmented reality,augmented reality applications,augmented reality iphone4,augmented reality technologies,b,carmigniani,furht,j,reality on mobile devices,systems},
month = dec,
number = {1},
pages = {341--377},
title = {{Augmented reality technologies, systems and applications}},
url = {http://www.springerlink.com/index/10.1007/s11042-010-0660-6},
volume = {51},
year = {2010}
}
@article{Bajuraa,
author = {Bajura, Michael and Hill, U N C Chapel and Neumann, Ulrich and Reality, Keywords Augmented},
file = {::},
keywords = {augmented reality,reality,registration,virtual},
title = {{Dynamic Registration Correction in Video-Based Augmented Reality Systems}}
}
@article{Systems2003,
author = {Systems, Interactive Graphics},
file = {::},
journal = {Science},
publisher = {Citeseer},
title = {{Storytelling in Collaborative Augmented Reality Environments}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.1924\&amp;rep=rep1\&amp;type=pdf},
year = {2003}
}
@article{Knoerlein2007,
address = {New York, New York, USA},
author = {Knoerlein, Benjamin},
doi = {10.1145/1255047.1255065},
file = {::},
isbn = {9781595936400},
journal = {Proceedings of the},
keywords = {augmented reality,collaboration,haptics},
pages = {91},
publisher = {ACM Press},
title = {{Visuo-haptic collaborative augmented reality ping-pong}},
url = {http://portal.acm.org/citation.cfm?doid=1255047.1255065 http://portal.acm.org/citation.cfm?id=1255065},
year = {2007}
}
@article{Choi2010,
author = {Choi, Jinhyuk and Jang, Bongkyu and Kim, Gerard J.},
doi = {10.1007/s00779-010-0343-3},
file = {::},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {augmented reality,geospatial tags,mobile interface},
month = nov,
pages = {641--647},
title = {{Organizing and presenting geospatial tags in location-based augmented reality}},
url = {http://www.springerlink.com/index/10.1007/s00779-010-0343-3},
year = {2010}
}
@article{Kurz2012a,
author = {Kurz, Daniel and Benhimane, Selim},
file = {::},
journal = {Computers \& Graphics},
number = {7},
pages = {866--883},
title = {{Handheld Augmented Reality involving gravity measurements}},
url = {http://da.nielkurz.de/content/Handheld\_Augmented\_Reality\_involving\_gravity\_measurements},
volume = {36},
year = {2012}
}
@article{Lamberti2003a,
address = {New York, New York, USA},
author = {Lamberti, Fabrizio and Zunino, Claudio and Sanna, Andrea and Fiume, Antonino and Maniezzo, Marco},
file = {::},
journal = {Proceeding of the eighth international conference on 3D web technology - Web3D '03},
pages = {55},
publisher = {ACM Press},
title = {{An accelerated remote graphics architecture for PDAS}},
url = {http://portal.acm.org/citation.cfm?doid=636593.636602},
year = {2003}
}
@inproceedings{Kurz2012,
author = {Kurz, Daniel and Olszamowski, Thomas and Benhimane, Selim},
booktitle = { IEEE and ACM International Symposium on Mixed and Augmented Reality},
title = {{Representative Feature Descriptor Sets for Robust Handheld Camera Localization}},
url = {http://da.nielkurz.de/content/Representative\_Feature\_Descriptor\_Sets\_for\_Robust\_Handheld\_Camera\_Localization},
year = {2012}
}
@article{Steinicke2008a,
author = {Steinicke, Frank and Bruder, Gerd and Ropinski, Timo and Hinrichs, Klaus},
file = {::},
journal = {Proceedings of IEEE VRIC 2008 : 10th International Conference on Virtual Reality},
title = {{Moving Towards Generally Applicable Redirected Walking}},
url = {http://viscg.uni-muenster.de/publications/2008/SBRH08/},
year = {2008}
}
@article{Rosenblum2008a,
author = {Rosenblum, Editors Lawrence and Julier, Simon and Bruns, Erich},
file = {::},
journal = {IEEE Computer Graphics and Applications},
pages = {98--102},
publisher = {IEEE Computer Society},
title = {{Projects in VR}},
url = {http://doi.ieeecomputersociety.org/10.1109/MCG.2008.77},
year = {2008}
}
@article{Lee2005,
author = {Lee, W and Woo, Woontack and Lee, Jongweon},
file = {::},
journal = {Personal Computing},
keywords = {augmented reality,table top game,tangible user interface},
pages = {0--4},
title = {{TARBoard: Tangible Augmented Reality System for Table-top Game Environment}},
url = {http://www.ipsi.fraunhofer.de/ambiente/pergames2005/papers\_2005/PerGames2005\_TARBoard\_WLee.pdf},
volume = {5},
year = {2005}
}
@inproceedings{Prince2002,
abstract = {We present a complete system for live capture of 3D content and simultaneous presentation in augmented reality. The user sees the real world from his viewpoint, but modified so that the image of a remote collaborator is rendered into the scene. Fifteen cameras surround the collaborator, and the resulting video streams are used to construct a three-dimensional model of the subject using a shape-from-silhouette algorithm. Users view a two-dimensional fiducial marker using a video-see-through augmented reality interface. The geometric relationship between the marker and head-mounted camera is calculated, and the equivalent view of the subject is computed and drawn into the scene. Our system can generate 384 288 pixel images of the models at 25 fps, with a latency of < 100 ms. The result gives the strong impression that the subject is a real part of the 3D scene. We demonstrate applications of this system in 3D videoconferencing and entertainment.},
author = {Prince, S J D and Cheok, A D and Farbiz, F and Williamson, T and Johnson, N and Billinghurst, M and Kato, H},
booktitle = {International Symposium on Mixed and Augmented Reality ISMAR},
doi = {10.1109/ISMAR.2002.1115062},
file = {::},
isbn = {0769517811},
pages = {7--13},
publisher = {IEEE Comput. Soc},
title = {3d live: real time captured content for mixed reality},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1115062},
year = {2002}
}
@article{Barakonyi2007,
author = {Barakonyi, Istvan and Prendinger, Helmut and Schmalstieg, Dieter and Ishizuka, Mitsuru},
doi = {10.1109/3DUI.2007.340777},
file = {::},
isbn = {1-4244-0907-1},
journal = {2007 IEEE Symposium on 3D User Interfaces},
keywords = {augmented reality,eye tracking,multimodal interaction,remote collaboration,tangible interface},
pages = {71--78},
publisher = {Ieee},
title = {{Cascading Hand and Eye Movement for Augmented Reality Videoconferencing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4142848},
year = {2007}
}
@article{Lee2011,
author = {Lee, Jae Yeol and Seo, Dong Woo and Rhee, Gue Won},
doi = {10.1016/j.compind.2010.07.003},
file = {::},
issn = {01663615},
journal = {Computers in Industry},
month = jan,
number = {1},
pages = {107--119},
publisher = {Elsevier B.V.},
title = {{Tangible authoring of 3D virtual scenes in dynamic augmented reality environment}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0166361510001077},
volume = {62},
year = {2011}
}
@inproceedings{You2010,
author = {You, Suya and Neumann, Ulrich},
booktitle = {Internet Technology and Applications 2010 International Conference on},
file = {::},
keywords = {augmented reality,e,e business,image reacognition,internet,learning,mobile augmented reality enhancing e learning},
pages = {1--4},
publisher = {IEEE},
title = {{Mobile Augmented Reality for Enhancing E-Learning and E-Business}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5566168},
year = {2010}
}
@article{Regenbrecht2002,
address = {New York, New York, USA},
author = {Regenbrecht, HT},
doi = {10.1145/506444.506451},
file = {::},
isbn = {1581134541},
journal = {CHI\&\#39;02 extended abstracts on},
keywords = {augmented reality,collaboration,cscw,tangible user},
number = {figure 3},
pages = {504},
publisher = {ACM Press},
title = {{Interaction in a collaborative augmented reality environment}},
url = {http://portal.acm.org/citation.cfm?doid=506443.506451 http://portal.acm.org/citation.cfm?id=506451},
year = {2002}
}
@article{Martens2004,
author = {Martens, Jean-Bernard and Qi, Wen and Aliakseyeu, Dima and Kok, Arjan J F and {Van Liere}, Robert},
doi = {10.1145/1031419.1031425},
file = {::},
isbn = {1581139926},
journal = {Proceedings of the 2nd European Union symposium on Ambient intelligence EUSAI 04},
keywords = {3d interaction,augmented reality,human computer interaction,natural,optical tracking,virtual reality},
number = {November},
pages = {25},
publisher = {ACM Press},
title = {{Experiencing 3D interactions in virtual reality and augmented reality}},
url = {http://portal.acm.org/citation.cfm?doid=1031419.1031425},
volume = {telligence},
year = {2004}
}
@article{Godet-Bar2010,
abstract = {Context: Every interactive system is composed of a functional core and a user interface. However, the software engineering (SE) and human-computer interaction (HCI) communities do not share the same methods, models or tools. This usually induces a large work overhead when specialists from the two domains try to connect their applicative studies, especially when developing augmented reality systems that feature complex interaction cores. Objective: We present in this paper the essential activities and concepts of a development method integrating the SE and HCI development practices, from the specifications down to the design, as well as their application on a case study. Method: The efficiency of the method was tested in a qualitative study involving four pairs of SE and HCI experts in the design of an application for which an augmented reality interaction would provide better user performance than a classic interactive system. The effectivity of the method was evaluated in a qualitative study comparing the quality of three implementations of the same application fragment (based on the same analysis model), using software engineering metrics. Results: The first evaluation confirmed the ease of use of our method and the relevance of our tools for guiding the design process, but raised concerns on the handling of conflicting collaborative activities. The second evaluation gave indications that the structure of the analysis model facilitates the implementation of quality software (in terms of coupling, stability and complexity). Conclusion: It is concluded that our method enables design teams with different backgrounds in application development to collaborate for integrating augmented reality applications with information systems. Areas of improvement are also described. 2009 Elsevier B.V. All rights reserved.},
author = {Godet-Bar, G and Rieu, D and Dupuy-Chessa, S},
doi = {10.1016/j.infsof.2009.11.007},
file = {::},
issn = {09505849},
journal = {Information and Software Technology},
number = {5},
pages = {492--505},
publisher = {Elsevier B.V.},
title = {{HCI and business practices in a collaborative method for augmented reality systems}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584909002079},
volume = {52},
year = {2010}
}
@article{Broll2004,
author = {Broll, Wolfgang and Lindt, Irma and Ohlenburg, J and Wittkamper, Michael and Yuan, C and Novotny, T and Schieck, A F and Mottram, C and Strothman, A},
file = {::},
issn = {18602037},
journal = {Journal of Virtual Reality and Broadcasting},
keywords = {architectural design,augmented reality,simu,tangible user,terfaces,urban planning},
number = {1},
pages = {1--10},
publisher = {Citeseer},
title = {{ARTHUR: A Collaborative Augmented Environment for Architectural Design and Urban Planning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.184\&amp;rep=rep1\&amp;type=pdf},
volume = {1},
year = {2004}
}
@article{Badeche2006,
author = {Badeche, M. and Benmohamed, M.},
doi = {10.1109/ICTTA.2006.1684654},
file = {::},
isbn = {0-7803-9521-2},
journal = {2006 2nd International Conference on Information \& Communication Technologies},
keywords = {augmented reality,corner detection,corner tracking,in certain applications of,is very critical,kalman filter,process of tracking,real-,s factor,time,time tracking,what impose that the},
pages = {1773--1778},
publisher = {Ieee},
title = {{Real-Time Tracking for Augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1684654},
volume = {1},
year = {2006}
}
@article{Shen2010,
author = {Shen, Y. and Ong, S.K. and a.Y.C. Nee},
doi = {10.1016/j.destud.2009.11.001},
file = {::},
issn = {0142694X},
journal = {Design Studies},
keywords = {collaborative design,interface design,product design,virtual reality},
month = mar,
number = {2},
pages = {118--145},
publisher = {Elsevier Ltd},
title = {{Augmented reality for collaborative product design and development}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0142694X0900091X},
volume = {31},
year = {2010}
}
@article{Kaufmann2003,
author = {Kaufmann, Hannes and Schmalstieg, Dieter},
doi = {10.1016/S0097-8493(03)00028-1},
file = {::},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {augmented reality,geometry education,mathematics education,spatial intelligence},
month = jun,
number = {3},
pages = {339--345},
title = {{Mathematics and geometry education with collaborative augmented reality}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849303000281},
volume = {27},
year = {2003}
}
@incollection{Costanza2009,
abstract = {This chapter presents an overview of the Mixed Reality (MR) paradigm, which proposes to overlay our real-world environment with digital, computer-generated objects. It presents example applications and outlines limitations and solutions for their technical implementation. In MR systems, users perceive both the physical environment around them and digital elements presented through, for example, the use of semitransparent displays. By its very nature, MR is a highly interdisciplinary field engaging signal processing, computer vision, computer graphics, user interfaces, human factors, wearable computing, mobile computing, information visualization, and the design of displays and sensors. This chapter presents potential MR applications, technical challenges in realizing MR systems, as well as issues related to usability and collaboration in MR. It separately presents a section offering a selection of MR projects which have either been partly or fully undertaken at Swiss universities and rounds off with a section on current challenges and trends.},
author = {Costanza, Enrico and Kunz, Andreas and Fjeld, Morten},
booktitle = {Human Machine Interaction},
file = {::},
pages = {47--68},
publisher = {Springer-Verlag},
title = {{Mixed Reality: A Survey}},
url = {http://eprints.ecs.soton.ac.uk/20953/},
volume = {LNCS 5440},
year = {2009}
}
@article{Lamboray2004,
author = {Lamboray, E. and Wurmlin, S. and Gross, M.},
doi = {10.1109/VR.2004.1310060},
file = {::},
isbn = {0-7803-8415-6},
journal = {IEEE Virtual Reality 2004},
pages = {91--281},
publisher = {Ieee},
title = {{Real-time streaming of point-based 3D video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1310060},
year = {2004}
}
@article{Kroeker2010,
author = {Kroeker, Kirk L.},
doi = {10.1145/1785414.1785422},
file = {::},
issn = {00010782},
journal = {Communications of the ACM},
month = jul,
number = {7},
pages = {19},
title = {{Mainstreaming augmented reality}},
url = {http://portal.acm.org/citation.cfm?doid=1785414.1785422},
volume = {53},
year = {2010}
}
@article{ElSayed2011,
author = {a.M. {El Sayed}, Neven and Zayed, Hala H. and Sharawy, Mohamed I.},
doi = {10.1016/j.compedu.2010.10.019},
file = {::},
isbn = {0101355491},
issn = {03601315},
journal = {Computers \& Education},
month = may,
number = {4},
pages = {1045--1061},
publisher = {Elsevier Ltd},
title = {{ARSC: Augmented reality student card}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0360131510003040},
volume = {56},
year = {2011}
}
@article{Schmalstieg2002,
author = {Schmalstieg, Dieter and Fuhrmann, Anton and Hesina, Gerd and Szalav\'{a}ri, Zsolt and Encarna\c{c}\~{a}o, L. Miguel and Gervautz, Michael and Purgathofer, Werner},
doi = {10.1162/105474602317343640},
file = {::},
issn = {1054-7460},
journal = {Presence: Teleoperators and Virtual Environments},
month = feb,
number = {1},
pages = {33--54},
title = {{The Studierstube Augmented Reality Project}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/105474602317343640},
volume = {11},
year = {2002}
}
@article{Lee2009,
abstract = {We describe a novel markerless camera tracking approach and user interaction methodology for augmented reality (AR) on unprepared tabletop environments. We propose a real-time system architecture that combines two types of feature tracking. Distinctive image features of the scene are detected and tracked frame-to-frame by computing optical flow. In order to achieve real-time performance, multiple operations are processed in a synchronized multi-threaded manner: capturing a video frame, tracking features using optical flow, detecting distinctive invariant features, and rendering an output frame. We also introduce user interaction methodology for establishing a global coordinate system and for placing virtual objects in the AR environment by tracking a user's outstretched hand and estimating a camera pose relative to it. We evaluate the speed and accuracy of our hybrid feature tracking approach, and demonstrate a proof-of-concept application for enabling AR in unprepared tabletop environments, using bare hands for interaction.},
author = {Lee, Taehee and H\"{o}llerer, Tobias},
doi = {10.1109/TVCG.2008.190},
file = {::},
issn = {1077-2626},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Artificial Intelligence,Computer Graphics,Computer Simulation,Hand,Hand: anatomy \& histology,Hand: physiology,Humans,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Models, Biological,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,User-Computer Interface},
number = {3},
pages = {355--68},
pmid = {19282544},
title = {{Multithreaded hybrid feature tracking for markerless augmented reality.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19282544},
volume = {15},
year = {2009}
}
@article{Krohn2005a,
author = {Krohn, a. and Beigl, M. and Hazas, M. and Gellersen, H.-W.},
file = {::},
journal = {25th IEEE International Conference on Distributed Computing Systems Workshops},
pages = {463--468},
publisher = {Ieee},
title = {{Using Fine-Grained Infrared Positioning to Support the Surface-Based Activities of Mobile Users}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1437212},
year = {2005}
}
@article{Ong2009,
author = {Ong, S.K. and Shen, Y.},
doi = {10.1016/j.cirp.2009.03.020},
file = {::},
issn = {00078506},
journal = {CIRP Annals - Manufacturing Technology},
number = {1},
pages = {139--142},
title = {{A mixed reality environment for collaborative product design and development}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0007850609000225},
volume = {58},
year = {2009}
}
@article{Newman2006a,
author = {Newman, J. and Schall, G. and Barakonyi, I. and Schurzinger, A. and Schmalstieg, D.},
file = {::},
journal = {Advances in Pervasive Computing},
pages = {3--6},
title = {{Wide-Area Tracking Tools for Augmented Reality}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Wide-Area+Tracking+Tools+for+Augmented+Reality\#0},
volume = {207},
year = {2006}
}
@incollection{Khoo2010,
abstract = {This chapter presents steps for designing an intergenerational mixed reality entertainment system, which focuses on physical and social interactions using a mixed reality floor system. The main design goals include the following: facilitating interactions between users with varied levels of skill in utilizing technology, utilizing the familiar physical motions from other activities to make an intuitive physical interface, and encouraging social interactions among families and friends. Detailed implementation of these steps is presented in the design of our intergenerational entertainment system, Age Invaders. Our design process is based on user-centered design. The results of the study help to focus the refinements of the existing platform from a usability standpoint and also aid in the development of new physical entertainment and interactive applications. This study provides insights into user issues including how users interact in a complex mixed reality experience.},
author = {Khoo, Eng Tat and Merritt, Tim and Cheok, Adrian David},
booktitle = {The Engineering of Mixed Reality Systems},
chapter = {7},
doi = {10.1007/978-1-84882-733-2},
editor = {Dubois, Emmanuel and Gray, Philip and Nigay, Laurence},
file = {::},
isbn = {9781848827349},
pages = {121--141},
publisher = {Springer London},
series = {Human-Computer Interaction Series},
title = {{Designing a Mixed Reality Intergenerational Entertainment System}},
url = {http://www.springerlink.com/content/r60h1v521r572j71},
year = {2010}
}
@article{Guan2010,
abstract = {This paper focuses on online scene learning and fast camera relocalisation which are two key problems currently limiting the performance of wide area augmented reality systems. Firstly, we propose to use adaptive random trees to deal with the online scene learning problem. The algorithm can provide more accurate recognition rates than traditional methods, especially with large scale workspaces. Secondly, we use the enhanced PROSAC algorithm to obtain a fast camera relocalisation method. Compared with traditional algorithms, our method can significantly reduce the computation complexity, which facilitates to a large degree the process of online camera relocalisation. Finally, we implement our algorithms in a multithreaded manner by using a parallel-computing scheme. Camera tracking, scene mapping, scene learning and relocalisation are separated into four threads by using multi-CPU hardware architecture. While providing real-time tracking performance, the resulting system also possesses the ability to track multiple maps simultaneously. Some experiments have been conducted to demonstrate the validity of our methods.},
author = {Guan, Tao and Duan, Liya and Chen, Yongjian and Yu, Junqing},
doi = {10.3390/s100606017},
file = {::},
issn = {1424-8220},
journal = {Sensors},
month = jun,
number = {6},
pages = {6017--6043},
title = {{Fast Scene Recognition and Camera Relocalisation for Wide Area Augmented Reality Systems}},
url = {http://www.mdpi.com/1424-8220/10/6/6017/},
volume = {10},
year = {2010}
}
@article{Ni2006b,
author = {Ni, T and Schmidt, G S and Staadt, O G and Livingston, M A and Ball, R and May, R},
journal = {Virtual Reality},
title = {{A Survey of Large High-Resolution Display Technologies, Techniques, and Applications}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:A+Survey+of+Large+High-Resolution+Display+Technologies,+Techniques,+and+Applications\#0},
year = {2006}
}
@article{Do-Lenh2010,
abstract = {Tangible User Interfaces (TUIs) offer the potential to facilitate collaborative learning in new ways. This paper presents an empirical study that investigated the effects of a TUI in a classroom setting on task performance and learning outcomes. In the tangible condition, apprentices worked together around an interactive tabletop warehouse simulation using tangible inputs. In the paper condition, they performed the same activity with only paper and pens. Results showed that the tangible condition resulted in better task performance (more alternative solutions explored and better final solution) but did not affect learning outcomes, i.e. understanding of important concepts and applying them to a problem-solving question. We discuss reasons for this in terms of task structure and type, nature of tangible user interfaces and effective interaction requirements.},
author = {Do-Lenh, Son and Jermann, Patrick},
doi = {10.1007/978-3-642-16020-2},
editor = {Wolpers, Martin and Kirschner, Paul A and Scheffel, Maren and Lindstaedt, Stefanie and Dimitrova, Vania},
file = {::},
isbn = {9783642160196},
journal = {Learning},
keywords = {augmented reality,collaborative learning,human computer,tabletop,tangible user interface,technology enhanced learning},
pages = {78--92},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Task Performance vs . Learning Outcomes : A Study of a Tangible User Interface in the Classroom}},
url = {http://www.springerlink.com/content/dt71j12632438276/},
volume = {6383},
year = {2010}
}
@article{Kim2010,
author = {Kim, Seungjun and Dey, Anind K.},
doi = {10.1016/j.cad.2008.10.009},
file = {::},
issn = {00104485},
journal = {Computer-Aided Design},
month = may,
number = {5},
pages = {373--386},
publisher = {Elsevier Ltd},
title = {{AR interfacing with prototype 3D applications based on user-centered interactivity}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S001044850800198X},
volume = {42},
year = {2010}
}
@phdthesis{Systeme2004a,
author = {Systeme, Interaktive and Universit, Technischen and Schmalstieg, Dieter and Reitmayr, Gerhard},
file = {::},
title = {{XML Databases for Augmented Reality}},
year = {2004}
}
@article{Tian2010,
abstract = {To produce a realistic augmentation in Augmented Reality, the correct relative positions of real objects and virtual objects are very important. In this paper, we propose a novel real-time occlusion handling method based on an object tracking approach. Our method is divided into three steps: selection of the occluding object, object tracking and occlusion handling. The user selects the occluding object using an interactive segmentation method. The contour of the selected object is then tracked in the subsequent frames in real-time. In the occlusion handling step, all the pixels on the tracked object are redrawn on the unprocessed augmented image to produce a new synthesized image in which the relative position between the real and virtual object is correct. The proposed method has several advantages. First, it is robust and stable, since it remains effective when the camera is moved through large changes of viewing angles and volumes or when the object and the background have similar colors. Second, it is fast, since the real object can be tracked in real-time. Last, a smoothing technique provides seamless merging between the augmented and virtual object. Several experiments are provided to validate the performance of the proposed method.},
author = {Tian, Yuan and Guan, Tao and Wang, Cheng},
doi = {10.3390/s100402885},
file = {::},
issn = {1424-8220},
journal = {Sensors},
keywords = {augmented reality,graph cuts,mean shift,occlusion,optical flow,tracking},
month = mar,
number = {4},
pages = {2885--2900},
title = {{Real-Time Occlusion Handling in Augmented Reality Based on an Object Tracking Approach}},
url = {http://www.mdpi.com/1424-8220/10/4/2885/},
volume = {10},
year = {2010}
}
@article{Chen2009,
abstract = {This paper combines Tangible Augmented Reality and shape grammar into collaborative design learning to bridge the gaps such as the difficulties of imaging the spatial form in a complex content and the obstacle of communication during the collaborative design. This work has been successful in mapping out a space of technical possibilities and providing a possible system setup to pursue the innovative idea. It not only describes the latent trends and assumptions that might be used to motivate and guide the design in cooperative work, but also makes links with existing research in cognitive science and education.},
author = {Chen, I R and Wang, X and Wang, W},
doi = {10.1109/CSCWD.2009.4968103},
file = {::},
isbn = {9781424435340},
journal = {2009 13th International Conference on Computer Supported Cooperative Work in Design},
keywords = {augmented reality,shape grammar,tangible augmented reality,tangible user},
pages = {468--473},
publisher = {IEEE Comput. Soc},
title = {{Bridging shape grammar and Tangible Augmented Reality into collaborative design learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4968103},
year = {2009}
}
@article{Regenbrecht2006,
author = {Regenbrecht, H. and Ott, C. and Wagner, M. and Lum, T. and Kohler, P. and Wilke, W. and Mueller, E.},
doi = {10.1109/ISMAR.2003.1240725},
file = {::},
isbn = {0-7695-2006-5},
journal = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
pages = {290--291},
publisher = {IEEE Comput. Soc},
title = {{An augmented virtuality approach to 3D videoconferencing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1240725},
year = {2006}
}
@article{Kaufmann2003a,
abstract = {Construct3D is a 3D geometric construction tool specifically designed for mathematics and geometry education. It is based on the mobile collaborative augmented reality system Studierstube. We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. In order to support various teacher-student interaction scenarios we implemented flexible methods for context and user dependent rendering of parts of the construction. Together with hybrid hardware setups they allow the use of Construct3D in today's classrooms and provide a testbed for future evaluations. Means of application and integration in mathematics and geometry education at high school as well as university level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions and improves spatial skills.},
author = {Kaufmann, Hannes and Schmalstieg, Dieter},
doi = {10.1016/S0097-8493(03)00028-1},
file = {::},
isbn = {1581135254},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {augmented reality,geometry education,mathematics education,spatial intelligence},
number = {3},
pages = {339--345},
publisher = {ACM Press},
title = {{Mathematics and geometry education with collaborative augmented reality}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849303000281},
volume = {27},
year = {2003}
}
@article{Ag,
author = {Ag, Daimlerchrysler},
file = {::},
journal = {Virtual Reality},
number = {3},
pages = {338--355},
title = {{Using Augmented Virtuality for}},
volume = {13}
}
@article{Green2008,
abstract = {Future space exploration will demand the cultivation of human-robotic systems, however, little attention has been paid to the development of human-robot teams. Current methods for autonomous plan creation are often complex and difficult to use. So a system is needed that enables humans and robotic systems to naturally and effectively collaborate. Effective collaboration takes place when the participants are able to communicate in a natural and effective manner. Grounding, the common understanding between conversational participants, shared spatial referencing and situational awareness, are crucial components of communication and collaboration. This paper briefly reviews the fields of human-robot interaction and Augmented Reality (AR), the overlaying of computer graphics onto the real worldview. The strengths of AR are discussed and how they might be used for human-robot collaboration is described. Then a description of an architecture that we have developed is given that uses AR as a means for real time understanding of the shared spatial scene. This architecture enables grounding and enhances situational awareness, thus laying the necessary groundwork for natural and effective human-robot collaboration.},
author = {Green, Scott A and Billinghurst, Mark and Chen, Xiaoqi and Chase, J Geoffrey},
file = {::},
journal = {International Journal of Advanced Robotic Systems},
keywords = {augmented reality,collaboration,communication,computer interaction,human,robot,robot interaction,robotics},
number = {1},
pages = {1--18},
publisher = {ASME},
title = {{Human-robot collaboration: A literature review and augmented reality approach in design}},
url = {http://ir.canterbury.ac.nz/handle/10092/2262},
volume = {5},
year = {2008}
}
@article{Bruder2009a,
author = {Bruder, G. and Steinicke, F. and Hinrichs, K.H.},
file = {::},
journal = {2009 IEEE Symposium on 3D User Interfaces},
keywords = {3d user interfaces,architectural walkthroughs,locomotion,passive haptic feed-,redirected walking,virtual environments},
month = mar,
pages = {75--82},
publisher = {Ieee},
title = {{Arch-Explore: A natural user interface for immersive architectural walkthroughs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4811208},
year = {2009}
}
@article{Jung2010,
author = {Jung, Sungmo and Song, Jae-gu and Hwang, Dae-Joon and Ahn, Jae Young and Kim, Seoksoo},
doi = {10.3390/s101109857},
file = {::},
issn = {1424-8220},
journal = {Sensors},
month = nov,
number = {11},
pages = {9857--9871},
title = {{A Study on Software-based Sensing Technology for Multiple Object Control in AR Video}},
url = {http://www.mdpi.com/1424-8220/10/11/9857/},
volume = {10},
year = {2010}
}
@article{Papagiannakis2008,
abstract = {Recent advances in hardware and software for mobile computing have enabled a new breed of mobile AR systems and applications. A new breed of computing called augmented ubiquitous computing has resulted from the convergence of wearable computing, wireless networking and mobile AR interfaces. In this paper we provide a survey of different mobile and wireless technologies and how they have impact AR. Our goal is to place them into different categories so that it becomes easier to understand the state of art and to help identify new directions of research.},
author = {Papagiannakis, George and Singh, Gurminder and Magnenat-Thalmann, Nadia},
doi = {10.1002/cav.221},
file = {::},
institution = {MIRALab, University of Geneva},
issn = {15464261},
journal = {Computer Animation And Virtual Worlds},
keywords = {augmented mixed reality,mobile systems,wireless networking},
number = {1},
pages = {3--22},
publisher = {Wiley Online Library},
title = {{A survey of mobile and wireless technologies for augmented reality systems}},
url = {http://doi.wiley.com/10.1002/cav.221},
volume = {19},
year = {2008}
}
@inproceedings{Wagner2007,
abstract = {In this paper we present ARToolKitPlus, a successor to the popular ARToolKit pose tracking library. ARToolKitPlus has been optimized and extended for the usage on mobile devices such as smartphones, PDAs and Ultra Mobile PCs (UMPCs). We explain the need and specific requirements of pose tracking on mobile devices and how we met those requirements. To prove the applicability we performed an extensive benchmark series on a braod range of off-the-shelf handhelds.},
author = {Wagner, Daniel and Schmalstieg, Dieter},
booktitle = {Proceedings of 12th Computer Vision Winter Workshop CVWW07},
doi = {10.1.1.157.1879},
file = {::},
pages = {139--146},
publisher = {Citeseer},
title = {{ARToolKitPlus for Pose Tracking on Mobile Devices ARToolKit}},
url = {http://www.icg.tu-graz.ac.at/Members/daniel/ARToolKitPlusMobilePoseTracking},
year = {2007}
}
@article{Azuma1997,
abstract = {This paper surveys the field of Augmented Reality, in which 3-D virtual objects are integrated into a 3-D real environment in real time. It describes the medical, manufacturing, visualization, path planning, entertainment and military applications that have been explored. This paper describes the characteristics of Augmented Reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective Augmented Reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using Augmented Reality.},
author = {Azuma, Ronald T},
doi = {10.1.1.30.4999},
file = {::},
issn = {10547460},
journal = {Media},
number = {4},
pages = {355--385},
publisher = {Citeseer},
title = {{A Survey of Augmented Reality}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.35.5387\&amp;rep=rep1\&amp;type=pdf},
volume = {6},
year = {1997}
}
@article{Coiras2007,
abstract = {A proof of concept for a model-less target detection and classification system for side-scan imagery is presented. The system is based on a supervised approach that uses augmented reality (AR) images for training computer added detection and classification (CAD/CAC) algorithms, which are then deployed on real data. The algorithms are able to generalise and detect real targets when trained on AR ones, with performances comparable with the state-of-the-art in CAD/CAC. To illustrate the approach, the focus is on one specific algorithm, which uses Bayesian decision and the novel, purpose-designed central filter feature extractors. Depending on how the training database is partitioned, the algorithm can be used either for detection or classification. Performance figures for these two modes of operation are presented, both for synthetic and real targets. Typical results show a detection rate of more that 95\% and a false alarm rate of less than 5\%. The proposed supervised approach can be directly applied to train and evaluate other learning algorithms and data representations. In fact, a most important aspect is that it enables the use of a wealth of legacy pattern recognition algorithms for the sonar CAD/CAC applications of target detection and target classification},
author = {Coiras, E and Mignotte, P Y and Petillot, Y and Bell, J and Lebart, K},
doi = {10.1049/iet-rsn:20060098},
file = {::},
issn = {17518784},
journal = {Radar Sonar Navigation IET},
number = {1},
pages = {83--90},
title = {{Supervised target detection and classification by training on augmented reality data}},
volume = {1},
year = {2007}
}
@article{Abdullah2002,
abstract = {Camera calibration is an essential and important part of an Augmented Reality (AR) system. The use of a planebased calibration technique can give a good accuracy, which can be important for AR applications. The calibration technique used in the current ARToolKit requires user intervention, which is prone to error and involves a lengthy calibration time. The camera has to be recalibrated every time the focal length changes which is cumbersome and less suitable for applications where a more automated and easier approach is needed. This paper investigates the use of camera self-calibration for the ARToolKit, which has the advantage of simplicity of implementation. In order to improve its accuracy, a distortion model is also investigated. In this context several interesting results are presented.},
author = {Abdullah, Junaidi and Martinez, Kirk},
file = {::},
title = {{Camera Self-Calibration for the ARToolkit}},
url = {http://eprints.ecs.soton.ac.uk/8885/},
year = {2002}
}
@article{Humphreys2002a,
author = {Humphreys, Greg and Houston, Mike and Ng, Ren and Frank, Randall and Ahern, Sean and Kirchner, Peter D. and Klosowski, James T.},
journal = {Work},
keywords = {cluster rendering,dering,parallel ren-,remote graphics,scalable rendering,stream,tiled displays,virtual graphics},
month = jul,
number = {3},
title = {{Chromium: a stream-processing framework for interactive rendering on clusters}},
url = {http://portal.acm.org/citation.cfm?doid=566654.566639},
volume = {21},
year = {2002}
}
@inproceedings{Field2004a,
address = {Hobart, Tasmania},
author = {Field, Tom and Bay, Sandy and Vamplew, Peter},
booktitle = {AISAT2004: International Conference on Artificial Intelligence in Science and Technology},
file = {::},
title = {{Generalised Algorithms for Redirected Walking in Virtual Environments}},
url = {http://eprints.utas.edu.au/109/},
year = {2004}
}
@article{Hachet2008a,
author = {Hachet, M and Kitamura, Y},
file = {::},
journal = {Science},
pages = {1--4},
title = {{3D interaction with and from handheld computers}},
url = {http://www.recolecta.net/buscador/single\_page.jsp?id=oai:hal.archives-ouvertes.fr:hal-00308241\_v1},
year = {2008}
}
@phdthesis{Glenncross2002a,
author = {Glenncross, Masshuda},
file = {::},
school = {University of Manchester},
title = {{A Framework for Physically Based Modelling in Virtual Reality}},
year = {2002}
}
@article{Siegl2007,
author = {Siegl, H and Hanheide, M and Wrede, S and Pinz, a},
doi = {10.1016/j.imavis.2006.04.027},
file = {::},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {3d cursor,cognitive vision system,computer interaction,human,mobile augmented reality},
month = dec,
number = {12},
pages = {1895--1903},
title = {{An augmented reality human–computer interface for object localization in a cognitive vision system}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885606002873},
volume = {25},
year = {2007}
}
@article{RobinKirk2005,
author = {{Robin Kirk}, Jan Newmarch},
file = {::},
journal = {Second IEEE Consumer Communications and Networking Conference, 2005. CCNC. 2005},
keywords = {- home networks,interoperability,location-based services,middleware,mobility,multimedia distribution protocols,multimedia technologies,network architecture,pervasive computing,session user and device},
number = {C},
pages = {343--347},
publisher = {Ieee},
title = {{A location-aware, service-based audio system}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1405194},
volume = {00},
year = {2005}
}
@article{Shih2003,
author = {Shih, T.K. and Lin, N.H.},
doi = {10.1109/ICDCSW.2003.1203626},
file = {::},
isbn = {0-7695-1921-0},
journal = {23rd International Conference on Distributed Computing Systems Workshops, 2003. Proceedings.},
keywords = {analog technology,communication will replace traditional,distance learning,mpeg,multimedia communication system,synchronization,video conferencing},
pages = {646--651},
publisher = {Ieee},
title = {{Augmented video conferencing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1203626},
year = {2003}
}
@article{Nirnimesh2007a,
abstract = {Cluster-based tiled display walls can provide cost-effective and scalable displays with high resolution and a large display area. The software to drive them needs to scale too if arbitrarily large displays are to be built. Chromium is a popular software API used to construct such displays. Chromium transparently renders any OpenGL application to a tiled display by partitioning and sending individual OpenGL primitives to each client per frame. Visualization applications often deal with massive geometric data with millions of primitives. Transmitting them every frame results in huge network requirements that adversely affect the scalability of the system. In this paper, we present Garuda, a client-server-based display wall framework that uses off-the-shelf hardware and a standard network. Garuda is scalable to large tile configurations and massive environments. It can transparently render any application built using the Open Scene Graph (OSG) API to a tiled display without any modification by the user. The Garuda server uses an object-based scene structure represented using a scene graph. The server determines the objects visible to each display tile using a novel adaptive algorithm that culls the scene graph to a hierarchy of frustums. Required parts of the scene graph are transmitted to the clients, which cache them to exploit the interframe redundancy. A multicast-based protocol is used to transmit the geometry to exploit the spatial redundancy present in tiled display systems. A geometry push philosophy from the server helps keep the clients in sync with one another. Neither the server nor a client needs to render the entire scene, making the system suitable for interactive rendering of massive models. Transparent rendering is achieved by intercepting the cull, draw, and swap functions of OSG and replacing them with our own. We demonstrate the performance and scalability of the Garuda system for different configurations of display wall. We also show that the server and network loads grow sublinearly with the increase in the number of tiles, which makes our scheme suitable to construct very large displays.},
author = {Nirnimesh, Harish P. and Narayanan, Pawan J.},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Algorithms,Computer Communication Networks,Computer Communication Networks: instrumentation,Computer Graphics,Computer Graphics: instrumentation,Computer-Assisted,Computer-Assisted: instrumen,Computer-Assisted: instrumentat,Computer-Assisted: methods,Data Display,Equipment Design,Equipment Failure Analysis,Image Enhancement,Image Enhancement: instrumentation,Image Enhancement: methods,Image Interpretation,Information Storage and Retrieval,Information Storage and Retrieval: methods,Microcomputers,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,User-Computer Interface},
number = {5},
pages = {864--77},
title = {{Garuda: A Scalable Tiled Display Wall Using Commodity PCs}},
volume = {13},
year = {2007}
}
@article{Morrison2011,
author = {Morrison, Ann and Mulloni, Alessandro and Lemmel\"{a}, Saija and Oulasvirta, Antti and Jacucci, Giulio and Peltonen, Peter and Schmalstieg, Dieter and Regenbrecht, Holger},
doi = {10.1016/j.cag.2011.04.009},
file = {::},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {mobile augmented reality},
month = aug,
number = {4},
pages = {789--799},
title = {{Collaborative use of mobile augmented reality with paper maps}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849311001129},
volume = {35},
year = {2011}
}
@article{Chen2009a,
author = {Chen, David M. and Tsai, Sam S. and Vedantham, Ramakrishna and Grzeszczuk, Radek and Girod, Bernd},
doi = {10.1109/ISMAR.2009.5336472},
file = {::},
isbn = {978-1-4244-5390-0},
journal = {2009 8th IEEE International Symposium on Mixed and Augmented Reality},
month = oct,
pages = {181--182},
publisher = {Ieee},
title = {{Streaming mobile augmented reality on mobile phones}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5336472},
year = {2009}
}
@article{Billinghurst2002,
author = {Billinghurst, Mark and Kato, Hirokazu},
doi = {10.1145/514236.514265},
issn = {00010782},
journal = {Communications of the ACM},
month = jul,
number = {7},
pages = {64--70},
publisher = {ACM},
title = {{Collaborative augmented reality}},
url = {http://portal.acm.org/citation.cfm?doid=514236.514265 http://portal.acm.org/citation.cfm?id=514265},
volume = {45},
year = {2002}
}
@article{Butz1999,
author = {Butz, A and H\"{o}llerer, T and Feiner, S and MacIntyre, B and Beshers, C},
doi = {10.1109/IWAR.1999.803804},
file = {::},
isbn = {0769503594},
journal = {Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality IWAR99},
pages = {35--44},
publisher = {IEEE Comput. Soc},
title = {{Enveloping users and computers in a collaborative 3D augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=803804},
volume = {99},
year = {1999}
}
@article{Vasilakos2008,
abstract = {Our system of Mixed Reality and 3D Live with Ambient Intelligence (AmI) is indented to bring performance art to the people while offering to the performance artists a creative tool to extend the grammar of the traditional theatre. Actors and dancers at different places are captured by multiple cameras and their images are rendered in 3D form in such a way that they can play and dance together on the same place in real-time. Our Quanticum Man is an allegory of the time of the General Relativity and the matter of the Quantum Mechanics. The new type of interactive theatre enables social networking by supporting simultaneous participants in human-to-human social manner.},
author = {Vasilakos, A and Wei, L and Nguyen, T and Thienqui, T and Chen, L and Boj, C and Diaz, D and Cheok, A and Marentakis, G},
doi = {10.1016/j.ins.2007.08.029},
file = {::},
issn = {00200255},
journal = {Information Sciences},
keywords = {3d live,ambient intelligence,interactive theatre,mixed reality,performance art},
number = {3},
pages = {679--693},
publisher = {Elsevier Science Inc.},
title = {{Interactive theatre via mixed reality and Ambient Intelligence}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020025507004033},
volume = {178},
year = {2008}
}
@article{Nilsson2009a,
abstract = {This paper presents results from a study on using an AR application to support collaborative command and control activities requiring the collaboration of three different civil service organisations. The technology is used to create a common ground between the organisations and allows the users to interact, plan resources and react to the ongoing events on a digital map. The AR application was developed and evaluated in a study where a forest fire scenario was simulated. Participants from the involved organisations acted as command and control teams in the simulated scenario and both quantitative and qualitative results were obtained. The results show that AR can become a useful tool in these situations in the future.},
author = {Nilsson, Susanna and Johansson, Bj\"{o}rn J E and J\"{o}nsson, Arne},
doi = {10.1145/1670252.1670291},
file = {::},
isbn = {9781605589121},
journal = {Proceedings of the 8th International Conference on Virtual Reality Continuum and its Applications in Industry VRCAI 09},
pages = {179},
publisher = {ACM Press},
series = {VRCAI '09},
title = {{A co-located collaborative augmented reality application}},
url = {http://portal.acm.org/citation.cfm?doid=1670252.1670291},
year = {2009}
}
@article{Zhou2008a,
author = {Zhou, F. and Duh, H.B.L. and Billinghurst, Mark},
doi = {10.1109/ISMAR.2008.4637362},
file = {::},
isbn = {978-1-4244-2840-3},
journal = {2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality},
month = sep,
pages = {193--202},
publisher = {IEEE},
title = {{Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4637362 http://www.computer.org/portal/web/csdl/doi/10.1109/ISMAR.2008.4637362},
year = {2008}
}
@article{Lee2009a,
author = {Lee, Sang and Choi, Junyeong and Park, Jong-il},
doi = {10.1109/TCE.2009.5174470},
file = {::},
issn = {0098-3063},
journal = {IEEE Transactions on Consumer Electronics},
month = may,
number = {2},
pages = {883--890},
title = {{Interactive e-learning system using pattern recognition and augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5174470},
volume = {55},
year = {2009}
}
@article{Thomas2006,
author = {Thomas, GA},
doi = {10.1109/MCG.2010.23},
file = {::},
issn = {0272-1716},
journal = {Visual Media Production, 2006. CVMP 2006. 3rd},
month = may,
number = {3},
pages = {56--68},
title = {{Real-time camera pose estimation for augmenting sports scenes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5396282 http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4156005},
volume = {31},
year = {2006}
}
@inproceedings{Fuhrmann1997,
author = {Fuhrmann, Anton and Liiffelmamr, Helwig and Schmalstieg, Dieter},
booktitle = {IEEE Visualization},
file = {::},
isbn = {0818682620},
pages = {459--462},
publisher = {IEEE Computer Society Press},
title = {{Collaborative augmented reality: exploring dynamical systems}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Collaborative+Augmented+Reality+:+Exploring+Dynamical+Systems\#0},
volume = {97},
year = {1997}
}
@article{Wither2011,
author = {Wither, Jason and Tsai, Yun-Ta and Azuma, Ronald},
doi = {10.1016/j.cag.2011.04.010},
file = {::},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {Evaluation,Information Presentation,Mixed Reality,User Interface},
month = aug,
number = {4},
pages = {810--822},
publisher = {Elsevier},
title = {{Indirect augmented reality}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849311001130},
volume = {35},
year = {2011}
}
@article{Broll2008a,
author = {Broll, Wolfgang and Lindt, Irma and Herbst, Iris and Ohlenburg, Jan and Braun, Anne-kathrin and Wetzel, Richard},
file = {::},
journal = {Advances},
title = {{Toward Next-Gen Mobile AR Games}},
year = {2008}
}
@inproceedings{Reitmayr2004,
abstract = {Augmented reality (AR) can provide an excellent user interface for visualization in a mobile computing application. The user's view is augmented with location based information at the correct spatial location, thus providing an intuitive way of presenting such information. In this work we demonstrate the use of AR for collaborative navigation and information browsing tasks in an urban environment. A navigation function allows one or more users to roam through a city and guides them to selected destinations. Information browsing presents users with information about objects in their surrounding. Both functions feature support for collaboration. The developed system does not only concentrate on the user interface aspects but also provides a scalable infrastructure to support mobile applications. To this end we developed a 3-tier architecture to manage a common data model for a set of applications. It is inspired by current Internet application frameworks and consists of a central storage layer using a common data model, a transformation layer responsible for filtering and adapting the data to the requirements of a particular applications on request, and finally of the applications itself.},
author = {Reitmayr, Gerhard and Schmalstieg, Dieter},
booktitle = {Science},
file = {::},
pages = {31--41},
publisher = {Citeseer},
title = {{Collaborative augmented reality for outdoor navigation and information browsing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.1864\&amp;rep=rep1\&amp;type=pdf},
volume = {66},
year = {2004}
}
@article{Sheng2010,
author = {Sheng, Yu and Yapo, Theodore C. and Cutler, Barbara},
doi = {10.1111/j.1467-8659.2009.01608.x},
file = {::},
issn = {01677055},
journal = {Computer Graphics Forum},
month = jun,
number = {2},
pages = {387--396},
title = {{Global Illumination Compensation for Spatially Augmented Reality}},
url = {http://doi.wiley.com/10.1111/j.1467-8659.2009.01608.x},
volume = {29},
year = {2010}
}
@article{Nilsson2009,
author = {Nilsson, Susanna and Gustafsson, Torbj\"{o}rn and Carleberg, Per},
file = {::},
journal = {PsychNology Journal},
keywords = {augmented reality,gaze,gaze controlled augmented reality,mixed reality},
number = {2},
pages = {175 -- 196},
publisher = {Citeseer},
title = {{Hands Free Interaction with Virtual Information in a Real Environment : Eye Gaze as an Interaction Tool in an Augmented Reality System}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Hands+Free+Interaction+with+Virtual+Information+in+a+Real+Environment+:+Eye+Gaze+as+an+Interaction+Tool+in+an+Augmented+Reality+System\#0},
volume = {7},
year = {2009}
}
@article{Ahn,
author = {Ahn, M.H.},
doi = {10.1109/ICPR.1998.712048},
file = {::},
isbn = {0-8186-8512-3},
journal = {Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)},
pages = {1694--1696},
publisher = {IEEE Comput. Soc},
title = {{Video augmentation by image-based rendering under the perspective camera model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=712048},
volume = {2}
}
@article{Gintautas2006,
abstract = {We present experimental data on the limiting behavior of an interreality system comprising a virtual horizontally driven pendulum coupled to its real-world counterpart, where the interaction time scale is much shorter than the time scale of the dynamical system. We present experimental evidence that if the physical parameters of the simplified virtual system match those of the real system within a certain tolerance, there is a transition from an uncorrelated dual reality state to a mixed reality state of the system in which the motion of the two pendula is highly correlated. The region in parameter space for stable solutions has an Arnold tongue structure for both the experimental data and for a numerical simulation. As virtual systems better approximate real ones, even weak coupling in other interreality systems may produce sudden changes to mixed reality states.},
author = {Gintautas, Vadas and Hubler, Alfred},
file = {::},
pages = {4},
title = {{Mixed Reality States in a Bidirectionally Coupled Interreality System}},
url = {http://arxiv.org/abs/physics/0611293},
year = {2006}
}
@article{Geller2008b,
author = {Geller, T.},
file = {::},
journal = {IEEE Computer Graphics and Applications},
number = {4},
pages = {11--17},
publisher = {IEEE Computer Society Press},
title = {{Overcoming the uncanny valley}},
url = {http://www.computer.org/portal/cms\_docs\_cga/cga/content/mcg2008040011.pdf},
volume = {28},
year = {2008}
}
@article{Wrzesien2010,
abstract = {Recent research presents Augmented Reality Exposure Therapy (ARET) for treatment of phobia of cockroaches as a potentially effective technique. However, to the authors' knowledge no studies have been published concerning the Human-Computer-Interaction issues of such a system. The aim of this paper is to report some preliminary data on how patients, therapists and an Augmented Reality system collaborate and interact during the therapeutic process. The results show that the therapeutic process is distributed between individuals (patient and therapist) and artifacts (e.g. AR cockroaches, a computer screen, a Head Mounted Display (HMD), a keyboard, a swatter and therapists' notes on paper). The results are discussed in terms of possible improvement of the ARET system.},
author = {Wrzesien, Maja and Burkhardt, Jean-Marie and {Alca\~{n}iz Raya}, Mariano and Botella, Cristina and {Bret\'{o}n L\'{o}pez}, Juana Maria},
file = {::},
institution = {Instituto Interuniversitario de Investigaci\'{o}n en Bioingenier\'{\i}a y Tecnolog\'{\i}a Orientada al Ser Humano, Universidad Polit\'{e}cnica de Valencia (I3BH), (UPV), 46022 Valencia, Espa\~{n}a. mwrzesien@labhuman.i3bh.es},
journal = {Studies In Health Technology And Informatics},
pages = {134--139},
pmid = {20543285},
title = {{Analysis of distributed-collaborative activity during augmented reality exposure therapy for cockroach phobia.}},
volume = {154},
year = {2010}
}
@article{Langlotz2011,
author = {Langlotz, Tobias and Degendorfer, Claus and Mulloni, Alessandro and Schall, Gerhard and Reitmayr, Gerhard and Schmalstieg, Dieter},
doi = {10.1016/j.cag.2011.04.004},
file = {::},
issn = {00978493},
journal = {Computers \& Graphics},
keywords = {Annotation,Augmented reality,Mobile phone,Tracking},
month = aug,
number = {4},
pages = {831--840},
publisher = {Elsevier},
title = {{Robust detection and tracking of annotations for outdoor augmented reality browsing}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849311001075},
volume = {35},
year = {2011}
}
@article{Pilet2010,
author = {Pilet, Julien and Saito, Hideo},
doi = {10.1109/VR.2010.5444811},
file = {:home/acmt/Dropbox/Documentos/Mendeley/2010 IEEE Virtual Reality Conference (VR)/2010/Pilet, Saito/Pilet, Saito - 2010 - Virtually augmenting hundreds of real pictures An approach based on learning, retrieval, and tracking.pdf:pdf},
isbn = {978-1-4244-6237-7},
journal = {2010 IEEE Virtual Reality Conference (VR)},
month = mar,
pages = {71--78},
publisher = {Ieee},
title = {{Virtually augmenting hundreds of real pictures: An approach based on learning, retrieval, and tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5444811},
year = {2010}
}
@inproceedings{Wang2009,
author = {Wang, R.Y. and Popovi\'{c}, Jovan},
booktitle = {ACM SIGGRAPH 2009 papers},
doi = {10.1145/1531326.1531369},
file = {::},
issn = {07300301},
keywords = {augmented reality,hand tracking,motion capture,user},
month = jul,
number = {3},
pages = {1--8},
publisher = {ACM},
title = {{Real-time hand-tracking with a color glove}},
url = {http://portal.acm.org/citation.cfm?doid=1531326.1531369 http://portal.acm.org/citation.cfm?id=1531369},
volume = {28},
year = {2009}
}
@phdthesis{DeLaRiviere2001,
author = {{De La Rivi\`{e}re}, Jean-Baptiste},
file = {::},
title = {{Interaction 3D : Utilisations Conjointes d’un pointeur Laser et d’un Grand Ecran}},
year = {2001}
}
@article{Fischer2005,
author = {Fischer, J. and Bartz, D. and Strasser, W.},
doi = {10.1109/VR.2005.1492774},
file = {:home/acmt/Dropbox/Documentos/Mendeley/IEEE Proceedings. VR 2005. Virtual Reality, 2005/2005/Fischer, Bartz, Strasser/Fischer, Bartz, Strasser - 2005 - Stylized augmented reality for improved immersion(2).pdf:pdf},
isbn = {0-7803-8929-8},
journal = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
keywords = {augmented reality,immersion,non-,painterly filtering,photorealistic rendering},
pages = {195--325},
publisher = {Ieee},
title = {{Stylized augmented reality for improved immersion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1492774},
volume = {2005},
year = {2005}
}
@phdthesis{Bimber2005a,
author = {Bimber, O. and Raskar, R.},
file = {:home/acmt/Dropbox/Documentos/Mendeley/Unknown/2005/Bimber, Raskar/Bimber, Raskar - 2005 - Spatial augmented reality.pdf:pdf},
publisher = {Peters},
title = {{Spatial augmented reality}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Spatial+Augmented+Reality\#0},
year = {2005}
}
@article{Park2006,
author = {Park, Sang-cheol and Lee, Sang-woong and Lee, Seong-whan},
doi = {10.1109/ICPR.2006.1093},
file = {::},
isbn = {0-7695-2521-0},
journal = {18th International Conference on Pattern Recognition (ICPR'06)},
pages = {897--900},
publisher = {Ieee},
title = {{Superimposing 3D Virtual Objects using Markerless Tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1699670},
year = {2006}
}
@article{Brudera,
author = {Bruder, Gerd},
file = {::},
journal = {Image (Rochester, N.Y.)},
keywords = {3d user interfaces,architectural walkthroughs,locomotion,passive haptic feed-,redirected walking,virtual environments},
title = {{A Natural User Interface for Immersive Architectural Walkthroughs}}
}
@article{Klopfer2005,
address = {Morristown, NJ, USA},
author = {Klopfer, Eric and Perry, Judy and Squire, Kurt and Jan, Ming-Fong},
doi = {10.3115/1149293.1149333},
file = {::},
isbn = {0805857826},
journal = {Proceedings of the 2005 conference on Computer support for collaborative learning learning 2005: the next 10 years! - CSCL '05},
keywords = {games,handhelds,pda,role play,simulations},
pages = {311--315},
publisher = {Association for Computational Linguistics},
title = {{Collaborative learning through augmented reality role playing}},
url = {http://portal.acm.org/citation.cfm?doid=1149293.1149333},
year = {2005}
}
@article{Mower2009,
author = {Mower, James E.},
doi = {10.1080/13658810802001313},
file = {::},
issn = {1365-8816},
journal = {International Journal of Geographical Information Science},
keywords = {augmented reality,dem,perspective imaging,tin},
month = aug,
number = {8},
pages = {993--1011},
title = {{Creating and delivering augmented scenes}},
url = {http://www.tandfonline.com/doi/abs/10.1080/13658810802001313},
volume = {23},
year = {2009}
}
@article{Juillet2003a,
author = {Juillet, Roberto Ortelli and Mireille, Jury and Nova, Nicolas and Schneider, Daniel K},
file = {::},
journal = {Recherche},
title = {{Styles d’interaction dans les PocketPC: analyses et comparaisons}},
year = {2003}
}
@article{Nakanishi2008,
author = {Nakanishi, Miwa and Ozeki, Mugihiko and Akasaka, Toshiya and Okada, Yusaku},
doi = {10.4304/jmm.3.3.34-43},
file = {::},
issn = {17962048},
journal = {Journal of Multimedia},
number = {3},
pages = {34--43},
title = {{What Conditions are Required to Effectively Use Augmented Reality for Manuals in Actual Work}},
url = {http://ojs.academypublisher.com/index.php/jmm/article/view/2204},
volume = {3},
year = {2008}
}
@article{Rohs2007,
abstract = {A user study was conducted to compare the performance of three methods for map navigation with mobile devices. These methods are joystick navigation, the dynamic peep- hole method without visual context, and the magic lens paradigm using external visual context. The joystick method is the familiar scrolling and panning of a virtual map keep- ing the device itself static. In the dynamic peephole method the device is moved and the map is fixed with respect to an external frame of reference, but no visual information is present outside the devices display. The magic lens method augments an external content with graphical overlays, hence providing visual context outside the device display. Here too motion of the device serves to steer navigation. We compare these methods in a study measuring user performance, mo- tion patterns, and subjective preference via questionnaires. The study demonstrates the advantage of dynamic peephole and magic lens interaction over joystick interaction in terms of search time and degree of exploration of the search space.},
author = {Rohs, Michael and Sch\"{o}ning, Johannes and Raubal, Martin and Essl, Georg and Kr\"{u}ger, A},
doi = {10.1145/1322192.1322219},
file = {::},
isbn = {9781595938176},
journal = {Computing},
keywords = {augmented reality,camera based interaction,camera phones,handheld displays,interac,maps,mobile devices,navigation,spatially aware displays,tion techniques},
pages = {146--153},
publisher = {ACM Press},
title = {{Map navigation with mobile devices: virtual versus physical movement with and without visual context}},
url = {http://portal.acm.org/citation.cfm?id=1322219},
year = {2007}
}
@article{Nini2004,
author = {Nini, B and Batouche, M C},
doi = {10.1109/ICIT.2004.1490732},
file = {::},
isbn = {0780386620},
journal = {2004 IEEE International Conference on Industrial Technology 2004 IEEE ICIT 04},
keywords = {augmented reality,collaboration,departure,do,encrustation,manipulation,network,object,pattern,supposed superposed,used,virtual object},
pages = {1204--1208},
publisher = {Ieee},
title = {{Virtual object manipulation in collaborative augmented reality environment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1490732},
year = {2004}
}
@incollection{Liarokapis2002,
abstract = {This paper presents an innovative approach to enhance students learning and understanding of digital design using complex commercial design flows. The architecture of our experimental system is based on new technologies such as Augmented Reality (AR), XML metadata, and an XML integrated database system. The database provides the content for multimedia presentation in a virtual environment, visualised through AR, which enables students to engage effectively in the process of learning. These include textual descriptions, animated videos, auditory information, images and 3D computer generated objects of related diagrams and designs. An XML interface will allow the teacher to input multimedia information into the database remotely in a simple way. Using an AR interface the content of the database will be visualized in real time using state-of-the-art virtual reality technologies, i.e. head mounted displays, tracking devices, etc. Through this interface, users will be able to interact collaboratively. Using collaborative AR the students will also develop team skills that play a significant role in the learning process. Finally, we provide an illustrative description of how our experimental system will operate, and we present some initial results.},
author = {Liarokapis, F and Mourkoussis, N and Petridis, P and Rumsey, S and Lister, P F and White, M},
file = {::},
keywords = {l education (general),qa75 electronic computers. computer science,qa76 computer software},
publisher = {UICEE},
title = {{An Interactive Augmented Reality System for Engineering Education}},
url = {http://eprints.sussex.ac.uk/1176/},
year = {2002}
}
@article{Reitmayr2001,
abstract = {The combination of mobile computing and collaborative augmented reality into a single system makes the power of computer enhanced interaction and communication in the real world accessible anytime and everywhere. The paper describes our work to build a mobile collaborative augmented reality system that supports true stereoscopic 3D graphics, a pen and pad interface and direct interaction with virtual objects. The system is assembled from off-the-shelf hardware components and serves as a basic test bed for user interface experiments related to computer supported collaborative work in augmented reality. A mobile platform implementing the described features and collaboration between mobile and stationary users are demonstrated},
author = {Reitmayr, G and Schmalstieg, D},
doi = {10.1109/ISAR.2001.970521},
file = {::},
isbn = {0769513751},
journal = {Proceedings IEEE and ACM International Symposium on Augmented Reality},
keywords = {3d interaction,a user wearing,able computing,augmented reality,computer supported collaborative work,figure 1,hybrid tracking,mobile aug,mobile computing,wear},
pages = {114--123},
publisher = {IEEE Comput. Soc},
title = {{Mobile collaborative augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=970521},
year = {2001}
}
@article{Schmalstieg2007e,
author = {Schmalstieg, Dieter and Wagner, Daniel},
file = {::},
journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
keywords = {augmented reality games,cultural heritage,mobile augmented reality,wearable computing},
month = nov,
pages = {1--13},
publisher = {Ieee},
title = {{Experiences with Handheld Augmented Reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538819},
year = {2007}
}
@article{Ong2007,
author = {Ong, S.K. and Pang, Y. and a.Y.C. Nee},
doi = {10.1016/j.cirp.2007.05.014},
file = {::},
issn = {00078506},
journal = {CIRP Annals - Manufacturing Technology},
keywords = {assembly design,augmented reality,product evaluation},
number = {1},
pages = {49--52},
title = {{Augmented Reality Aided Assembly Design and Planning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0007850607000145},
volume = {56},
year = {2007}
}
@article{Xu2008,
author = {Xu, K},
doi = {10.1016/j.imavis.2007.08.015},
file = {::},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {augmented reality,fundamental matrix,homography,optical flow,vision based tracking},
month = may,
number = {5},
pages = {673--689},
title = {{Real-time camera tracking for marker-less and unprepared augmented reality environments}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885607001266},
volume = {26},
year = {2008}
}
@article{Klein2007,
author = {Klein, Georg and Murray, David},
doi = {10.1109/ISMAR.2007.4538852},
file = {::},
isbn = {978-1-4244-1749-0},
journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
month = nov,
pages = {1--10},
publisher = {Ieee},
title = {{Parallel Tracking and Mapping for Small AR Workspaces}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538852},
year = {2007}
}
@book{Hartley2004a,
abstract = {How to reconstruct scenes from images using geometry and algebra, with applications to computer vision.},
author = {Hartley, R and Zisserman, Andrew},
chapter = {189},
pages = {672},
publisher = {Cambridge University Press},
title = {{Multiple View Geometry in Computer Vision}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0521540518},
year = {2004}
}
@article{Wang2008,
abstract = {In this paper, we present a robust road detection and tracking method based on a condensation particle filter for real-time video-based navigation applications. The image is divided into horizontal strips, and vanishing point (VP) detection is performed on each image strip. We propose a method for estimating the density of road boundary line segments in the image so that VP detection in an image strip takes into account the detection results in the neighboring image strips. This use of contextual information for VP detection leads to more accurate detection results. The estimated road parameters are then used to initialize the condensation tracker. Experiments using real road videos demonstrate the robustness of our method to difficult road conditions due to the presence of partial occlusion, shadows and road signs.},
author = {Wang, Y and Bai, Li and Fairhurst, M C},
journal = {Intelligent Transportation Systems IEEE Transactions on},
keywords = {image processing,ta1637 image analysis},
number = {4},
pages = {570--579},
title = {{Robust Road Modeling and Tracking using Condensation}},
url = {http://dx.doi.org/10.1109/TITS.2008.2006733},
volume = {9},
year = {2008}
}
@inproceedings{Belluta1989,
author = {Belluta, P and Collini, G and Verri, V and Torre, V},
booktitle = {IEEE Workshop on interpretation of 3D scenes},
pages = {41--49},
title = {{3D visual information from vanishing points}},
year = {1989}
}
@book{Fusiello2005,
author = {Fusiello, Andrea},
booktitle = {International Journal of Computer Vision},
number = {2},
pages = {123--140},
title = {{Elements of Computer Vision: Multiple View Geometry}},
url = {http://www.springerlink.com/index/10.1007/s11263-005-3954-9},
volume = {66},
year = {2005}
}
@article{Rother2002a,
abstract = {A man-made environment is characterized by a lot of parallel lines and a lot of orthogonal edges. In this article, a new method for detecting the three mutual orthogonal directions of such an environment is presented. Since real-time performance is not necessary for architectural application, like building reconstruction, a computationally more intensive approach was chosen. On the other hand, our approach is more rigorous than existing techniques, since the information given by the condition of three mutual orthogonal directions in the scene is identified and incorporated. Since knowledge about the camera geometry can be deduced from the vanishing points of three mutual orthogonal directions, we use this knowledge to reject falsely detected vanishing points. Results are presented from interpreting outdoor scenes of buildings.},
author = {Rother, C},
journal = {Image and Vision Computing},
keywords = {architecture,camera calibration,geometric constraints,vanishing lines,vanishing points},
number = {9-10},
pages = {647--655},
publisher = {Elsevier},
title = {{A new approach to vanishing point detection in architectural environments}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885602000549},
volume = {20},
year = {2002}
}
@article{Landsberg2002,
abstract = {We give an elementary introduction to our papers relating the geometry of rational homogeneous varieties to representation theory. We also describe related work and recent progress.},
author = {Landsberg, J M and Manivel, L},
journal = {Algebraic transformation groups and algebraic varieties proceedings of the conference Interesting algebraic varieties arising in algebraic transformation group theory held at the Erwin Schr\"{o}dinger Institute Vienna October 2226 2001},
pages = {37},
publisher = {Springer Verlag},
title = {{Representation theory and projective geometry}},
url = {http://arxiv.org/abs/math/0203260},
volume = {132},
year = {2002}
}
@inproceedings{Brauer-Burchardt2000,
author = {Brauer-Burchardt, C and Voss, K},
booktitle = {Proceedings 15th International Conference on Pattern Recognition ICPR2000},
pages = {559--562},
publisher = {IEEE Comput. Soc},
title = {{Robust vanishing point determination in noisy images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=905399},
year = {2000}
}
@inproceedings{Rother2002,
author = {Rother, C},
booktitle = {Proceedings of the British Machine Vision Conference BMVC},
keywords = {computer,vision},
title = {{A new approach for vanishing point detection in architectural environments}},
volume = {20},
year = {2002}
}
@article{Olsson2009,
abstract = {Work in multiple view geometry has focused on obtaining globally optimal solutions at the price of computational time efficiency. On the other hand, traditional bundle adjustment algorithms have been found to provide good solutions even though there may be multiple local minima. In this paper we justify this observation by giving a simple sufficient condition for global optimality that can be used to verify that a solution obtained from any local method is indeed global. The method is tested on numerous problem instances of both synthetic and real data sets. In the vast majority of cases we are able to verify that the solutions are optimal, in particular for small-scale problems. We also develop a branch and bound procedure that goes beyond verification. In cases where the sufficient condition does not hold, the algorithm returns either of the following two results: (i) a certificate of global optimality for the local solution or (ii) the global solution.},
author = {Olsson, Carl and Kahl, Fredrik},
journal = {IEEE Conference on Computer Vision and Pattern Recognition (2009)},
pages = {1216--1223},
publisher = {Ieee},
title = {{Projective Least-Squares: Global Solutions with Local Optimization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206864},
year = {2009}
}
@misc{Tokekar2008,
author = {Tokekar, Pratap},
booktitle = {Transform},
title = {{Vanishing points based navigation}},
year = {2008}
}
@incollection{Bevington2002,
author = {Bevington, Philip},
booktitle = {Data Reduction and Error Analysis for the Physical Sciences},
chapter = {6},
title = {{Least-Squares Fit to a Straight Line}},
year = {2002}
}
@article{Almansa2003a,
abstract = {Even though vanishing points in digital images result from parallel lines in the 3D scene, most of the proposed detection algorithms are forced to rely heavily either on additional properties (like orthogonality or coplanarity and equal distance) of the underlying 3D lines, or on knowledge of the camera calibration parameters, in order to avoid spurious responses. In this work, we develop a new detection algorithm that relies on the Helmoltz principle recently proposed for computer vision by Desolneux et al (2001; 2003), both at the line detection and line grouping stages. This leads to a vanishing point detector with a low false alarms rate and a high precision level, which does not rely on any a priori information on the image or calibration parameters, and does not require any parameter tuning.},
author = {Almansa, A and Desolneux, A and Vamech, S},
journal = {Transactions on Pattern Analysis and Machine Intelligence},
number = {4},
pages = {502--507},
title = {{Vanishing point detection without any a priori information}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1190575},
volume = {25},
year = {2003}
}
@article{Nieto2007,
author = {Nieto, Marcos and Salgado, Luis and Jaureguizar, Fernando and Cabrera, Julian},
journal = {2007 IEEE Intelligent Vehicles Symposium},
pages = {315--320},
publisher = {Ieee},
title = {{Stabilization of Inverse Perspective Mapping Images based on Robust Vanishing Point Estimation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4290133},
year = {2007}
}
@inproceedings{Rasmussen2004,
author = {Rasmussen, C},
booktitle = {British Machine Vision Conference},
title = {{Texture-Based Vanishing Point Voting for Road Shape Estimation}},
year = {2004}
}
@article{McLean1995,
abstract = {This paper presents a new method for the detection of vanishing points based on sub-pixel line descriptions which recognizes the existence of errors in feature detection and which does not rely on supervision or the arbitrary specification of thresholds. Image processing and image analysis are integrated into a coherent scheme which extracts straight line structure from images, develops a measure of line quality for each line, estimates the number of vanishing points and their approximate orientations, and then computes optimal vanishing point estimates through combined clustering and numerical optimization. Both qualitative and quantitative evaluation of the algorithms performance is included in the presentation},
author = {McLean, G F and Kotturi, D},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1090--1095},
title = {{Vanishing point detection by line clustering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=473236},
volume = {17},
year = {1995}
}
@book{Hartley2003,
abstract = {How to reconstruct scenes from images using geometry and algebra, with applications to computer vision.},
author = {Hartley, R and Zisserman, A},
institution = {Cambridge University Press},
publisher = {Cambridge Univ Pr},
title = {{Multiple View Geometry in Computer Vision}},
url = {http://books.google.co.uk/books?hl=en\&lr=\&id=si3R3Pfa98QC\&oi=fnd\&pg=PR11\&dq=multi+view+geometry+hartley\&ots=aPv1ip987I\&sig=NkRc7wSgKBQ8EVhqqJoDCzr-z0k},
year = {2003}
}
@incollection{Mohr1992,
abstract = {Une petite lecon de geometrie avec des parties nouvelles dans le domaine.},
author = {Mohr, R},
booktitle = {Handbook of Pattern Recognition and Computer Vision},
chapter = {2.4},
pages = {369--393},
publisher = {World Scientific},
title = {{Projective Geometry and Computer Vision}},
year = {1992}
}
@article{Colombo2005,
abstract = {Image analysis and computer vision can be effectively employed to recover the three-dimensional structure of imaged objects, together with their surface properties. In this paper, we address the problem of metric reconstruction and texture acquisition from a single uncalibrated view of a surface of revolution (SOR). Geometric constraints induced in the image by the symmetry properties of the SOR structure are exploited to perform self-calibration of a natural camera, 3D metric reconstruction, and texture acquisition. By exploiting the analogy with the geometry of single axis motion, we demonstrate that the imaged apparent contour and the visible segments of two imaged cross sections in a single SOR view provide enough information for these tasks. Original contributions of the paper are: single view self-calibration and reconstruction based on planar rectification, previously developed for planar surfaces, has been extended to deal also with the SOR class of curved surfaces; self-calibration is obtained by estimating both camera focal length (one parameter) and principal point (two parameters) from three independent linear constraints for the SOR fixed entities; the invariant-based description of the SOR scaling function has been extended from affine to perspective projection. The solution proposed exploits both the geometric and topological properties of the transformation that relates the apparent contour to the SOR scaling function. Therefore, with this method, a metric localization of the SOR occluded parts can be made, so as to cope with them correctly. For the reconstruction of textured SORs, texture acquisition is performed without requiring the estimation of external camera calibration parameters, but only using internal camera parameters obtained from self-calibration.},
author = {Colombo, Carlo and {Del Bimbo}, Alberto and Pernici, Federico},
institution = {Dipartimento di Sistemi e Informatica-Universit\`{a} di Firenze, Via Santa Marta 3, 1-50139 Firenze, Italy. colombo@dsi.unifi.it},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {1},
pages = {99--114},
title = {{Metric 3D reconstruction and texture acquisition of surfaces of revolution from a single uncalibrated view.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15628272},
volume = {27},
year = {2005}
}
@article{Barnard1983a,
abstract = {Ce papier parle des contraintes perspective: notamment les points de fuite, l'angle etc. pas mal.},
author = {Barnard, S T},
journal = {Artificial Intelligence},
number = {4},
pages = {435--462},
title = {{Interpreting Perspective Images}},
url = {http://dx.doi.org/10.1016/S0004-3702(83)80021-6},
volume = {21},
year = {1983}
}
@article{Delphenich2005,
abstract = {Some concepts of real and complex projective geometry are applied to the fundamental physical notions that relate to Minkowski space and the Lorentz group. In particular, it is shown that the transition from an infinite speed of propagation for light waves to a finite one entails the replacement of a hyperplane at infinity with a light cone and the replacement of an affine hyperplane - or rest space - with a proper time hyperboloid. The transition from the metric theory of electromagnetism to the pre-metric theory is discussed in the context of complex projective geometry, and ultimately it is proposed that the geometrical issues are more general than electromagnetism, namely, they pertain to the transition from point mechanics to wave mechanics.},
author = {Delphenich, David},
journal = {Philosophy of Science},
number = {425--438},
pages = {41},
title = {{Projective geometry and special relativity}},
url = {http://arxiv.org/abs/gr-qc/0512125},
volume = {46},
year = {2005}
}
@inproceedings{Dubrofsky2008,
author = {Dubrofsky, Elan and Woodham, Robert J},
booktitle = {4th International Symposium ISVC 2008},
pages = {202--213},
publisher = {Springer},
title = {{Combining Line and Point Correspondences for Homography Estimation}},
year = {2008}
}
@article{Wang1991,
author = {Wang, Ling-Ling and Tsai, Wen-Hsiang},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {4},
pages = {370--376},
title = {{Camera calibration by vanishing lines for 3-D computer vision}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=88572},
volume = {13},
year = {1991}
}
@inproceedings{Kumar2003,
abstract = {This paper considers video surveillance applied to traffic video streams. We present a framework for analyzing and recognizing different traffic behaviors from image sequences acquired from a fixed camera. Two types of interactions have been mainly considered. In one there is interaction between two or more mobile objects in the Field of View (FOV) of the camera. The other is interaction between mobile objects and static objects in the environment. The framework is based on two types of a priori information: (1) the contextual information of the cameras FOV, in terms of the description of the different static objects of the scene and (2) sets of predefined behavior scenarios, which need to be analyzed in different contexts. At present the system is designed to recognize behavior from stored videos and retrieve the frames in which the specific behaviors took place. We demonstrate successful behavior recognition results for pedestrian-vehicle interaction and vehicle-checkpost interactions.},
author = {Kumar, Pankaj and Ranganath, Surendra and Sengupta, Kuntal},
booktitle = {Intelligent Transportation Systems 2003 Proceedings 2003 IEEE},
number = {2},
pages = {201--209},
publisher = {IEEE},
title = {{Behavior Interpretation from Traffic Video Streams}},
volume = {25},
year = {2003}
}
@article{Shufelt1999,
abstract = {Vanishing point detection algorithms based on a Gaussian sphere representation have been employed in a variety of computer vision systems, for extracting 3D line orientations as a first step towards object detection. Typically, these algorithms have been applied to imagery with strong perspective effects and with little noise or texture, resulting in good solutions for line orientations. However, these algorithms can fail if perspective effects are weak, or if texture edges are predominant; they also fail to take advantage of knowledge about the objects to be detected. In this paper, two new techniques for robust vanishing point detection on the Gaussian sphere are presented; primitive-based vanishing point analysis and interpretation plane error modeling. The performance of these methods, along with two other existing methods from the literature, are quantitatively evaluated and compared for the task of building detection in complex aerial imagery},
author = {Shufelt, J A},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {282--288},
title = {{Performance evaluation and analysis of vanishing point detection techniques}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=754631},
volume = {21},
year = {1999}
}
@article{Marquez-Neila2008,
author = {Marquez-Neila, Pablo and {Garcia Miro}, Jacobo and Buenaposada, Jose M and Baumela, Luis},
journal = {2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {1--8},
publisher = {Ieee},
title = {{Improving RANSAC for fast landmark recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4563138},
year = {2008}
}
@article{Havasi2006,
abstract = {Knowledge of the position of the vanishing point is the key for geometrical modeling of an image containing a reflective surface or cast shadows. Such an image can be analyzed as two subimages that constitute a stereo pair. For this model-estimation task an automatic method is presented that utilizes motion statistics and the statistical properties of image points for the determination of point correspondence and the subsequent estimation of vanishing point position, optimized by use of a goodness-of-fit function. We show that this approach gives robust results in widely different real-world environments, even when the correspondence is corrupted with considerable amounts of noise.},
author = {Havasi, L\'{a}szl\'{o} and Szir\'{a}nyi, Tam\'{a}s},
institution = {Faculty of Information Technology, P\'{e}ter P\'{a}zm\'{a}ny Catholic University, Budapest Pr\'{a}ter Utca, Hungary. havasi@digitus.itk.ppke.hu},
journal = {Optics Letters},
number = {10},
pages = {1411--1413},
title = {{Estimation of the vanishing point in camera-mirror scenes from video.}},
volume = {31},
year = {2006}
}
@incollection{Baer2007,
author = {Baer, Philipp A},
chapter = {Communicat},
pages = {1--28},
publisher = {I-Tech Education and Publishing},
title = {{Robotic Soccer}},
url = {http://purl.org/spica/robotic-soccer-ars},
year = {2007}
}
@article{Grammatikopoulos2007,
author = {Grammatikopoulos, L and Karras, G and Petsa, E},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {automation,calibration,distortion,edge,extraction},
number = {1},
pages = {64--76},
title = {{An automatic approach for camera calibration from vanishing points}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0924271607000081},
volume = {62},
year = {2007}
}
@article{Lozano1992,
author = {Lozano, R and Brogliato, B},
journal = {IEEE Transactions on Automatic Control},
number = {1},
pages = {30--37},
title = {{Adaptive control of a simple nonlinear system without a priori information on the plant parameters}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=109636},
volume = {37},
year = {1992}
}
@inproceedings{Liebowitz1998a,
abstract = {We describe the geometry constraints and algorithmic implementation for metric rectification of planes. The rectification allows metric properties, such as angles and length ratios, to be measured on the world plane from a perspective image. The novel contributions are: first, that in a stratified context the various forms of providing metric information, which include a known angle, two equal though unknown angles, and a known length ratio; can all be represented as circular constraints on the parameters of an affine transformation of the plane-this provides a simple and uniform framework for integrating constraints; second, direct rectification from right angles in the plane; third, it is shown that metric rectification enables calibration of the internal camera parameters; fourth, vanishing points are estimated using a Maximum Likelihood estimator; fifth, an algorithm for automatic rectification. Examples are given for a number of images, and applications demonstrated for texture map acquisition and metric measurements},
author = {Liebowitz, David and Zisserman, Andrew},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
organization = {Citeseer},
pages = {482--488},
publisher = {IEEE Comput. Soc},
title = {{Metric rectification for perspective images of planes}},
url = {http://www.robots.ox.ac.uk/~vgg/publications/papers/liebowitz98.pdf},
year = {1998}
}
@article{Cantzler1981,
author = {Cantzler, H},
journal = {Institute for PerceptionAction and Behaviour Division},
pages = {2--5},
title = {{Random sample consensus (ransac)}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.106.3035\&amp;rep=rep1\&amp;type=pdf},
year = {1981}
}
@article{Chen1996,
abstract = {An inverse eigenvalue problem, where a matrix is to be constructed from some or all of its eigenvalues, may not have a real-valued solution at all. An approximate solution in the sense of least squares is sometimes desirable. Two types of least squares problems are formulated and explored in this paper. In spite of their different appearance, the two problems are shown to be equivalent. Thus one new numerical method, modified from the conventional alternating projection method, is proposed. The method converges linearly and globally and can be used to generate good starting values for other faster but more expensive and locally convergent methods. The idea can be applied to multiplicative inverse eigenvalue problems for the purpose of preconditioning. Numerical examples are presented.},
author = {Chen, Xuzhou and Chu, Moody Ten-Chao},
journal = {SIAM Journal on Numerical Analysis},
keywords = {inverse eigenvalue problem,least squares,lift projection},
number = {6},
pages = {2417},
title = {{On the Least Squares Solution of Inverse Eigenvalue Problems}},
url = {http://link.aip.org/link/SJNAAM/v33/i6/p2417/s1\&Agg=doi},
volume = {33},
year = {1996}
}
@misc{Mohr1996,
author = {Mohr, R and Triggs, B},
booktitle = {Int Symp Photogrammetry and Remote Sensing},
institution = {ISPRS workshop tutorial},
number = {July},
publisher = {Citeseer},
title = {{Projective Geometry for Image Analysis}},
url = {http://www-kogs.iitb.fhg.de/~cveducat/ECV\_Tut\_Proj\_Geom/ProjGeometry.html},
year = {1996}
}
@inproceedings{Liebowitz1998,
abstract = {We describe the geometry constraints and algorithmic implementation for metric rectification of planes. The rectification allows metric properties, such as angles and length ratios, to be measured on the world plane from a perspective image. The novel contributions are: first, that in a stratified context the various forms of providing metric information, which include a known angle, two equal though unknown angles, and a known length ratio; can all be represented as circular constraints on the parameters of an affine transformation of the plane-this provides a simple and uniform framework for integrating constraints; second, direct rectification from right angles in the plane; third, it is shown that metric rectification enables calibration of the internal camera parameters; fourth, vanishing points are estimated using a Maximum Likelihood estimator; fifth, an algorithm for automatic rectification. Examples are given for a number of images, and applications demonstrated for texture map acquisition and metric measurements},
author = {Liebowitz, D and Zisserman, A},
booktitle = {Proceedings 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
file = {::},
organization = {Citeseer},
pages = {482--488},
publisher = {IEEE Comput. Soc},
title = {{Metric rectification for perspective images of planes}},
url = {http://ieeexplore.ieee.org/xpl/freeabs\_all.jsp?arnumber=698649},
year = {1998}
}
@misc{Toussaint1990,
author = {Toussaint, G},
institution = {McGill Univ.},
title = {{Computational geometry and computer vision}},
year = {1990}
}
@article{Schaffalitzky2000,
abstract = {A method for detecting repeated patterns in images and, based on repetitions, vanishing points and lines. Peter.},
author = {Schaffalitzky, F and Zisserman, Andrew},
journal = {Image and Vision Computing},
pages = {647--658},
title = {{Planar Grouping for Automatic Detection of Vanishing Lines and Points}},
url = {http://www.robots.ox.ac.uk/~vgg},
volume = {18},
year = {2000}
}
@article{Rasmussen2008,
abstract = {We present a vision- and ladar-based approach to autonomous driving on rural and desert roads that has been tested extensively in a closed-loop system. The vision component uses Gabor wavelet filters for texture analysis to find ruts and tracks from which the road vanishing point can be inferred via Hough-style voting, yielding a direction estimate for steering control. The ladar component projects detected obstacles along the road direction onto the plane of the front of the vehicle and tracks the 1-D obstacle "gap" presumed due to the road to yield a lateral offset estimate. Several image- and state-based tests to detect failure conditions such as off-road poses (i.e., there is no road to follow) and poor lighting due to sun glare or distracting shadows are also explained. The system's efficacy is demonstrated with analysis of diverse logged data including from the 2005 DARPA Grand Challenge, as well as tests with full control of a vehicle over 15 km of difficult roads at up to 37 km/h with no waypoints. 2008 Springer Science+Business Media, LLC.},
author = {Rasmussen, Christopher},
journal = {Autonomous Robots},
keywords = {control system analysis,estimation,roads streets,testing,vehicles},
number = {3},
pages = {205--229},
publisher = {Kluwer Academic Publishers},
title = {{RoadCompass : following rural roads with vision + ladar using vanishing point tracking}},
url = {http://dx.doi.org/10.1007/s10514-008-9091-x},
volume = {25},
year = {2008}
}
@misc{Zuliani2008,
author = {Zuliani, Marco},
booktitle = {October},
publisher = {MathWorks, URL:" http://www. mathworks. com", November},
title = {{RANSAC for Dummies}},
url = {http://vision.ece.ucsb.edu/~zuliani/Research/RANSAC/docs/RANSAC4Dummies.pdf},
year = {2008}
}
@inproceedings{Aguilera2005,
author = {Aguilera, D G and Lahoz, J G\'{o}mez and Codes, J Finat},
booktitle = {Proceedings of the ISPRS Working Group V4 Workshop 3DARCH 2005 Virtual Reconstruction and Visualization of Complex Architectures},
keywords = {3d reconstruction,geometry,photogrammetry,single image technique,vanishing points estimation},
title = {{A New Method for Vanishing Point Detection in 3D Reconstruction from a Single View}},
year = {2005}
}
@article{Dornaika1999,
abstract = {The problem of a real-time pose estimation between a 3D scene and a single camera is a fundamental task in most 3D computer vision and robotics applications such as object tracking, visual servoing, and virtual reality. In this paper we present two fast methods for estimating the 3D pose using 2D to 3D point and line correspondences. The first method is based on the iterative use of a weak perspective camera model and forms a generalization of DeMenthon's method (1995) which consists of determining the pose from point correspondences. In this method the pose is iteratively improved with a weak perspective camera model and at convergence the computed pose corresponds to the perspective camera model. The second method is based on the iterative use of a paraperspective camera model which is a first order approximation of perspective. We describe in detail these two methods for both non-planar and planar objects. Experiments involving synthetic data as well as real range data indicate the feasibility and robustness of these two methods. We analyse the convergence of these methods and we conclude that the iterative paraperspective method has better convergence properties than the iterative weak perspective method. We also introduce a non-linear optimization method for solving the pose problem. (C) 1999 Academic Press.},
author = {Dornaika, F},
journal = {RealTime Imaging},
number = {3},
pages = {215--230},
title = {{Pose estimation using point and line correspondences}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077201497901170},
volume = {5},
year = {1999}
}
@article{Leung2006,
abstract = {For Gaussian regression, we develop and analyze methods for combining estimators from various models. For squared-error loss, an unbiased estimator of the risk of the mixture of general estimators is developed. Special attention is given to the case that the component estimators are least-squares projections into arbitrary linear subspaces, such as those spanned by subsets of explanatory variables in a given design. We relate the unbiased estimate of the risk of the mixture estimator to estimates of the risks achieved by the components. This results in simple and accurate bounds on the risk and its estimate, in the form of sharp and exact oracle inequalities. That is, without advance knowledge of which model is best, the resulting performance is comparable to or perhaps even superior to what is achieved by the best of the individual models. Furthermore, in the case that the unknown parameter has a sparse representation, our mixture estimator adapts to the underlying sparsity. Simulations show that the performance of these mixture estimators is better than that of a related model-selection estimator which picks a model with the highest weight. Also, the connection between our mixtures with Bayes procedures is discussed.},
author = {Leung, G},
journal = {IEEE Transactions on Information Theory},
number = {8},
pages = {3396--3410},
title = {{Information Theory and Mixing Least-Squares Regressions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1661826},
volume = {52},
year = {2006}
}
@article{Chang2007,
abstract = {In recent years, the success of single-robot SLAM has led to more multi-robot SLAM (MR-SLAM) research. A team of robots with MR-SLAM can explore an environment more efficiently and reliably; however, MR-SLAM also raises many challenging problems, including map fusion, unknown robot poses and scalability issues. The first two problems can be considered as an optimization problem of finding a consistent joint map based on robots\&x2019; relative poses and sensory data. This optimization problem exhibits a similar property of a singlerobot topological/metric mapping. To exploit this property, we propose a multi-robot SLAM (MR-SLAM) algorithm, which builds a graph-like topological map with vertices representing local metric maps and edges describing relative positions of adjacent local maps. In this MR-SLAM algorithm, the map fusion between two robots can be naturally done by adding an edge that connects two topological maps, and the estimation of relative robot pose is simply performed by optimizing this edge. For the third scalable problem, the proposed algorithm is also scalable to the number of robots and the size of an environment. Computer simulations with a public data set and experimental work on Pioneer 3-DX robots have been conducted to validate the performance of the proposed MR-SLAM algorithm.},
author = {Chang, H Jacky and Lee, C S George and Hu, Y Charlie and {Yung-Hsiang Lu}, A Yung-Hsiang Lu},
journal = {2007 IEEERSJ International Conference on Intelligent Robots and Systems},
pages = {1467--1472},
publisher = {Ieee},
title = {{Multi-robot SLAM with topological/metric maps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4399142},
year = {2007}
}
@article{Magee1984,
author = {Magee, M J and Aggarwal, J K},
journal = {Computer Vision Graphics and Image Processing},
number = {2},
pages = {256--267},
publisher = {Elsevier},
title = {{Determining Vanishing Points From Perspective Images}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0734189X84901889},
volume = {26},
year = {1984}
}
@book{Hartley2000,
author = {Hartley, Richard and Zisserman, Andrew},
booktitle = {axiomanueduau},
publisher = {Cambridge University Press},
title = {{Multiple View Geometry}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Multiple+view+geometry\#0},
volume = {4},
year = {2000}
}
@article{Barreto2005,
abstract = {In central catadioptric systems, lines in a scene are projected to conic curves in the image. This work studies the geometry of the central catadioptric projection of lines and its use in calibration. It is shown that the conic curves where the lines are mapped possess several projective invariant properties. From these properties, it follows that any central catadioptric system can be fully calibrated from an image of three or more lines. The image of the absolute conic, the relative pose between the camera and the mirror, and the shape of the reflective surface can be recovered using a geometric construction based on the conic loci where the lines are projected. This result is valid for any central catadioptric system and generalizes previous results for paracatadioptric sensors. Moreover, it is proven that systems with a hyperbolic/elliptical mirror can be calibrated from the image of two lines. If both the shape and the pose of the mirror are known, then two line images are enough to determine the image of the absolute conic encoding the camera's intrinsic parameters. The sensitivity to errors is evaluated and the approach is used to calibrate a real camera.},
author = {Barreto, Jo\~{a}o P and Araujo, Helder},
institution = {Institute for Systems and Robotics, Department of Electrical and Computer Engineering, Polo II, University of Coimbra, 3030 Coimbra, Portugal. jpbar@isr.uc.pt},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {1327--1333},
title = {{Geometric properties of central catadioptric line images and their application in calibration.}},
volume = {27},
year = {2005}
}
@article{Caprile1990,
abstract = {D crire qq. proprietes interessantes de points de fuite, et de l , ils calibrent la camera.},
author = {Caprile, B and Torre, V},
journal = {International Journal of Computer Vision},
number = {2},
pages = {127--140},
publisher = {Springer},
title = {{Using vanishing points for camera calibration}},
url = {http://www.springerlink.com/index/k75077108473tm15.pdf},
volume = {4},
year = {1990}
}
@article{Okada2003,
author = {Okada, Ryuzo and Taniguchi, Yasuhiro and Furukawa, Kenji and Onoguchi, Kazunori and Corporation, Toshiba},
journal = {Proceedings Ninth IEEE International Conference on Computer Vision},
number = {Iccv},
pages = {330--337 vol.1},
publisher = {Ieee},
title = {{Obstacle detection using projective invariant and vanishing lines}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1238363},
year = {2003}
}
@article{Burkhard2007,
author = {Burkhard, Hans-Dieter and Berger, Ralf},
journal = {CaseBased Reasoning Research and Development},
pages = {1--15},
title = {{Cases in Robotic Soccer}},
url = {http://dx.doi.org/10.1007/978-3-540-74141-1\_1},
year = {2007}
}
@inproceedings{Castelpietra2000,
abstract = {Coordination among multiple robots has been extensively studied, since a number of practical tasks can be performed in a more effective way by employing a fleet of coordinated robotic bases. In particular, distributed coordination among robotic agents has been considered within the framework offered by the robotic soccer competitions. We describe the methods and the results achieved in coordinating the players of the ART team participating in the RoboCup F-2000 league. The team is formed by several heterogeneous robots having different mechanics, different sensors, different control software, and, in general, different abilities for playing soccer. The coordination framework we have developed has been successfully applied during the 1999 official competitions allowing both for a significant improvement of the overall team performance and for a complete interchangeability of all the robots},
author = {Castelpietra, C and Iocchi, L and Nardi, D and Piaggio, M and Scalzo, A and Sgorbissa, A},
booktitle = {Intelligent Robots and Systems 2000 IROS 2000 Proceedings 2000 IEEERSJ International Conference on},
pages = {1385 --1390 vol.2},
title = {{Coordination among Heterogeneous Robotic Soccer Players}},
volume = {2},
year = {2000}
}
@article{Birchfield1998,
abstract = {These lecture notes attempt to explain the main ideas of the theory of the quantum Hall effect. The emphasis is on the localization and interaction physics in the extreme quantum limit which gives rise to the quantum Hall effect. The interaction physics in the extreme quantum limit which is responsible for the fractional quantum Hall effect is discussed at length and from an elementary point of view.},
author = {Birchfield, Stan},
pages = {278},
publisher = {Springer},
title = {{An Introduction to Projective Geometry}},
url = {http://arxiv.org/abs/cond-mat/9410047},
year = {1998}
}
@article{York1966,
author = {York, D},
journal = {Canadian J of Phys},
keywords = {VAN},
pages = {1079--1086},
title = {{Least squares fitting of a straight line}},
volume = {44},
year = {1966}
}
@book{Trucco1998,
author = {Trucco, E and Verri, A},
pages = {1--212},
publisher = {Prentice Hall},
title = {{Introductory Techniques for 3-D Computer Vision}},
url = {http://www.amazon.com/Introductory-Techniques-3-D-Computer-Vision/dp/0132611082},
year = {1998}
}
@article{Fischler1981,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
author = {Fischler, Martin A and Bolles, Robert C},
journal = {Communications of the ACM},
number = {6},
pages = {381--395},
publisher = {ACM},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {1981}
}
@article{Zhang1997,
abstract = {The most commonly used techniques for parameter estimation are presented. Particular attention has been devoted to discussions about the choice of appropriate minimization criteria and the robustness of the different techniques. Their application to conic fitting is described. Zhong-Dan LAN zhong-dan.lanimag.fr},
author = {Zhang, Z},
institution = {$\backslash$sc inria},
journal = {Image and Vision Computing},
number = {1},
pages = {59--76},
title = {{Parameter estimation techniques: a tutorial with application to conic fitting}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885696011122},
volume = {15},
year = {1997}
}
@book{Faugeras1993,
author = {Faugeras, Olivier},
pages = {695},
publisher = {MIT Press},
title = {{Three-Dimensional Computer Vision: A Geometric Viewpoint}},
year = {1993}
}
@article{Barnard1983,
abstract = {Ce papier parle des contraintes perspective: notamment les points de fuite, l'angle etc. pas mal.},
author = {Barnard, S T},
journal = {Artificial Intelligence},
number = {4},
pages = {435--462},
title = {{Interpreting Perspective Images}},
url = {http://dx.doi.org/10.1016/S0004-3702(83)80021-6},
volume = {21},
year = {1983}
}
@article{Tai1993,
author = {Tai, A and Kittler, J and Petrou, M and Windeatt, T},
journal = {Image and Vision Computing},
number = {4},
pages = {240--245},
title = {{Vanishing point detection}},
url = {http://linkinghub.elsevier.com/retrieve/pii/026288569390042F},
volume = {11},
year = {1993}
}
@book{Hartley2004,
author = {Hartley, R and Zisserman, Andrew},
booktitle = {Cambridge UK Cambridge Univ Press},
title = {{Multiple View Geometry in Computer Vision, 2nd ed}},
volume = {0521540518},
year = {2004}
}
@article{Guillemaut2005,
abstract = {The majority of camera calibration methods, including the Gold Standard algorithm, use point-based information and simultaneously estimate all calibration parameters. In contrast, we propose a novel calibration method that exploits line orientation information and decouples the problem into two simpler stages. We formulate the problem as minimization of the lateral displacement between single projected image lines and their vanishing points. Unlike previous vanishing point methods, parallel line pairs are not required. Additionally, the invariance properties of vanishing points mean that multiple images related by pure translation can be used to increase the calibration data set size without increasing the number of estimated parameters. We compare this method with vanishing point methods and the Gold Standard algorithm and demonstrate that it has comparable performance.},
author = {Guillemaut, Jean-Yves and Aguado, Alberto S and Illingworth, John},
institution = {School of Electronics and Physical Sciences, University of Surrey, Guildford, Surrey, GU2 7XH, UK. jean-yves.guillemaut@surrey.ac.uk},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {265--270},
title = {{Using points at infinity for parameter decoupling in camera calibration.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15688563},
volume = {27},
year = {2005}
}
@article{Barwick2006,
abstract = {The ARIANNA concept utilizes the Ross Ice Shelf near the coast of Antarctica to increase the sensitivity to cosmogenic neutrinos by roughly an order of magnitude when compared to the sensitivity of existing detectors and those under construction. Therefore, ARIANNA can test a wide variety of scenarios for GZK neutrino production, and probe for physics beyond the standard model by measuring the neutrino cross-section at center of momentum energies near 100 TeV. ARIANNA capitalizes on several remarkable properties of the Ross Ice Shelf: shelf ice is relatively transparent to electromagnetic radiation at radio frequencies and the water-ice boundary below the shelf creates a good mirror to reflect radio signals from neutrino interactions in any downward direction. The high sensitivity results from nearly six months of continuous operation, low energy threshold (\~{}3x10\^{}17 eV), and more than 2pi of sky coverage. The baseline concept for ARIANNA consists of moderately high gain antenna stations arranged on a 100 x 100 square grid, separated by about 300m. Each station consists of a small group of cross-polarized antennas residing just beneath the snow surface and facing downwards. They communicate with a central control hub by wireless links to generate global triggers.},
author = {Barwick, Steven W},
pages = {8},
title = {{ARIANNA: A New Concept for UHE Neutrino Detection}},
url = {http://arxiv.org/abs/astro-ph/0610631},
year = {2006}
}
@article{Derpanis2005,
author = {Derpanis, K G},
journal = {csyorkuca},
pages = {1--2},
title = {{Overview of the RANSAC Algorithm}},
url = {http://www.cs.yorku.ca/~kosta/CompVis\_Notes/ransac.pdf},
volume = {4},
year = {2005}
}
@article{Kong2009,
abstract = {Given a single image of an arbitrary road, that may not be well-paved, or have clearly delineated edges, or some a priori known color or texture distribution, is it possible for a computer to find this road? This paper addresses this question by decomposing the road detection process into two steps: the estimation of the vanishing point associated with the main (straight) part of the road, followed by the segmentation of the corresponding road area based on the detected vanishing point. The main technical contributions of the proposed approach are a novel adaptive soft voting scheme based on variable-sized voting region using confidence-weighted Gabor filters, which compute the dominant texture orientation at each pixel, and a new vanishingpoint- constrained edge detection technique for detecting road boundaries. The proposed method has been implemented, and experiments with 1003 general road images demonstrate that it is both computationally efficient and effective at detecting road regions in challenging conditions.},
author = {Kong, Hui and Audibert, Jean-Yves and Ponce, Jean},
journal = {IEEE Conference on Computer Vision and Pattern Recognition (2009)},
keywords = {machine vision},
number = {3},
pages = {96--103},
publisher = {Ieee},
title = {{Vanishing point detection for road detection}},
url = {http://eprints.pascal-network.org/archive/00006913/},
year = {2009}
}
@article{Almansa2003,
abstract = {Even though vanishing points in digital images result from parallel lines in the 3D scene, most of the proposed detection algorithms are forced to rely heavily either on additional properties (like orthogonality or coplanarity and equal distance) of the underlying 3D lines, or on knowledge of the camera calibration parameters, in order to avoid spurious responses. In this work, we develop a new detection algorithm that relies on the Helmoltz principle recently proposed for computer vision by Desolneux et al (2001; 2003), both at the line detection and line grouping stages. This leads to a vanishing point detector with a low false alarms rate and a high precision level, which does not rely on any a priori information on the image or calibration parameters, and does not require any parameter tuning.},
author = {Almansa, A and Desolneux, A and Vamech, S},
journal = {Transactions on Pattern Analysis and Machine Intelligence},
number = {4},
pages = {502--507},
title = {{Vanishing point detection without any a priori information}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1190575},
volume = {25},
year = {2003}
}
@article{Geyer2001,
author = {Geyer, C and Daniilidis, K},
journal = {International Journal of Computer Vision},
number = {3},
pages = {223--243},
title = {{Catadioptric Projective Geometry}},
volume = {45},
year = {2001}
}
@article{Schaefer2006,
abstract = {We provide an image deformation method based on Moving Least Squares using various classes of linear functions including affine, similarity and rigid transformations. These deformations are realistic and give the user the impression of manipulating real-world objects. We also allow the user to specify the deformations using either sets of points or line segments, the later useful for controlling curves and profiles present in the image. For each of these techniques, we provide simple closed-form solutions that yield fast deformations, which can be performed in real-time.},
author = {Schaefer, S and McPhail, T and Warren, J},
journal = {ACM Transactions on Graphics},
keywords = {deformations,moving least squares,rigid transforma},
number = {3},
pages = {533},
publisher = {ACM},
title = {{Image deformation using moving least squares}},
url = {http://portal.acm.org/citation.cfm?doid=1141911.1141920},
volume = {25},
year = {2006}
}
@article{Stone2001,
author = {Stone, Peter and McAllester, David},
editor = {M�ller, J�rg P and Andre, Elisabeth and Sen, Sandip and Frasson, Claude},
journal = {International Conference on Autonomous Agents},
pages = {316--323},
publisher = {ACM Press},
title = {{An Architecture for Action Selection in Robotic Soccer}},
year = {2001}
}
@article{Kogan2009,
author = {Kogan, H and Maurer, R and Keshet, R},
journal = {IEEE Conference on Computer Vision and Pattern Recognition (2009)},
number = {1},
pages = {755--761},
publisher = {Ieee},
title = {{Vanishing points estimation by self-similarity}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206713},
year = {2009}
}
@article{Gonzalez2009,
author = {Gonz\'{a}lez, Javier and Pe\~{n}a, Daniel and Romera, Rosario},
journal = {Journal of Chemometrics},
keywords = {donoho estimator,kurtosis,projections,robust covariance matrix,stahel},
number = {2},
pages = {78--90},
title = {{A robust partial least squares regression method with applications}},
url = {http://doi.wiley.com/10.1002/cem.1195},
volume = {23},
year = {2009}
}
@article{Liu2007,
abstract = {A fast method for vanishing point (VP) estimation and tracking in road images and its application for lane detection are investigated in this paper. After determine the region of interest of the road images, we use 'Sobel' operator to detect boundaries. Then the boundary image is transformed to the Hough Space by Hough Transform. We proposed a Gaussian Predication Model on the basis of the latest vanishing point to predicate the current VP, and we constructed an objective function including the linear segment intensity and the predicated VP, subsequently the optimal position of the VP is estimated through the least square method. The computational complexity of the estimation algorithm is 0(n), which can operate in real time. We test our algorithm on many structured road and semi-structured road images, the results show that it can robustly determine the VP position regardless the disturbance of around pedestrian, crowd and vehicles. Moreover, experiments on PETS2001 dataset show its application to the road lane detection for the driver assistant system. 2006 IEEE.},
author = {Liu, Hua-Jun and Birner, Regina},
pages = {170},
publisher = {John Benjamins Publishing Co},
title = {{A fast method for vanishing point estimation and tracking and its application in road images}},
url = {http://dx.doi.org/10.1109/ITST.2006.288791},
year = {2007}
}
@article{Torr2000,
author = {Torr, P},
institution = {MSR},
journal = {Computer Vision and Image Understanding},
number = {1},
pages = {138--156},
publisher = {Citeseer},
title = {{MLESAC: A New Robust Estimator with Application to Estimating Image Geometry}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314299908329},
volume = {78},
year = {2000}
}
@article{Lutton1994,
author = {Lutton, E and Maitre, H and Lopez-Krahe, J},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {4},
pages = {430--438},
publisher = {Published by the IEEE Computer Society},
title = {{Contribution to the determination of vanishing points using hough transform}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/34.277598},
volume = {16},
year = {1994}
}
@article{Kogecka2002,
author = {Kogecka, J},
journal = {Proceedings 2002 IEEE International Conference on Robotics and Automation Cat No02CH37292},
keywords = {bile aerial robots,calibration using vanishing points,from vanishing points,lines image,plane,relative orienta,tion,vanishing point estimation,vision guided mo},
pages = {223--228},
publisher = {Ieee},
title = {{Efficient computation of vanishing points}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1013365},
year = {2002}
}
@article{Li2005,
abstract = {We describe an algorithm, ReAS, to recover ancestral sequences for transposable elements (TEs) from the unassembled reads of a whole genome shotgun. The main assumptions are that these TEs must exist at high copy numbers across the genome and must not be so old that they are no longer recognizable in comparison to their ancestral sequences. Tested on the japonica rice genome, ReAS was able to reconstruct all of the high copy sequences in the Repbase repository of known TEs, and increase the effectiveness of RepeatMasker in identifying TEs from genome sequences.},
author = {Li, Ruiqiang and Ye, Jia},
institution = {James D. Watson Institute of Genome Sciences of Zhejiang University, Hangzhou, China.},
journal = {Computational Statistics},
keywords = {robust estimation},
number = {4},
pages = {329--339},
title = {{Trimmed Least Squares Estimator Resistant to Leverage Points}},
volume = {2},
year = {2005}
}
@inproceedings{Hartley1999,
author = {Hartley, Richard and Zisserman, Andrew},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (2008)},
number = {June},
title = {{Tutorial on Multiple View Geometry}},
year = {1999}
}
@article{Brillault-O'Mahony1991,
abstract = {Another method to extract major vanishing points in a inddor secne. .},
author = {Brillault-O'Mahony, B},
institution = {EDF-Direction des Etudes et Recherches},
pages = {289--300},
title = {{New Method for Vanishing Point Detection}},
volume = {54},
year = {1991}
}
@techreport{Kim2006,
author = {Kim, Zuwhan},
title = {{Geometry of Vanishing Points and its Application to External Calibration and Realtime Pose Estimation}},
year = {2006}
}
@article{Zhang2009,
abstract = {Road detection is a crucial part of autonomous driving system. Most of the methods proposed nowadays only achieve reliable results in relatively clean environments. In this paper, we combine edge detection with road area extraction to solve this problem. Our method works well even on noisy campus road whose boundaries are blurred with sidewalks and surface is often covered with unbalanced sunlight. First, segmentation is done and the segments which belong to road are chosen and merged. Second, we use Hough transform and a voting method to get the vanishing point. Then, the boundaries are searched according to the road shape. We also employ prediction to make our method achieve better performance in video sequence. Our method is fast enough to meet real-time requirement. Experiments were carried out on the intelligent vehicle SpringRobot (Fig. 1) on campus roads, which is a good representation of urban environment.},
author = {Zhang, G and Zheng, N N and Cui, C and Yan, Y Z and Yuan, Z J},
journal = {2009 Ieee Intelligent Vehicles Symposium Vols 1 and 2},
keywords = {segmentation,system},
pages = {556--561 1424},
title = {{An Efficient Road Detection Method In Noisy Urban Environment}},
year = {2009}
}
@article{Tardif2009,
author = {Tardif, J P},
journal = {12th IEEE International Conference on Computer Vision Kyoto Japan September 27 October 4 2009},
number = {Iccv},
title = {{Non-iterative Approach for Fast and Accurate Vanishing Point Detection}},
year = {2009}
}
@phdthesis{Aulinas2008,
author = {Aulinas, Josep},
file = {::},
school = {Heriot-Watt University},
title = {{MSc. Thesis dissertation: 3D Visual SLAM applied to large-scale underwater scenarios}},
type = {M.SC},
year = {2008}
}
@inproceedings{Newman2003,
address = {Sienna},
author = {Newman, Paul M and Leonard, John J. and Rikoski, Richard J},
booktitle = {Proceedings of the Eleventh International Symposium on Robotics Research},
file = {::},
title = {{Towards Constant-Time SLAM on an Autonomous Underwater Vehicle Using Synthetic Aperture Sonar}},
year = {2003}
}
@article{Konolige2008,
author = {Konolige, K. and Agrawal, M.},
file = {::},
journal = {IEEE Transactions on Robotics},
month = oct,
number = {5},
pages = {1066--1077},
title = {{FrameSLAM: From Bundle Adjustment to Real-Time Visual Mapping}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4648456},
volume = {24},
year = {2008}
}
@inproceedings{Kim2005,
author = {Kim, Jae-hean and Chung, Myung Jin},
booktitle = {Proceedings of the 2005 IEEE International Conference on Robotics and Automation},
file = {::},
number = {April},
pages = {3360--3365},
publisher = {Ieee},
title = {{Absolute Stereo SFM without Stereo Correspondence for Vision Based SLAM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1570629},
year = {2005}
}
@phdthesis{Thomas2008,
author = {Thomas, Stephen and Salvi, Joaquim and Petillot, Yvan R},
file = {::},
school = {Heriot-Watt University},
title = {{Real-time stereo visual SLAM for autonomous underwater vehicles}},
type = {Project Thesis},
year = {2008}
}
@inproceedings{Richmond2006,
author = {Richmond, Kristof and Rock, Stephen},
booktitle = {Oceans 2006},
file = {::},
month = sep,
pages = {1--6},
publisher = {Ieee},
title = {{An Operational Real-Time Large-Scale Visual Mosaicking and Navigation System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4099052},
year = {2006}
}
@phdthesis{Architecture2008,
author = {Thomas, Stephen},
file = {::},
title = {{Real-time Stereo Visual SLAM}},
type = {MSc},
year = {2008}
}
@article{Eustice2006,
author = {Eustice, Ryan M. and Singh, Hanumant and Leonard, John J. and Walter, M. R.},
file = {::},
journal = {The International Journal of Robotics Research},
keywords = {and the woods,and underwater vehicles,cambridge,computer vision,data association,engineering of,eustice was with the,information lters,joint program in oceanographic,ma,mobile robotics,r,slam,technology,the massachusetts institute of},
month = dec,
number = {12},
pages = {1223--1242},
title = {{Visually Mapping the RMS Titanic: Conservative Covariance Estimates for SLAM Information Filters}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364906072512},
volume = {25},
year = {2006}
}
@inproceedings{Petillot2008,
author = {Petillot, Yvan R and Salvi, Joaquim and Batlle, Elisabet},
booktitle = {Second IFAC Workshop Navigation, Guidance and Control of Underwater Vehicles},
file = {::},
keywords = {computer vision,simultaneous localization and mapping,stereo vision,surface registration,underwater imaging,visual slam},
number = {0},
title = {{3D Large-Scale Seabed Reconstruction for UUV Simultaneous Localization and Mapping}},
volume = {44},
year = {2008}
}
@article{Corke2007,
author = {Corke, Peter and Lobo, J. and Dias, J.},
file = {::},
journal = {The International Journal of Robotics Research},
keywords = {inertial sensing,sensor fusion,vision},
month = jun,
number = {6},
pages = {519--535},
title = {{An Introduction to Inertial and Visual Sensing}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364907079279},
volume = {26},
year = {2007}
}
@inproceedings{Leonard1991,
author = {Leonard, John J. and Durrant-Whyte, H.F.},
booktitle = {Intelligence for Mechanical Systems, Proceedings IROS'91},
file = {::},
number = {91},
pages = {1442--1447},
publisher = {Ieee},
title = {{Simultaneous map building and localization for an autonomous mobile robot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=174711},
year = {1991}
}
@inproceedings{Saez,
author = {Saez, J.M. and Hogue, A. and Escolano, F. and Jenkin, M.},
booktitle = {Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.},
file = {::},
pages = {3562--3567},
publisher = {Ieee},
title = {{Underwater 3D SLAM through entropy minimization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1642246},
year = {2006}
}
@phdthesis{Shahrabi2000,
author = {Shahrabi, Babak Ameri},
booktitle = {Perspective},
file = {::},
school = {University of Stuttgart},
title = {{Automatic Recognition and 3D Reconstruction of Buildings through Computer Vision and Digital Photogrammetry}},
type = {Dissertation},
year = {2000}
}
@article{Olson2006,
author = {Olson, Edwin and Leonard, John J. and Teller, Seth},
file = {::},
journal = {IEEE Journal of Oceanic Engineering},
month = oct,
number = {4},
pages = {949--958},
title = {{Robust Range-Only Beacon Localization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4089076},
volume = {31},
year = {2006}
}
@inproceedings{Nicosevici2008,
address = {Kobe},
author = {Nicosevici, Tudor and Garcia, Rafael},
booktitle = {OCEANS 2008 - MTS/IEEE Kobe Techno-Ocean},
file = {::},
month = apr,
pages = {1--7},
publisher = {IEEE},
title = {{Online Robust 3D Mapping Using Structure from Motion Cues}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4531022},
year = {2008}
}
@inproceedings{Hogue2007,
author = {Hogue, Andrew and German, Andrew and Jenkin, Michael},
booktitle = {2007 IEEE International Conference on Systems, Man and Cybernetics},
file = {::},
month = oct,
pages = {2372--2377},
publisher = {Ieee},
title = {{Underwater environment reconstruction using stereo and inertial data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4413666},
year = {2007}
}
@article{Lemaire2007,
author = {Lemaire, Thomas and Berger, Cyrille and Jung, Il-Kyun and Lacroix, Simon},
file = {::},
journal = {International Journal of Computer Vision},
keywords = {3d slam,bearing only slam,finally,interest point matching,knowledge of robot motions,planners relies on the,precise,the correct execution of,the geometric trajec-,tories provided by the},
number = {3},
pages = {343--364},
title = {{Vision-Based SLAM: Stereo and Monocular Approaches}},
url = {http://www.springerlink.com/index/10.1007/s11263-007-0042-3},
volume = {74},
year = {2007}
}
@phdthesis{Eustice2008,
author = {Eustice, Ryan M.},
file = {::},
month = apr,
number = {2},
pages = {103--122},
school = {MIT},
title = {{Large-Area Visually Augmented Navigation for Autonomous Underwater Vehicles}},
type = {PhD},
volume = {33},
year = {2008}
}
@article{Eustice2008a,
author = {Eustice, Ryan M. and Singh, Hanumant and Pizarro, O.},
file = {::},
journal = {IEEE Journal of Oceanic Engineering},
month = apr,
number = {2},
pages = {103--122},
title = {{Visually Augmented Navigation for Autonomous Underwater Vehicles}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4625213},
volume = {33},
year = {2005}
}
@article{Bay2008,
author = {Bay, H and Ess, a and Tuytelaars, T and Vangool, L},
file = {::},
journal = {Computer Vision and Image Understanding},
keywords = {camera calibration,feature description,interest points,local features,object recognition},
month = jun,
number = {3},
pages = {346--359},
title = {{Speeded-Up Robust Features (SURF)}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314207001555},
volume = {110},
year = {2008}
}
@article{Davison2007,
author = {Davison, Andrew J and Reid, Ian D and Molton, Nicholas D and Stasse, Olivier},
file = {::},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Computer Systems,Computer-Assisted,Computer-Assisted: methods,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Information Storage and Retrieval,Information Storage and Retrieval: methods,Numerical Analysis,Pattern Recognition,Photogrammetry,Photogrammetry: methods,Signal Processing,Three-Dimensional,Three-Dimensional: methods,Video Recording,Video Recording: methods},
month = jun,
number = {6},
pages = {1052--67},
title = {{MonoSLAM: real-time single camera SLAM.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17431302},
volume = {29},
year = {2007}
}
@article{Zhen2005,
author = {Zhen, GUO and Feng, SUN},
file = {::},
journal = {Marine Science},
keywords = {auv,dvl,integrated navigation system,sins},
number = {2},
pages = {34--38},
title = {{Research on integrated navigation method for AUV}},
volume = {4},
year = {2005}
}
@article{Eustice2006a,
author = {Eustice, Ryan M. and Singh, Hanumant and Leonard, John J.},
file = {::},
journal = {IEEE Transactions on Robotics},
month = dec,
number = {6},
pages = {1100--1114},
title = {{Exactly Sparse Delayed-State Filters for View-Based SLAM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4020357},
volume = {22},
year = {2006}
}
@inproceedings{Dudek,
author = {Dudek, G. and Jenkin, M. and Prahacs, C. and Hogue, A. and Sattar, J. and German, Andrew and Giguere, P. and Saunderson, S. and Ripsman, A. and Simhon, S. and Torres, L. and Milios, E. and Rekletis, I.},
booktitle = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems},
file = {::},
keywords = {-autonomous robot,aquatic robot,robotic sensing},
pages = {1749--1754},
publisher = {Ieee},
title = {{A visually guided swimming robot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1545231},
year = {2005}
}
@phdthesis{Gracias2003,
author = {Gracias, N.R.},
file = {::},
month = oct,
number = {4},
pages = {609--624},
title = {{Mosaic-based navigation for autonomous underwater vehicles}},
type = {Dissertation},
volume = {28},
year = {2003}
}
@phdthesis{Chang2002,
author = {Chang, Peng},
booktitle = {Camera},
file = {::},
school = {Carnegie Mellon University},
title = {{Robust tracking and structure from motion with sampling method}},
type = {Dissertation},
year = {2002}
}
@article{Brown2009,
author = {Brown, Hunter C. and Kim, Ayoung and Eustice, Ryan M.},
file = {::},
journal = {Marine Technology Society Journal},
keywords = {auvs,mapping,navigation,slam,testbed},
month = may,
number = {2},
pages = {33--47},
title = {{An Overview of Autonomous Underwater Vehicle Research and Testbed at PeRL}},
url = {http://openurl.ingenta.com/content/xref?genre=article\&issn=0025-3324\&volume=43\&issue=2\&spage=33},
volume = {43},
year = {2009}
}
@article{Smith1990,
author = {Smith, R and Self, M and Cheeseman, P},
file = {::},
journal = {UAI '86. Proceedings of the Second Annual Conference on Uncertainty in Artificial Intelligence.},
pages = {167--193},
title = {{Estimating uncertain spatial relationships in robotics}},
year = {1990}
}
@article{Se2008,
author = {Se, Stephen and Jasiobedzki, Piotr},
file = {::},
journal = {International Journal},
number = {1},
pages = {46--57},
title = {{Stereo-Vision Based 3D Modeling and Localization for Unmanned Vehicles}},
volume = {13},
year = {2008}
}
@article{Zhou2012,
author = {Zhou, Fuqiang and Wang, Yexin and Peng, Bin and Cui, Yi},
doi = {10.1016/j.measurement.2012.10.031},
file = {::},
issn = {02632241},
journal = {Measurement},
month = nov,
publisher = {Elsevier Ltd},
title = {{A Novel Way of Understanding for Calibrating Stereo Vision Sensor Constructed by a Single Camera and Mirrors}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0263224112004034},
year = {2012}
}
@article{Wanga2012,
author = {Wanga, Jijing},
doi = {10.1016/j.proeng.2012.01.259},
issn = {18777058},
journal = {Procedia Engineering},
keywords = {data fusion,map-matching,mixed navigation,mobile phone net},
month = jan,
pages = {2045--2049},
title = {{Design of Mobile Phone Networks and Map-matching Navigation System}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S187770581200269X},
volume = {29},
year = {2012}
}
@article{Tsai2012,
abstract = {During nuclear accidents, when radioactive materials spread into the environment, the people in the affected areas should evacuate immediately. However, few information systems are available regarding escape guidelines for nuclear accidents. Therefore, this study constructs escape guidelines on mobile phones. This application is called Mobile Escape Guidelines (MEG) and adopts two techniques. One technique is the geographical information that offers multiple representations; the other is the augmented reality that provides semi-realistic information services. When this study tested the mobile escape guidelines, the results showed that this application was capable of identifying the correct locations of users, showing the escape routes, filtering geographical layers, and rapidly generating the relief reports. Users could evacuate from nuclear accident sites easily, even without relief personnel, since using slim devices to access the mobile escape guidelines is convenient. Overall, this study is a useful reference for a nuclear accident emergency response.},
author = {Tsai, Ming-Kuan and Lee, Yung-Ching and Lu, Chung-Hsin and Chen, Mei-Hsin and Chou, Tien-Yin and Yau, Nie-Jia},
doi = {10.1016/j.jenvrad.2011.12.025},
issn = {1879-1700},
journal = {Journal of environmental radioactivity},
keywords = {Geographic Information Systems,Guidelines as Topic,Nuclear Power Plants,Radioactive Hazard Release},
month = jul,
pages = {36--44},
pmid = {22260929},
publisher = {Elsevier Ltd},
title = {{Integrating geographical information and augmented reality techniques for mobile escape guidelines on nuclear accident sites.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22260929},
volume = {109},
year = {2012}
}
@article{Xue2012a,
author = {Xue, Ting and Qu, Liqun and Cao, Zhaofeng and Zhang, Tao},
doi = {10.1016/j.flowmeasinst.2012.07.007},
issn = {09555986},
journal = {Flow Measurement and Instrumentation},
keywords = {gas,liquid two-phase flow},
month = oct,
pages = {29--36},
publisher = {Elsevier Ltd},
title = {{Three-dimensional feature parameters measurement of bubbles in gas–liquid two-phase flow based on virtual stereo vision}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0955598612000799},
volume = {27},
year = {2012}
}
@article{Potyrailo2012,
abstract = {New sensor technologies for homeland security applications must meet the key requirements of sensitivity to detect agents below risk levels, selectivity to provide minimal false-alarm rates, and response speed to operate in high throughput environments, such as airports, sea ports, and other public places. Chemical detection using existing sensor systems is facing a major challenge of selectivity. In this review, we provide a brief summary of chemical threats of homeland security importance; focus in detail on modern concepts in chemical sensing; examine the origins of the most significant unmet needs in existing chemical sensors; and, analyze opportunities, specific requirements, and challenges for wireless chemical sensors and wireless sensor networks (WSNs). We further review a new approach for selective chemical sensing that involves the combination of a sensing material that has different response mechanisms to different species of interest, with a transducer that has a multi-variable signal-transduction ability. This new selective chemical-sensing approach was realized using an attractive ubiquitous platform of battery-free passive radio-frequency identification (RFID) tags adapted for chemical sensing. We illustrate the performance of RFID sensors developed in measurements of toxic industrial materials, humidity-independent detection of toxic vapors, and detection of chemical-agent simulants, explosives, and strong oxidizers.},
author = {Potyrailo, Radislav a and Nagraj, Nandini and Surman, Cheryl and Boudries, Hacene and Lai, Hanh and Slocik, Joseph M and Kelley-Loughnane, Nancy and Naik, Rajesh R},
doi = {10.1016/j.trac.2012.07.013},
issn = {0165-9936},
journal = {Trends in analytical chemistry : TRAC},
keywords = {Radio-frequency identification (RFID),Response speed,Toxic industrial material (TIM),Wireless sensor network (WSN),chemical agent,chemical sensing,homeland security,multivariate statistical analysis,radio-frequency identification,response,rfid,selectivity,sensitivity,speed,tim,toxic industrial material,wireless sensor network,wsn},
month = nov,
number = {4},
pages = {133--145},
pmid = {23175590},
publisher = {Elsevier Ltd},
title = {{Wireless sensors and sensor networks for homeland security applications.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23175590},
volume = {40},
year = {2012}
}
@article{Losada2012,
author = {Losada, Cristina and Mazo, Manuel and Palazuelos, Sira E. and Pizarro, Daniel and Marr\'{o}n, Marta and Velasco, Jos\'{e} F.},
doi = {10.1016/j.robot.2012.11.007},
file = {::},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
month = dec,
title = {{Identification and tracking of robots in an intelligent space using static cameras and an XPFCP}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889012002187},
year = {2012}
}
@misc{TheMendeleySupportTeam2011a,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {::},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{Kolter2009,
author = {Kolter, J. Zico and Ng, Andrew Y.},
doi = {10.1109/ROBOT.2009.5152795},
isbn = {978-1-4244-2788-8},
journal = {2009 IEEE International Conference on Robotics and Automation},
month = may,
pages = {1557--1564},
publisher = {Ieee},
title = {{Stereo vision and terrain modeling for quadruped robots}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5152795},
year = {2009}
}
@article{Chatterjee2011,
author = {Chatterjee, Avishek and Ray, Olive and Chatterjee, Amitava and Rakshit, Anjan},
doi = {10.1016/j.eswa.2011.01.007},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {extended kalman filter},
month = jul,
number = {7},
pages = {8266--8274},
publisher = {Elsevier Ltd},
title = {{Development of a real-life EKF based SLAM system for mobile robots employing vision sensing}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417411000273},
volume = {38},
year = {2011}
}
@article{Niu2012,
author = {Niu, Yan and Xia, Heng and Huan, Lele},
doi = {10.1016/j.phpro.2012.03.105},
issn = {18753892},
journal = {Physics Procedia},
month = jan,
pages = {415--420},
publisher = {Elsevier Srl},
title = {{MIDP-based Realization of a Simple Phone Contact Book}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1875389212005214},
volume = {25},
year = {2012}
}
@article{Simoes2013,
author = {Sim\~{o}es, Jorge and Redondo, Rebeca D\'{\i}az and Vilas, Ana Fern\'{a}ndez},
doi = {10.1016/j.chb.2012.06.007},
issn = {07475632},
journal = {Computers in Human Behavior},
month = mar,
number = {2},
pages = {345--353},
title = {{A social gamification framework for a K-6 learning platform}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0747563212001574},
volume = {29},
year = {2013}
}
@article{Mustafah2012,
author = {Mustafah, Yasir Mohd and Azman, Amelia Wong and Akbar, Fajril},
doi = {10.1016/j.proeng.2012.07.214},
isbn = {6012604270},
issn = {18777058},
journal = {Procedia Engineering},
keywords = {indoor localization,stereo vision,uav},
month = jan,
number = {Iris},
pages = {575--579},
title = {{Indoor UAV Positioning Using Stereo Vision Sensor}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877705812026148},
volume = {41},
year = {2012}
}
@article{Arroqui2012,
author = {Arroqui, Mauricio and Mateos, Cristian and Machado, Claudio and Zunino, Alejandro},
doi = {10.1016/j.compag.2012.05.016},
issn = {01681699},
journal = {Computers and Electronics in Agriculture},
keywords = {agricultural information systems},
month = sep,
pages = {14--18},
publisher = {Elsevier B.V.},
title = {{RESTful Web Services improve the efficiency of data transfer of a whole-farm simulator accessed by Android smartphones}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0168169912001305},
volume = {87},
year = {2012}
}
@article{Boyer2012,
author = {Boyer, Kim and Cetin, Mujdat and Jain, Anil and Lee, Seong-Whan},
doi = {10.1016/j.patrec.2012.02.010},
issn = {01678655},
journal = {Pattern Recognition Letters},
month = may,
number = {7},
pages = {808--810},
publisher = {Elsevier B.V.},
title = {{Special Issue on Awards from ICPR 2010}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865512000517},
volume = {33},
year = {2012}
}
@article{Milanes2012,
author = {Milan\'{e}s, Vicente and Llorca, David F. and Villagr\'{a}, Jorge and P\'{e}rez, Joshu\'{e} and Fern\'{a}ndez, Carlos and Parra, Ignacio and Gonz\'{a}lez, Carlos and Sotelo, Miguel a.},
doi = {10.1016/j.eswa.2011.09.024},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {advanced vehicle control and,safety},
month = feb,
number = {3},
pages = {3362--3373},
title = {{Intelligent automatic overtaking system using vision for vehicle detection}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417411013339},
volume = {39},
year = {2012}
}
@article{Han2001,
author = {Han, Kyu-phil and Song, Kun-woen and Chung, Eui-yoon and Cho, Seok-je and Ha, Yeong-ho},
file = {::},
keywords = {crossover,disparity,fitness function,genetic algorithm,informed generation,intensity similarity,mutation,natural selection,smoothness,stereo matching},
pages = {1729--1740},
title = {{Stereo matching using genetic algorithm with adaptive chromosomes}},
volume = {34},
year = {2001}
}
@incollection{Sokratis2011,
author = {Vavilis, Sokratis and Kavallieratou, Ergina and Paredes, Roberto and Sotiropoulos, Kostas},
booktitle = {LEARNING STRUCTURE AND SCHEMAS FROM DOCUMENTS},
file = {::},
keywords = {binarization algorithm,document image processing,historical document images,hybrid algorithm},
pages = {165--179},
publisher = {Springer},
title = {{A Hybrid Binarization Technique for Document Images}},
year = {2011}
}
@article{Esmaeilabadi2012,
author = {Esmaeilabadi, Mir Ebad Ahmadzadeh},
doi = {10.1016/j.protcy.2012.03.013},
file = {::},
issn = {22120173},
journal = {Procedia Technology},
keywords = {face detection,head tracking,lighting,skin color,thresholding,virtual reality},
month = jan,
pages = {121--131},
title = {{3D Virtual Reality Navigation by Human's Head Movements Using a Generic Webcam}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2212017312002423},
volume = {3},
year = {2012}
}
@article{NirmalSingh2011,
author = {{Nirmal Singh}, N. and Chatterjee, Avishek and Chatterjee, Amitava and Rakshit, Anjan},
doi = {10.1016/j.measurement.2010.12.002},
file = {::},
issn = {02632241},
journal = {Measurement},
month = may,
number = {4},
pages = {620--641},
publisher = {Elsevier Ltd},
title = {{A two-layered subgoal based mobile robot navigation algorithm with vision system and IR sensors}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0263224110003271},
volume = {44},
year = {2011}
}
@article{Sun2012,
author = {Sun, Wei and Chen, Long and Liu, Kai},
doi = {10.7321/jscse.v2.n5.1},
issn = {22517545},
journal = {International Journal of Soft Computing and Software Engineering},
keywords = {autonomous navigation,binocular,object recognition,optical measurement,rvd},
month = may,
number = {5},
pages = {1--12},
title = {{Research on Vision-based Autonomous Navigation Algorithm for RVD between Spacecrafts}},
url = {http://www.jscse.com/papers/?vol=2\&no=5\&n=1},
volume = {2},
year = {2012}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
author = {Lowe, David G},
doi = {10.1023/B:VISI.0000029664.99615.94},
file = {::},
isbn = {1568811012},
issn = {09205691},
journal = {International Journal of Computer Vision},
number = {2},
pages = {91--110},
pmid = {20064111},
publisher = {Springer},
series = {Int. J. Comput. Vis. (Netherlands)},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
url = {http://www.springerlink.com/openurl.asp?id=doi:10.1023/B:VISI.0000029664.99615.94},
volume = {60},
year = {2004}
}
@article{Chapitre,
author = {Chapitre, Cours},
pages = {1--11},
title = {{1. Qu'est-ce que la vision ? 1.1.}}
}
@article{Tang2011a,
author = {Tang, Xinxing and Yamada, Hironao},
doi = {10.1016/j.proeng.2011.08.198},
issn = {18777058},
journal = {Procedia Engineering},
keywords = {computer graphics,hydraulic servo system,tele-operation construction robot,virtual reality},
month = jan,
pages = {1071--1076},
title = {{Tele-operation Construction Robot Control System with Virtual Reality Technology}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877705811016997},
volume = {15},
year = {2011}
}
@article{REN2011,
abstract = {This paper proposes a simple and discriminative framework, using graphical model and 3D geometry to understand the diversity of urban scenes with varying viewpoints. Our algorithm constructs a conditional random field (CRF) network using over-segmented superpixels and learns the appearance model from different set of features for specific classes of our interest. Also, we introduce a training algorithm to learn a model for edge potential among these superpixel areas based on their feature difference. The proposed algorithm gives competitive and visually pleasing results for urban scene segmentation. We show the inference from our trained network improves the class labeling performance compared to the result when using the appearance model solely.},
author = {REN, Ke-yan and SUN, Han-xu and JIA, Qing-xuan and WU, Yao-hong and ZHANG, Wei-yu and GAO, Xin and YE, Ping and SONG, Jing-zhou},
doi = {10.1016/S1005-8885(10)60072-6},
issn = {10058885},
journal = {The Journal of China Universities of Posts and Telecommunications},
keywords = {3d geometry,crf,graphical model,scene recognition},
month = jun,
number = {3},
pages = {110--119},
publisher = {The Journal of China Universities of Posts and Telecommunications},
title = {{Urban scene recognition by graphical model and 3D geometry}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1005888510600726},
volume = {18},
year = {2011}
}
@article{Shabeer2012,
author = {Shabeer, H. Abdul and Wahidabanu, R.S.D.},
doi = {10.1016/j.proeng.2012.01.907},
issn = {18777058},
journal = {Procedia Engineering},
keywords = {cell phone while driving,locate the vehicle using,mobile jammer,mobile phone,vehicle number plate},
month = jan,
number = {2011},
pages = {623--630},
title = {{Averting mobile phone use while driving and technique to locate the mobile phone used vehicle}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877705812009174},
volume = {30},
year = {2012}
}
@article{Koong2012,
author = {Koong, Chorng-Shiuh and Shih, Chihhsiong and Hsiung, Pao-Ann and Lai, Hung-Jui and Chang, Chih-Hung and Chu, William C. and Hsueh, Nien-Lin and Yang, Chao-Tung},
doi = {10.1016/j.jss.2011.08.030},
issn = {01641212},
journal = {Journal of Systems and Software},
month = jan,
number = {1},
pages = {43--60},
publisher = {Elsevier Inc.},
title = {{Automatic testing environment for multi-core embedded software—ATEMES}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0164121211002305},
volume = {85},
year = {2012}
}
@article{Moreda2012,
author = {Moreda, G.P. and Mu\~{n}oz, M.a. and Ruiz-Altisent, M. and Perdigones, a.},
doi = {10.1016/j.jfoodeng.2011.08.011},
issn = {02608774},
journal = {Journal of Food Engineering},
month = jan,
number = {2},
pages = {245--261},
publisher = {Elsevier Ltd},
title = {{Shape determination of horticultural produce using two-dimensional computer vision – A review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0260877411004511},
volume = {108},
year = {2012}
}
@article{Bardsley,
author = {Bardsley, Daniel},
file = {::},
pages = {1--11},
title = {{3D Reconstruction Using the Direct Linear Transform}}
}
@inproceedings{Blum1998,
address = {New York, New York, USA},
author = {Blum, Avrim and Mitchell, Tom},
booktitle = {Proceedings of the eleventh annual conference on Computational learning theory - COLT' 98},
file = {::},
number = {4},
pages = {92--100},
publisher = {ACM Press},
title = {{Combining labeled and unlabeled data with co-training}},
url = {http://www.mendeley.com/catalog/combining-labeled-unlabeled-data-co-training/},
volume = {98},
year = {1998}
}
@article{Chen2012,
author = {Chen, Gang and Guo, Yubo and Wang, Hanping and Ye, Dong and Gu, Yanfeng},
doi = {10.1016/j.ijleo.2011.05.030},
issn = {00304026},
journal = {Optik - International Journal for Light and Electron Optics},
month = apr,
number = {8},
pages = {731--734},
publisher = {Elsevier GmbH.},
title = {{Stereo vision sensor calibration based on random spatial points given by CMM}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0030402611003081},
volume = {123},
year = {2012}
}
@inproceedings{Sokratis2011a,
author = {Vavilis, Sokratis and Kavallieratou, Ergina},
booktitle = {2011 International Conference on Document Analysis and Recognition},
file = {::},
keywords = {- document,algorithms,binarization,tool,training,user-feedback},
month = sep,
pages = {1--5},
publisher = {Ieee},
title = {{A Tool for Tuning Binarization Techniques}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6065265},
year = {2011}
}
@article{Blas2011,
author = {Blas, Morten Rufus and Blanke, Mogens},
doi = {10.1016/j.compag.2010.10.012},
issn = {01681699},
journal = {Computers and Electronics in Agriculture},
keywords = {texture classification},
month = jan,
number = {1},
pages = {159--168},
publisher = {Elsevier B.V.},
title = {{Stereo vision with texture learning for fault-tolerant automatic baling}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S016816991000222X},
volume = {75},
year = {2011}
}
@article{Michalos2012,
author = {Michalos, G. and Makris, S. and Eytan, a. and Matthaiakis, S. and Chryssolouris, G.},
doi = {10.1016/j.procir.2012.07.061},
issn = {22128271},
journal = {Procedia CIRP},
keywords = {assembly,robotic welding,vision system},
month = jan,
pages = {352--357},
title = {{Robot Path Correction Using Stereo Vision System}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2212827112002338},
volume = {3},
year = {2012}
}
@article{Barnes2012,
author = {Barnes, Nick},
doi = {10.1016/j.imavis.2012.05.007},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Bionic eye,Computer vision,Face recognition,Orientation and mobility,Prosthetic vision,Retinal implants},
month = aug,
number = {8},
pages = {478--479},
publisher = {Elsevier B.V.},
title = {{The role of computer vision in prosthetic vision}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885612000832},
volume = {30},
year = {2012}
}
@article{Glette2006,
author = {Glette, K. and Torresen, J. and Yasunaga, M. and Yamaguchi, Y.},
doi = {10.1109/AHS.2006.55},
isbn = {0-7695-2614-4},
journal = {First NASA/ESA Conference on Adaptive Hardware and Systems (AHS'06)},
pages = {373--380},
publisher = {Ieee},
title = {{On-Chip Evolution Using a Soft Processor Core Applied to Image Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1638187},
year = {2006}
}
@article{Chapitrea,
author = {Chapitre, Cours},
pages = {1--5},
title = {{1. Probl\`{e}me g\'{e}n\'{e}ral de la vision du relief. 1.1.}}
}
@article{Bakar2012,
author = {Bakar, Mohd Nazri Abu and Saad, Abdul Rahman Mohd.},
doi = {10.1016/j.proeng.2012.07.138},
file = {::},
issn = {18777058},
journal = {Procedia Engineering},
keywords = {autonomous robot,detection and tracking,doctor following robot,monocular vision,range finding},
month = jan,
number = {Iris},
pages = {22--31},
title = {{A Monocular Vision-based Specific Person Detection System for Mobile Robot Applications}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877705812025246},
volume = {41},
year = {2012}
}
@article{Menezes2011,
author = {Menezes, Paulo and Lerasle, Fr\'{e}d\'{e}ric and Dias, Jorge},
doi = {10.1016/j.imavis.2011.01.003},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Assistant robot,Computer vision,Data fusion,Human motion capture,Particle filtering},
month = may,
number = {6},
pages = {382--393},
publisher = {Elsevier B.V.},
title = {{Towards human motion capture from a camera mounted on a mobile robot}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885611000102},
volume = {29},
year = {2011}
}
@article{Nundy2013,
abstract = {Behavioral models for mobile phone-based diabetes interventions are lacking. This study explores the potential mechanisms by which a text message-based diabetes program affected self-management among African-Americans.},
author = {Nundy, Shantanu and Dick, Jonathan J and Solomon, Marla C and Peek, Monica E},
doi = {10.1016/j.pec.2012.09.008},
issn = {1873-5134},
journal = {Patient education and counseling},
month = jan,
number = {1},
pages = {125--32},
pmid = {23063349},
publisher = {Elsevier Ireland Ltd},
title = {{Developing a behavioral model for mobile phone-based diabetes interventions.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23063349},
volume = {90},
year = {2013}
}
@article{Hoseinnezhad2011,
author = {Hoseinnezhad, Reza and Bab-Hadiashar, Alireza},
doi = {10.1016/j.cviu.2011.03.007},
file = {::},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = aug,
number = {8},
pages = {1145--1156},
publisher = {Elsevier Inc.},
title = {{An M-estimator for high breakdown robust estimation in computer vision}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314211000956},
volume = {115},
year = {2011}
}
@article{Sabto2012,
author = {Sabto, Nosaiba a. and {Al Mutib}, Khalid},
doi = {10.1016/j.jksuci.2012.10.001},
file = {::},
issn = {13191578},
journal = {Journal of King Saud University - Computer and Information Sciences},
month = oct,
number = {October},
publisher = {King Saud University},
title = {{Autonomous mobile robot localization based on RSSI measurements using an RFID sensor and neural network BPANN}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1319157812000377},
year = {2012}
}
@article{Vidal-Calleja2011,
author = {Vidal-Calleja, Teresa a. and Berger, Cyrille and Sol\`{a}, Joan and Lacroix, Simon},
doi = {10.1016/j.robot.2011.05.008},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {multi-robots cooperation},
month = sep,
number = {9},
pages = {654--674},
publisher = {Elsevier B.V.},
title = {{Large scale multiple robot visual mapping with heterogeneous landmarks in semi-structured terrain}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889011000923},
volume = {59},
year = {2011}
}
@article{Garcia2011,
author = {Garc\'{\i}a, Antonio and Erenas, M.M. and Marinetto, Eugenio D. and Abad, Carlos a. and de Orbe-Paya, Ignacio and Palma, Alberto J. and Capit\'{a}n-Vallvey, Luis F.},
doi = {10.1016/j.snb.2011.04.045},
issn = {09254005},
journal = {Sensors and Actuators B: Chemical},
month = aug,
number = {1},
pages = {350--359},
title = {{Mobile phone platform as portable chemical analyzer}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S092540051100342X},
volume = {156},
year = {2011}
}
@article{Deepa2012,
author = {Deepa, P. and Vasanthanayaki, C.},
doi = {10.1016/j.mejo.2012.05.001},
file = {::},
issn = {00262692},
journal = {Microelectronics Journal},
keywords = {Field programmable gate arrays,Image processing,Low power,Scan order,Single port SRAM,True dual port SRAM},
month = nov,
number = {11},
pages = {916--928},
publisher = {Elsevier},
title = {{FPGA based efficient on-chip memory for image processing algorithms}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0026269212000985},
volume = {43},
year = {2012}
}
@article{Liebe2006,
author = {Liebe, C.C. and Padgett, C. and Chapsky, J. and Wilson, D. and Brown, K. and Jerebets, S. and Goldberg, H. and Schroeder, J.},
doi = {10.1109/AERO.2006.1655898},
file = {::},
isbn = {0-7803-9545-X},
journal = {2006 IEEE Aerospace Conference},
pages = {1--10},
publisher = {Ieee},
title = {{Spacecraft Hazard Avoidance Utilizing Structured Light}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1655898},
year = {2006}
}
@article{Owens2013,
author = {Owens, Gary M},
issn = {1944-706X},
journal = {Journal of managed care pharmacy : JMCP},
month = jan,
number = {1 Suppl A},
pages = {S3},
pmid = {23383727},
title = {{Introduction.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23404255},
volume = {19},
year = {2013}
}
@article{Kim2012,
author = {Kim, Jung-Rack and Lin, Shih-Yuan and Hong, Jeong-Woo and Kim, Young-Hwi and Park, Chin-Kang},
doi = {10.1016/j.cageo.2011.09.018},
issn = {00983004},
journal = {Computers \& Geosciences},
keywords = {Geomorphology,High resolution DTM,Mars,Ortho-image,Stereo analysis,Virtual reality},
month = jul,
pages = {184--195},
publisher = {Elsevier},
title = {{Implementation of Martian virtual reality environment using very high-resolution stereo topographic data}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0098300411003475},
volume = {44},
year = {2012}
}
@article{Wasfy2012,
author = {Wasfy, Wael and Zheng, Hong},
doi = {10.1016/j.phpro.2012.05.122},
file = {::},
issn = {18753892},
journal = {Physics Procedia},
month = jan,
pages = {690--697},
title = {{General Structure Design for Fast Image Processing Algorithms Based upon FPGA DSP Slice}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1875389212014356},
volume = {33},
year = {2012}
}
@article{Wu2012,
author = {Wu, Di and Sun, Da-Wen},
doi = {10.1016/j.tifs.2012.08.004},
file = {::},
issn = {09242244},
journal = {Trends in Food Science \& Technology},
month = sep,
title = {{Colour measurements by computer vision for food quality control – A review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0924224412001835},
year = {2012}
}
@article{Kim2012a,
author = {Kim, Jeongdae and Do, Yongtae},
doi = {10.1016/j.proeng.2012.07.262},
isbn = {8253850662},
issn = {18777058},
journal = {Procedia Engineering},
keywords = {block-based motion estimation,mobile robot,obstacle avoidance,robot vision},
month = jan,
number = {Iris},
pages = {911--916},
title = {{Moving Obstacle Avoidance of a Mobile Robot Using a Single Camera}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877705812026628},
volume = {41},
year = {2012}
}
@article{Bayro-Corrochano2011,
author = {Bayro-Corrochano, Eduardo and Eklundh, Jan-Olof},
doi = {10.1016/j.patrec.2011.10.008},
file = {::},
issn = {01678655},
journal = {Pattern Recognition Letters},
month = dec,
number = {16},
pages = {2143--2144},
publisher = {Elsevier B.V.},
title = {{Advances in theory and applications of pattern recognition, image processing and computer vision}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865511003527},
volume = {32},
year = {2011}
}
@unpublished{Vezien,
abstract = {Ce cours est l’aboutissement d’un travail de r\'{e}daction coll\'{e}gial initi\'{e} par le professeur A. Gagalowicz au d\'{e}but des ann\'{e}es 1990 dans le cadre de cours dispens\'{e}s \`{a} l’ESIEA. Il a depuis fait l’objet de constants remaniements et additions, notamment au regard de l’explosion de la manipulation d’images et de contenus multim\'{e}dias dans le domaine grand public, et reste en \'{e}volution constante. 2},
author = {V\'{e}zien, Jean-Marc},
title = {{TRAITEMENT DES IMAGES et VISION PAR MACHINE}},
url = {http://www.limsi.fr/~vezien/pdf\_cours\_ima\_jmv.pdf}
}
@article{Doctorants2003,
author = {Doctorants, Guilde Des},
file = {::},
keywords = {doctorat,guide,recherche,th\`{e}se},
mendeley-tags = {doctorat,guide,recherche,th\`{e}se},
pages = {1--107},
title = {{Guide du Doctorant}},
url = {http://guilde.jeunes-chercheurs.org/Alire/guide/},
year = {2003}
}
@article{Ambrosch2011,
author = {Ambrosch, Kristian and Kubinger, Wilfried},
doi = {10.1016/j.cviu.2010.11.008},
file = {::},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = feb,
number = {2},
pages = {287},
title = {{Corrigendum to “Accurate hardware-based stereo vision” [Comput. Vis. Image Understanding 114 (2010) 1303–1316}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314210002444},
volume = {115},
year = {2011}
}
@article{Schneider2012,
author = {Schneider, Frank E. and Wildermuth, Dennis},
doi = {10.1016/j.robot.2012.05.001},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
month = nov,
number = {11},
pages = {1421--1428},
publisher = {Elsevier B.V.},
title = {{Influences of the robot group size on cooperative multi-robot localisation—Analysis and experimental validation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889012000577},
volume = {60},
year = {2012}
}
@article{Costa2012,
annote = {		L’article d\'{e}crit l’utilisation de une method de d\'{e}tection des objets mobile ( ie : obstacle mobile /humain)  en se basant sur la technique block based motion estimation , \c{c}a consiste \`{a} comparer deux bloc de deux images , prise par une cam\'{e}ra fix\'{e} sur un robot mobile , la comparaison des deux bloc d’images suit le m\^{e}me principe du block matching d’un objet pris par deux cameras , sauf que dans notre cas , l’image est prise par la m\^{e}me camera , mais en deux moment diff\`{e}rent  , 
Le technique block based motion estimation nous permet ainsi de construire  un vecteur de mouvement indiquant le sens mouvement de l’objet.           
Supposition : Surface simple / le robot dispose d’une carte pour \'{e}viter les objets fixe (i e murs.).           
R\'{e}sultats        
-          D\'{e}tecter des objets 
-          Eviter les objets mobiles.           
Limitations : 
-           Effet des couleurs sur la d\'{e}tection des objets et le block matching ! 
-          Relativit\'{e}  mouvement du robot/ mouvement de l’objet   (un objet se d\'{e}pla\c{c}ant de la m\^{e}me vitesse et sens du robot parait fixe .) },
author = {Costa, Paulo and Fernandes, Hugo and Martins, Paulo and Barroso, Jo\~{a}o and Hadjileontiadis, Leontios J.},
doi = {10.1016/j.procs.2012.10.010},
issn = {18770509},
journal = {Procedia Computer Science},
month = jan,
number = {Dsai},
pages = {83--93},
title = {{Obstacle Detection using Stereo Imaging to Assist the Navigation of Visually Impaired People}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050912007727},
volume = {14},
year = {2012}
}
@article{Gerla2011,
author = {Gerla, Mario and Kleinrock, Leonard},
doi = {10.1016/j.comnet.2010.10.015},
issn = {13891286},
journal = {Computer Networks},
month = feb,
number = {2},
pages = {457--469},
publisher = {Elsevier B.V.},
title = {{Vehicular networks and the future of the mobile internet}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1389128610003324},
volume = {55},
year = {2011}
}
@article{Xue2012,
author = {Xue, Junpeng and Su, Xianyu},
doi = {10.1016/j.ijleo.2011.09.025},
file = {::},
issn = {00304026},
journal = {Optik - International Journal for Light and Electron Optics},
month = nov,
number = {21},
pages = {1923--1927},
publisher = {Elsevier GmbH.},
title = {{A new approach for the bundle adjustment problem with fixed constraints in stereo vision}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0030402611005390},
volume = {123},
year = {2012}
}
@article{Nordin2010,
author = {Nordin, Norazah and Embi, Mohamed Amin and Yunus, Melor Md.},
doi = {10.1016/j.sbspro.2010.10.019},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {conceptual framework,design system,learning applications,lifelong learning,mobile learning},
month = jan,
number = {C},
pages = {130--138},
title = {{Mobile Learning Framework for Lifelong Learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877042810020239},
volume = {7},
year = {2010}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
chapter = {Graphical},
doi = {10.1117/1.2819119},
editor = {Jordan, M and Kleinberg, J and Sch\"{o}lkopf, B},
eprint = {0-387-31073-8},
file = {::},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
publisher = {Springer},
series = {Information science and statistics},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
